{
  "publication_id": "clara_third_report_2025",
  "created_at": "2025-12-29T10:26:54.771136+00:00",
  "source_zip": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\data\\overleaf_zips\\ThirdReport.zip",
  "source_zip_sha256": "74eb329c994795fbf391d98716debfcb53784ca2df6d40143fc09be25bf76633",
  "root_tex": "clara_third_report.tex",
  "flattened_tex": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\publications\\clara_third_report_2025\\flattened.tex",
  "model": "gpt-5.1-codex-max",
  "sections_count": 63,
  "candidate_top_k": 100,
  "sections": [
    {
      "section_id": "d21b4a64a2d8656a",
      "level": 1,
      "title": "Abstract",
      "label": null,
      "plain_text_len": 1947,
      "plain_text_excerpt": "Abstract\n\nabstract\n\nThis report presents an overview of progress during the period March 2024--April 2025 on ChatGPT-Based Learning And Reading Assistant (C-LARA), an open source online platform which supports creation of multimodal texts for language learners that integrate audio, images, glosses and other annotations. Building on earlier work, we use GPT-4o and other Lange Language Models to automate most or all of the annotation, guided by pedagogical needs and exploiting new AI capabilities. A central goal of the project is to explore how modern AI can act as a collaborative partner in research projects of this kind.\n\nOver the past year, our principle achievements are the following:\n\nitemize\n More accurate annotation: A principled treatment of multi-word expressions (MWEs) integrating segment-level translation now halves error rates in English glossing.\n Flexible image generation: We…",
      "analysis": {
        "section_summary": "The abstract outlines progress on C-LARA from March 2024 to April 2025, highlighting improved multimodal text creation for language learners. Key advances include more accurate annotations through principled handling of multi-word expressions, a coherent image generation pipeline, faster processing via parallelization, better support for Indigenous languages through manual modes, and leveraging new OpenAI models for both software engineering and academic writing. The team anticipates near-term capability to automatically produce high-quality annotated multimedia content and increasingly self-manage its development.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.2,
            "rationale": "Mentions a new pipeline for generating coherent sets of images; this view handles configuring and managing coherent images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.15,
            "rationale": "Related to monitoring coherent image generation progress, aligning with the abstract's focus on coherent image pipelines."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.15,
            "rationale": "Provides status for coherent image generation tasks, loosely tied to the described flexible image generation pipeline."
          }
        ],
        "concept_tags": [
          "C-LARA",
          "multimodal content",
          "language learning",
          "GPT-4o",
          "large language models",
          "annotations",
          "multi-word expressions",
          "coherent image generation",
          "parallelization",
          "Indigenous languages",
          "AI-assisted software engineering",
          "AI academic writing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15048,
        "completion_tokens": 613,
        "total_tokens": 15661,
        "estimated_cost_usd": 0.02494
      },
      "has_tex": true
    },
    {
      "section_id": "04304c6b44dd461d",
      "level": 1,
      "title": "How to access C-LARA",
      "label": null,
      "plain_text_len": 94,
      "plain_text_excerpt": "How to access C-LARA\n\nIf you want to experiment with C-LARA, we recommend doing the following:",
      "analysis": {
        "section_summary": "This section introduces how to gain access to C-LARA and suggests steps for those who want to try it out.",
        "relevant_views": [],
        "concept_tags": [
          "C-LARA access",
          "getting started",
          "experimentation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14799,
        "completion_tokens": 389,
        "total_tokens": 15188,
        "estimated_cost_usd": 0.02238875
      },
      "has_tex": true
    },
    {
      "section_id": "ae43069630e030ff",
      "level": 2,
      "title": "Create an account",
      "label": null,
      "plain_text_len": 310,
      "plain_text_excerpt": "Create an account\n\nGo to the C-LARA login page, https://c-lara.unisa.edu.au/accounts/login/. Use the Register tab to create an account. You need to supply an email address to be able to use password retrieval and email confirmation, and receive email updates. We will not share your address with third parties.",
      "analysis": {
        "section_summary": "Explains how to create a C-LARA account by visiting the login page and selecting the Register tab, providing an email address for password retrieval, email confirmation and updates, with reassurance about email privacy.",
        "relevant_views": [
          {
            "url_name": "register",
            "confidence": 0.9,
            "rationale": "This is the signup view where new users create accounts, matching the instruction to use the Register tab on the login page."
          }
        ],
        "concept_tags": [
          "account creation",
          "registration",
          "login page",
          "email address",
          "password retrieval",
          "email confirmation",
          "email updates",
          "privacy"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14995,
        "completion_tokens": 608,
        "total_tokens": 15603,
        "estimated_cost_usd": 0.02482375
      },
      "has_tex": true
    },
    {
      "section_id": "f6d4837a0bd1ddaa",
      "level": 2,
      "title": "Register an API key or submit a funding request",
      "label": null,
      "plain_text_len": 1550,
      "plain_text_excerpt": "Register an API key or submit a funding request\n\nCreating an account is free and language learners can use it to read existing C-LARA content and do a lot of other things. But the AI-based functionality which permits content creation and is the heart of the platform uses GPT-4 and other OpenAI models, which costs money. In order to access these functions, in particular to use the AI to create multimodal content, you need to be able to pay for the OpenAI calls. Independent language learners may well wish to create their own tailored content. It is easy and fast as the AI does most of the work. \n\nThe preferred solution is to use an OpenAI API key valid for GPT-4. Open a ChatGPT account at https://chat.openai.com/auth/login if you do not already have one and get an API key from https://platform.openai.com/api-keys. Then on C-LARA, go to User profile etc $>$ Edit configuration information an…",
      "analysis": {
        "section_summary": "The section explains that while C-LARA accounts are free, creating AI-based content incurs OpenAI costs. Users are encouraged to obtain a GPT-4–capable OpenAI API key, add it under their C-LARA configuration, and have their OpenAI account billed directly. If obtaining an API key is not possible, users can submit a small funding request via the Social network tab to receive limited credit for experimentation.",
        "relevant_views": [
          {
            "url_name": "user_config",
            "confidence": 0.9,
            "rationale": "This view lets users edit configuration settings and enter their OpenAI API key, which the section directs them to do."
          },
          {
            "url_name": "funding_request",
            "confidence": 0.9,
            "rationale": "This view allows users to submit a funding request for OpenAI credit when they cannot provide an API key, matching the described alternative."
          }
        ],
        "concept_tags": [
          "API key",
          "GPT-4",
          "OpenAI charges",
          "Configuration settings",
          "Funding request",
          "OpenAI credit",
          "ChatGPT account"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15099,
        "completion_tokens": 644,
        "total_tokens": 15743,
        "estimated_cost_usd": 0.02531375
      },
      "has_tex": true
    },
    {
      "section_id": "6a53ea0e82ea11de",
      "level": 2,
      "title": "Use ``Simple C-LARA'' to create a piece of content",
      "label": null,
      "plain_text_len": 566,
      "plain_text_excerpt": "Use ``Simple C-LARA'' to create a piece of content\n\nFollow the steps in Appendix~[REF:Section:UsingSimpleCLARA] to enter ``Simple C-LARA'' and create a piece of content. You just need to specify the languages and supply an initial prompt saying what you want C-LARA to write, and the AI does the rest for you.\n\nWhen you have created some content, consider filling out the satisfaction questionnaire at the end. It should take a couple of minutes. Don't feel you have to be nice. If you tell us about things that did not work, it is more likely that we will fix them.",
      "analysis": {
        "section_summary": "The section advises users to enter the Simple C-LARA wizard, specify languages and an initial prompt to let the AI generate content, and then optionally complete a brief satisfaction questionnaire about their experience.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.73,
            "rationale": "Simple C-LARA is the wizard interface for creating content by specifying languages and an initial prompt as described."
          },
          {
            "url_name": "satisfaction_questionnaire",
            "confidence": 0.36,
            "rationale": "The section suggests filling out a satisfaction questionnaire after creating content, which is handled by the satisfaction questionnaire view."
          }
        ],
        "concept_tags": [
          "Simple C-LARA",
          "AI-generated content",
          "Language selection",
          "User prompt",
          "Satisfaction questionnaire",
          "User feedback"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15189,
        "completion_tokens": 652,
        "total_tokens": 15841,
        "estimated_cost_usd": 0.02550625
      },
      "has_tex": true
    },
    {
      "section_id": "f50d0540bc2a95ed",
      "level": 1,
      "title": "Introduction and overview",
      "label": "Section:Introduction",
      "plain_text_len": 46,
      "plain_text_excerpt": "Introduction and overview\nSection:Introduction",
      "analysis": {
        "section_summary": "An introductory section that sets the stage for the report and provides a general overview of its scope.",
        "relevant_views": [],
        "concept_tags": [
          "Introduction",
          "Overview",
          "Report"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14059,
        "completion_tokens": 252,
        "total_tokens": 14311,
        "estimated_cost_usd": 0.02009375
      },
      "has_tex": true
    },
    {
      "section_id": "8aeb5f98a233f1c3",
      "level": 2,
      "title": "Background and overall goals",
      "label": "Section:Background",
      "plain_text_len": 4402,
      "plain_text_excerpt": "Background and overall goals\nSection:Background\n\nChatGPT-Based Learning And Reading Assistant (C-LARA; https://www.c-lara.org/) is an international open source project inaugurated in March 2023. Building on the earlier LARA project [CITE:LARA2019SLaTE], one of the top-level goals was the same: we aimed to create a online platform that would support construction of multimodal texts useful for language learners who wished to improve their reading and listening skills. A text of this kind would contain annotations typically including audio files, images, word glosses, a concordance, and maybe other things.\n\nThe recent release of GPT-4, however, opened up new possibilities. In LARA, a large part of the work involved in building a high-quality multimodal text had to be performed manually. It seemed to us that GPT-4's intelligence was at a completely different level to previous AI software, an…",
      "analysis": {
        "section_summary": "The section introduces C‑LARA, an open‑source successor to LARA launched in 2023 to help language learners by building multimodal, annotated texts with audio, images, glosses, and concordances. Leveraging GPT‑4, the team aims to automate much of the annotation, implementation, and even writing tasks, and by March 2024 had a publicly accessible platform. The next phase targets key challenges: better handling and display of multi‑word expressions, coherent multi‑image generation, faster processing, stronger support for manual annotation (notably for Indigenous languages), assessing AI on large codebases, and exploring AI’s ability to produce longer academic texts.",
        "relevant_views": [],
        "concept_tags": [
          "C-LARA platform",
          "GPT-4",
          "multimodal text",
          "language learning",
          "annotations",
          "audio and images",
          "word glosses",
          "concordance",
          "multi-word expressions",
          "image coherence",
          "processing speed",
          "manual annotation",
          "Indigenous languages",
          "AI software engineering",
          "academic writing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15652,
        "completion_tokens": 529,
        "total_tokens": 16181,
        "estimated_cost_usd": 0.024855
      },
      "has_tex": true
    },
    {
      "section_id": "9f258a8d9557401b",
      "level": 2,
      "title": "Main results",
      "label": "Section:Results",
      "plain_text_len": 2180,
      "plain_text_excerpt": "Main results\nSection:Results\n\nDuring the last year, we made good progress on all of the above issues.\n\ndescription\n\n We have implemented a principled treatment of Multi-Word Expressions, annotated using a Chain of Thought method (). Combining the MWE information with segment translation annotations, also added during this phase, we reduce the average glossing annotation error rate in English by more than 50\\%. MWEs are now displayed as single units in the final multimodal form.\n\n We have implemented a method for creation of coherent sets of images (), which in turn defines the style, the recurrent visual elements, and finally the page images. Coherence in terms of style is generally good. Coherence in terms of content---the recurrent visual elements---is still unsatisfactory with the deployed version of the system. It seems likely, however, that improved models currently in the process o…",
      "analysis": {
        "section_summary": "The report highlights significant gains over the past year: a principled chain-of-thought approach to annotating multi-word expressions, combined with segment translation, has halved glossing errors and renders MWEs as single units. A new pipeline generates coherent image sets defining style, recurring elements and pages, with style coherence good and content coherence expected to improve with newer models. Parallel processing now speeds costly tasks by an order of magnitude. An Indigenous languages editing mode organizes annotations by page and auto-checks consistency. Finally, AI capabilities have markedly improved, with recent models producing working Django code and even full papers with minimal human input.",
        "relevant_views": [
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.67,
            "rationale": "The section reports implementing coherent sets of images defining style, elements, and pages; this view manages and monitors coherent image generation in CLARA projects."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.64,
            "rationale": "Status monitoring for coherent image generation aligns with the described coherent image pipeline and its current content coherence issues."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.61,
            "rationale": "The image generation/editing interface for coherent images corresponds to the reported method for creating coherent sets of images."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.56,
            "rationale": "The text discusses principled treatment of multi-word expressions in annotations; this view supports creating MWE-tagged text in CLARA."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.42,
            "rationale": "Glossing error reduction from MWE and segment translation annotations suggests relevance to the glossed text creation/editing view."
          }
        ],
        "concept_tags": [
          "Multi-Word Expressions",
          "chain-of-thought annotation",
          "segment translation",
          "glossing error reduction",
          "coherent image generation",
          "visual style coherence",
          "recurrent visual elements",
          "parallel processing",
          "Indigenous languages editing mode",
          "automatic consistency checking",
          "AI code generation",
          "o1 model",
          "academic writing automation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15156,
        "completion_tokens": 964,
        "total_tokens": 16120,
        "estimated_cost_usd": 0.028585
      },
      "has_tex": true
    },
    {
      "section_id": "c53f81fc1f9723d2",
      "level": 1,
      "title": "Related work",
      "label": "Section:RelatedWork",
      "plain_text_len": 554,
      "plain_text_excerpt": "Related work\nSection:RelatedWork\n\nA growing body of research in computer-assisted language learning (CALL) highlights the importance of annotated, multimodal texts to increase reading comprehension and vocabulary acquisition [CITE:plonsky2016call]. Traditional platforms have long offered dictionary-based or teacher-curated annotations, while more recent systems incorporate artificial intelligence (AI) to automate tasks such as glossing, text generation, and image creation. Below, we summarize key developments and representative tools in this space.",
      "analysis": {
        "section_summary": "The section reviews prior work in computer-assisted language learning that emphasizes the value of annotated, multimodal texts for improving reading comprehension and vocabulary. It contrasts earlier platforms relying on dictionary or teacher-created annotations with newer AI-powered systems that automate glossing, text generation, and image creation, and outlines key developments and tools in this domain.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.29,
            "rationale": "Glossed texts and annotations are central to the related work discussion, aligning with views that create and manage glossed text versions."
          },
          {
            "url_name": "generate_text_status",
            "confidence": 0.25,
            "rationale": "AI-based text generation is mentioned as a recent development, and this endpoint monitors automated generation of annotated text versions."
          },
          {
            "url_name": "edit_images",
            "confidence": 0.2,
            "rationale": "Multimodal support and AI image creation are highlighted, corresponding to the image editing and generation views for projects."
          }
        ],
        "concept_tags": [
          "computer-assisted language learning",
          "annotated texts",
          "multimodal content",
          "reading comprehension",
          "vocabulary acquisition",
          "AI-powered glossing",
          "text generation",
          "image creation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14423,
        "completion_tokens": 877,
        "total_tokens": 15300,
        "estimated_cost_usd": 0.02679875
      },
      "has_tex": true
    },
    {
      "section_id": "48746c04f4e25bf8",
      "level": 2,
      "title": "Annotated Reading and Glossing Tools",
      "label": null,
      "plain_text_len": 1613,
      "plain_text_excerpt": "Annotated Reading and Glossing Tools\nSeveral open-source platforms focus on providing word- or phrase-level annotations to help learners read authentic texts with minimal interruption. Learning With Texts (LWT)https://learning-with-texts.sourceforge.io/ allows users to import any target-language text, tokenize it automatically, and click on unfamiliar words to look them up in an external dictionary. Translations and notes can then be saved to build a personal vocabulary database. A similar approach underpins LingQhttps://www.lingq.com/ and Readlanghttps://readlang.com/, both of which offer browser-based or mobile interfaces where users can translate words on the fly and create flashcards for spaced repetition review. By significantly reducing “lookup friction,” these systems encourage extensive reading in the target language.\n\nAnother example is Clilstorehttps://multidict.net/clilstore/,…",
      "analysis": {
        "section_summary": "The section surveys tools that add word- or phrase-level glosses to ease reading authentic texts. Learning With Texts, LingQ and Readlang let users click unknown words for dictionary lookups, save translations and notes, and generate flashcards to lower lookup friction and support extensive reading. Clilstore similarly links transcript words to online dictionaries alongside embedded audio or video via Wordlink, illustrating a tradition of static dictionary-based glossing that, while reliable, struggles with context-sensitive or idiomatic expressions.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.46,
            "rationale": "This view supports creating glossed versions of texts in C-LARA, akin to the dictionary-based glossing described in external tools."
          },
          {
            "url_name": "create_glossed_text_from_lemma",
            "confidence": 0.42,
            "rationale": "Generating glossed texts from lemma annotations parallels the word-level glossing functionality highlighted in the section."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.39,
            "rationale": "Combining lemma tagging with glosses aligns with the concept of annotated reading aids discussed."
          }
        ],
        "concept_tags": [
          "annotated reading",
          "glossing",
          "dictionary lookup",
          "extensive reading",
          "vocabulary learning",
          "spaced repetition",
          "CLIL",
          "audio transcript",
          "Wordlink",
          "idiomatic expressions"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14902,
        "completion_tokens": 1206,
        "total_tokens": 16108,
        "estimated_cost_usd": 0.0306875
      },
      "has_tex": true
    },
    {
      "section_id": "4aec95d317339757",
      "level": 2,
      "title": "Multimodal Approaches",
      "label": null,
      "plain_text_len": 1212,
      "plain_text_excerpt": "Multimodal Approaches\nTo provide richer context, platforms increasingly integrate audio and visuals alongside text. Many systems, including Clilstore and LingQ, support synchronized audio playback, allowing learners to listen to a native or text-to-speech (TTS) recording while following the transcript. Some tools highlight each sentence or phrase in “karaoke” style, improving comprehension and aiding pronunciation. Commercial apps such as Beelinguapphttps://beelinguapp.com/ focus on parallel text with audiobook narration, whereas projects like Duolingo Storieshttps://blog.duolingo.com/introducing-duolingo-stories/ embed illustrated narrative panels with native-speaker audio, eliciting users’ comprehension through mini-quizzes.\n\nImage and video integration often relies on manual embedding or linking; however, research suggests that visual context can boost learner engagement and recall [C…",
      "analysis": {
        "section_summary": "The section surveys multimodal language learning platforms that blend audio and visuals with text. It highlights synchronized audio playback with karaoke-style highlighting to aid comprehension and pronunciation, parallel text with audiobook narration, illustrated stories with native-speaker audio and embedded quizzes, and notes emerging trends toward automatic image generation to accompany textual segments. This move toward richer visual and auditory context aims to create more immersive, engaging reading environments.",
        "relevant_views": [
          {
            "url_name": "serve_audio_file",
            "confidence": 0.74,
            "rationale": "Provides HTTP delivery of project audio assets, aligning with discussion of synchronized playback and audiobook narration alongside text."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.7,
            "rationale": "Supports configuring and generating coherent images for project texts, relevant to multimodal integration and AI-based illustration of segments."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.63,
            "rationale": "Monitors asynchronous generation of coherent images, matching the trend toward automatic image creation to enrich textual content."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.58,
            "rationale": "Serves generated coherent image assets over HTTP, enabling visual accompaniment to text as described in multimodal approaches."
          },
          {
            "url_name": "serve_clara_image",
            "confidence": 0.51,
            "rationale": "Delivers stored project images, aligning with manual or generated visual embedding to enhance reading contexts."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.56,
            "rationale": "Handles image generation requests within the Simple C-LARA workflow, pertinent to automating illustrations keyed to textual segments."
          }
        ],
        "concept_tags": [
          "multimodal learning",
          "audio integration",
          "synchronized audio playback",
          "karaoke highlighting",
          "text-to-speech",
          "parallel text",
          "audiobook narration",
          "illustrated stories",
          "native-speaker audio",
          "visual context",
          "automatic image generation",
          "learner engagement",
          "immersive reading environments"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14639,
        "completion_tokens": 949,
        "total_tokens": 15588,
        "estimated_cost_usd": 0.02778875
      },
      "has_tex": true
    },
    {
      "section_id": "bf85f524049b4442",
      "level": 2,
      "title": "AI-Enhanced Tools and Content Creation",
      "label": null,
      "plain_text_len": 1550,
      "plain_text_excerpt": "AI-Enhanced Tools and Content Creation\nWhile dictionary-based or rule-based annotations remain popular, AI-driven features are growing rapidly. Commercial services like Quizlet Q-Chathttps://quizlet.com/blog/q-chat and Duolingo’s AI chatbots demonstrate how large language models (LLMs) can handle learner queries, explain tricky phrases, or create practice dialogues on the fly. However, few platforms go as far as providing entirely AI-generated reading content with automated annotations, which is the territory of emerging systems such as StoryWizard AIhttps://www.storywizard.ai/ (illustrated children’s stories) and advanced open-source prototypes like Multimodal-GPThttps://github.com/open-mmlab/Multimodal-GPT. These approaches aim to reduce teacher workload by generating text, glosses, and even morphological analysis without extensive human intervention.\n\nEarly evaluations of AI-based glo…",
      "analysis": {
        "section_summary": "The section notes the rapid rise of AI-driven language learning features, with LLM-powered chatbots now answering learner queries, explaining phrases, and even generating dialogues. Emerging systems go further by auto-generating reading texts with annotations (e.g., StoryWizard AI, Multimodal-GPT) to reduce teacher workload. Early studies of AI-based glossing show promising coverage but require human review, especially for complex morphology or multi-word expressions. Hybrid workflows combining AI bulk processing with expert refinement currently yield the best balance between dictionary-based reliability and AI’s context-sensitive but potentially hallucinated output.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.58,
            "rationale": "Supports creating and maintaining glossed text versions, aligning with AI-driven or semi-automated annotation workflows discussed for AI-based glossing."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.56,
            "rationale": "Handles lemma and gloss tagging generation/editing, matching the section’s focus on automated glossing and morphological annotation with human review."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.52,
            "rationale": "Provides lemma tagging creation, relevant to automated morphological analysis described as a target for AI-assisted annotation."
          },
          {
            "url_name": "generate_text_monitor",
            "confidence": 0.47,
            "rationale": "Monitors AI/text generation tasks, pertinent to the section’s mention of systems that auto-generate reading content with annotations."
          },
          {
            "url_name": "generate_text_status",
            "confidence": 0.45,
            "rationale": "Exposes status for text generation processes, relating to AI-created reading materials highlighted in the section."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.43,
            "rationale": "The Simple C-LARA wizard dispatches AI/image/text creation tasks, reflecting the trend toward end-to-end AI-generated learning content."
          }
        ],
        "concept_tags": [
          "AI-generated content",
          "Large Language Models (LLMs)",
          "automated glossing",
          "morphological analysis",
          "multi-word expressions",
          "hybrid human-AI workflows",
          "teacher workload reduction",
          "dictionary-based vs AI annotations",
          "hallucination risk",
          "context-sensitive explanations"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14983,
        "completion_tokens": 927,
        "total_tokens": 15910,
        "estimated_cost_usd": 0.02799875
      },
      "has_tex": true
    },
    {
      "section_id": "44a72c653c4106fc",
      "level": 2,
      "title": "Summary",
      "label": null,
      "plain_text_len": 696,
      "plain_text_excerpt": "Summary\nAcross open-source and commercial solutions, the consensus is that annotated, multimodal texts can significantly enhance reading comprehension and vocabulary uptake. Platforms like LWT and LingQ have validated the utility of rapid dictionary lookups and integrated flashcards, while Clilstore and Duolingo demonstrate how audio or video can bolster textual understanding. With the emergence of AI-assisted generation and glossing, tools are beginning to reduce the time and expertise required to create high-quality learning materials. The next sections discuss how our project builds on these ideas to offer a more scalable, community-driven approach to AI-facilitated annotated reading.",
      "analysis": {
        "section_summary": "The summary highlights consensus from existing open-source and commercial tools that annotated, multimodal texts—combining glosses, rapid lookups, flashcards, and audio or video—enhance reading comprehension and vocabulary learning. Emerging AI-assisted glossing and generation further lower the barrier to producing such materials. The project builds on these ideas toward a scalable, community-driven platform for AI-facilitated annotated reading.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.44,
            "rationale": "The section stresses AI-assisted glossing to produce annotated texts efficiently; this view handles creating and generating glossed text versions within C-LARA."
          },
          {
            "url_name": "render_text_start_normal",
            "confidence": 0.36,
            "rationale": "Audio-enhanced, multimodal reading is cited as beneficial; this rendering view initiates creating rendered texts with integrated audio, aligning with the multimodal focus."
          }
        ],
        "concept_tags": [
          "annotated texts",
          "multimodal learning",
          "glossing",
          "AI-assisted generation",
          "dictionary lookups",
          "flashcards",
          "audio integration",
          "reading comprehension",
          "vocabulary acquisition",
          "community-driven platform"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14691,
        "completion_tokens": 1122,
        "total_tokens": 15813,
        "estimated_cost_usd": 0.02958375
      },
      "has_tex": true
    },
    {
      "section_id": "4cbabc5d2368223c",
      "level": 1,
      "title": "New functionality",
      "label": "Section:NewFunctionality",
      "plain_text_len": 888,
      "plain_text_excerpt": "New functionality\nSection:NewFunctionality\n\nIn this section, we describe new functionality added between March 2024 and March 2025. Specifically, we consider the following:\ndescription\n\n Linguistic annotation has been much improved. In particular, it now includes a systematic treatment of Multi-Word Expressions.\n\n There is now good support for creation of texts with multiple images, where the images are aligned to be reasonably coherent with each other in terms of style and content.\n\n Both linguistic annotation and creation of images have been now use parallel processing, making them much faster.\n\n Simple C-LARA has been upgraded to incorporate most of the above functionality. In particular, it can be used to create texts that include annotations of MWEs and coherent image sets.\n\n The interface for editing of Indigenous language texts has been completely revised.\n\ndescription",
      "analysis": {
        "section_summary": "Between March 2024 and March 2025, C-LARA added richer linguistic annotation with systematic multi-word expression handling, improved coherent multi-image generation aligned in style and content, sped up both annotation and image creation via parallel processing, upgraded Simple C-LARA to include these capabilities, and overhauled the interface for editing Indigenous language texts.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.76,
            "rationale": "Supports configuring and generating coherent image sets for a project, matching the new multi-image coherent creation feature."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.63,
            "rationale": "Provides status monitoring for coherent image generation tasks, aligning with the new parallelized coherent image workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.6,
            "rationale": "Offers monitoring of coherent image generation, relevant to the enhanced multi-image coherent support."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.7,
            "rationale": "The Simple C-LARA wizard has been upgraded to include MWE annotation and coherent image set creation, matching this general entry point."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.55,
            "rationale": "Provides review of coherent images at the page level within Simple C-LARA, reflecting the added coherent image support."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.53,
            "rationale": "Allows reviewing coherent images for elements in Simple C-LARA, tied to the upgraded image functionality."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.53,
            "rationale": "Enables reviewing styles for coherent images in Simple C-LARA, pertinent to the coherent image set creation."
          }
        ],
        "concept_tags": [
          "linguistic annotation",
          "multi-word expressions",
          "coherent images",
          "image generation",
          "parallel processing",
          "Simple C-LARA",
          "indigenous languages interface"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15148,
        "completion_tokens": 1218,
        "total_tokens": 16366,
        "estimated_cost_usd": 0.031115
      },
      "has_tex": true
    },
    {
      "section_id": "dfb5c34d45418172",
      "level": 2,
      "title": "Improvements to linguistic annotation",
      "label": "Section:BetterAnnotation",
      "plain_text_len": 593,
      "plain_text_excerpt": "Improvements to linguistic annotation \nSection:BetterAnnotation\n\nIn the second C-LARA report, we evaluated annotation performance and identified poor handling of Multi-Word Expressions (MWEs) as the dominant issue. Here, we describe the principled solution we have implemented, which involves addition of a new annotation phase specifically for identification of MWEs. We have also added an annotation phase for addition of segment translations. We explain how these two new phases feed into processing for lemma-tagging and glossing, and also summarise improvements to segmentation and audio.",
      "analysis": {
        "section_summary": "The section outlines enhancements to C-LARA’s linguistic annotation pipeline, introducing dedicated phases to identify multi-word expressions and add segment-level translations. These feed into subsequent lemma-tagging and glossing, alongside refinements to segmentation and audio handling.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.9,
            "rationale": "This view handles creation of MWE-tagged text, matching the new phase for multi-word expression identification described."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.76,
            "rationale": "Segmentation improvements are noted; this view manages creating segmented text versions."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.64,
            "rationale": "Lemma tagging consumes outputs from MWE and segmentation phases; this view supports creating lemma-tagged texts."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.58,
            "rationale": "Glossing relies on prior annotation phases; this view enables creating glossed text, aligning with the described workflow."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.51,
            "rationale": "The new phase for adding segment translations aligns with this view for creating translated text versions."
          }
        ],
        "concept_tags": [
          "linguistic annotation",
          "multi-word expressions",
          "segment translations",
          "lemma tagging",
          "glossing",
          "segmentation",
          "audio improvements"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15120,
        "completion_tokens": 1135,
        "total_tokens": 16255,
        "estimated_cost_usd": 0.03025
      },
      "has_tex": true
    },
    {
      "section_id": "6683657665748368",
      "level": 3,
      "title": "Annotating Multi-Word Expressions",
      "label": "Section:MWEs",
      "plain_text_len": 882,
      "plain_text_excerpt": "Annotating Multi-Word Expressions \nSection:MWEs\n\nAfter some initial experimentation, we determined that a Chain of Thought approach seemed most suitable for annotating MWEs. We process each segment separately, using the prompt template shown in Figure~[REF:Figure:MWEPromptTemplate]. The prompt template contains slots for the text language, the text in JSON form, and a list of few-shot examples; a typical few-shot example for English is shown in Figure~[REF:Figure:MWEPromptExamples]. The few shot-examples are created in a bootstrapped manner by running the prompt on gpt-4o with an initial set of two handcrafted examples and editing the resulting output.\n\nInitial testing and prompt engineering was done using two annotated English Sherlock Holmes stories from the NTU-MC []bond-etal-2021-teaching. These include MWEs from an extended version of wordnet [CITE:_Fellbaum:1998].",
      "analysis": {
        "section_summary": "Describes the new Chain-of-Thought based workflow for annotating multi-word expressions: each segment is processed with a prompt template parameterised by language, JSON text, and few-shot examples. Few-shot examples are bootstrapped via GPT-4o starting from two handcrafted seeds. Initial prompt engineering and testing used annotated English Sherlock Holmes texts from the NTU-MC corpus, leveraging MWEs from an extended WordNet.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.74,
            "rationale": "This view supports creating and maintaining MWE-tagged text versions within CLARA projects, matching the described MWE annotation functionality."
          }
        ],
        "concept_tags": [
          "Multi-Word Expressions",
          "MWE annotation",
          "Chain-of-Thought prompting",
          "few-shot examples",
          "prompt template",
          "GPT-4o",
          "bootstrapping",
          "text segmentation",
          "NTU-MC corpus",
          "Sherlock Holmes",
          "WordNet"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14733,
        "completion_tokens": 433,
        "total_tokens": 15166,
        "estimated_cost_usd": 0.02274625
      },
      "has_tex": true
    },
    {
      "section_id": "2d81bb7abbc66856",
      "level": 3,
      "title": "Segment translations",
      "label": "Section:Translations",
      "plain_text_len": 464,
      "plain_text_excerpt": "Segment translations \nSection:Translations\n\nWe have also added a new annotation phase which creates translations for segments. The segments are divided into groups, currently of about 250 words each, and submitted to gpt-4o using a prompt which provides minimal instructions about format. As well as being useful in itself, the segment translation are also used for glossing, as described in the immediately following section, and sometimes for creation of images.",
      "analysis": {
        "section_summary": "This section introduces a new annotation phase that translates text in chunks of roughly 250 words by sending them to GPT-4o with minimal formatting instructions. These segment-level translations serve both as stand-alone outputs and as inputs for subsequent glossing and sometimes image generation.",
        "relevant_views": [
          {
            "url_name": "create_translated_text",
            "confidence": 0.56,
            "rationale": "The view handles creating and generating translated versions of a project's text using AI-driven workflows, aligning with the described segment translation annotation phase."
          }
        ],
        "concept_tags": [
          "segment translation",
          "translation annotation",
          "GPT-4o",
          "glossing support",
          "image generation",
          "annotation phase",
          "chunked processing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14909,
        "completion_tokens": 627,
        "total_tokens": 15536,
        "estimated_cost_usd": 0.02490625
      },
      "has_tex": true
    },
    {
      "section_id": "82c12d39cb6773e1",
      "level": 3,
      "title": "Glossing and lemma tagging",
      "label": "Section:GlossingLemmaTagging",
      "plain_text_len": 899,
      "plain_text_excerpt": "Glossing and lemma tagging \nSection:GlossingLemmaTagging\n\nThe point of adding MWE information is that it gives us options for improving glossing and lemma tagging. If a word is part of an MWE, we want the associated gloss and lemma to refer to the MWE, not the word itself. For glossing, we find it is also useful to make available the segment translation information and the full text of the document, in order to provide context.\n\nWe perform the glossing and lemma tagging operations on each segment separately. We experimented with several different ways of passing in the MWE information; the one that worked best was to present the segment as a JSON list with one element per lexical item, and the MWE information paired with the words it refers to. The prompt template is shown in Figure~[REF:Figure:GlossPromptTemplate].\n\nThe prompt template for lemma tagging is similar but a little simpler.",
      "analysis": {
        "section_summary": "This section explains enhancements to glossing and lemma tagging that leverage multiword expression (MWE) information. When a word belongs to an MWE, its gloss and lemma should be attached to the whole MWE, not the individual word. To improve glossing, segment translations and full document text are provided as context. Processing is done segment by segment, and the most effective input format sends each segment as a JSON list of lexical items with MWE annotations paired with their words. A similar but simpler prompt template is used for lemma tagging.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.74,
            "rationale": "Handles creating and editing glossed text, directly aligned with the glossing workflow described, including using contextual information."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.72,
            "rationale": "Supports creation of lemma-tagged text, matching the lemma tagging process discussed."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.71,
            "rationale": "Combines lemma and gloss tagging in one flow, relevant to applying both enhancements with MWE context."
          },
          {
            "url_name": "create_glossed_text_from_lemma",
            "confidence": 0.61,
            "rationale": "Generates glosses from existing lemma annotations, adjacent to the glossing improvements described."
          }
        ],
        "concept_tags": [
          "glossing",
          "lemma tagging",
          "multiword expressions",
          "context-aware annotation",
          "segment-level processing",
          "JSON segment representation",
          "prompt template"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14915,
        "completion_tokens": 577,
        "total_tokens": 15492,
        "estimated_cost_usd": 0.02441375
      },
      "has_tex": true
    },
    {
      "section_id": "c3b54ce18e8b33c8",
      "level": 3,
      "title": "Displaying the final multimedia text",
      "label": "Section:DisplayingMultimedia",
      "plain_text_len": 742,
      "plain_text_excerpt": "Displaying the final multimedia text \nSection:DisplayingMultimedia\n\nTo present the MWEs in an intuitive way, the multimedia text contains JavaScript which modifies the highlighting behaviour to show MWEs as units; thus clicking on one word in an MWE highlights the other words as well. Similarly, when creating audio associated with words, a word that is a component of an MWE is linked to audio for the whole MWE, and when compiling the concordance the lemma that appears is one for the whole MWE.\n\nFigure~[REF:Figure:MWEHighlightingExample] illustrates.\n\nfigure\n \n TheCatAndTheBat/MWEHighlightingExample.jpg\n Example from ``The Cat and the Bat'' illustrating presentation of MWEs in the multimodal text\n Figure:MWEHighlightingExample\nfigure",
      "analysis": {
        "section_summary": "The section describes how multiword expressions are presented in the final multimedia text: JavaScript ensures that clicking any word in an MWE highlights the entire expression, word-level audio links trigger audio for the whole MWE, and concordance entries use the lemma for the complete MWE. An example from \"The Cat and the Bat\" illustrates this integrated MWE handling.",
        "relevant_views": [
          {
            "url_name": "serve_rendered_text",
            "confidence": 0.7,
            "rationale": "This view serves the rendered multimedia text to the user, where the JavaScript-driven MWE highlighting and integrated audio behavior would be experienced."
          },
          {
            "url_name": "serve_audio_file",
            "confidence": 0.45,
            "rationale": "It delivers audio files associated with words or MWEs, aligning with the described linking of component words to audio for the whole expression."
          }
        ],
        "concept_tags": [
          "multiword expressions",
          "multimedia text",
          "highlighting",
          "audio linkage",
          "concordance lemma",
          "JavaScript interaction"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14303,
        "completion_tokens": 635,
        "total_tokens": 14938,
        "estimated_cost_usd": 0.02422875
      },
      "has_tex": true
    },
    {
      "section_id": "1486c3f4b0ab6554",
      "level": 3,
      "title": "Better segmentation",
      "label": "Section:Segmentation",
      "plain_text_len": 870,
      "plain_text_excerpt": "Better segmentation \nSection:Segmentation\n\nThe segmentation phase is now divided into two subphases. In the first subphase, the AI is used to divide the text into pages, with each page divided into segments. The prompt recommends that the AI first determines the genre of the text (story, essay, poem, dictionary, etc), and then makes an appropriate subdivision. For example, in a story a typical page would be 1--3 paragraphs and a typical segment would be a sentence, while in a poem a typical page might be 1--3 verses and a typical segment would be a line. In the second phase, each segment is processed separately, using a language-dependent prompt which gives examples of how to subdivide words or mark words containing punctuation marks (hyphens, apostrophes) as single lexical units.\n\nThe new scheme gives much better results, particularly in the first subphase.",
      "analysis": {
        "section_summary": "The segmentation step has been split into two stages: first, an AI divides the text into pages and segments after identifying its genre, applying genre-appropriate page and segment sizes; then each segment is processed with language-specific prompts to split words or treat punctuation-bound forms as single units. This two-phase approach yields markedly better segmentation, especially in the initial stage.",
        "relevant_views": [
          {
            "url_name": "create_segmented_text",
            "confidence": 0.74,
            "rationale": "This view handles creating and editing segmented text versions within projects, aligning with the described improvements to the segmentation phase."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.38,
            "rationale": "The Simple C-LARA wizard includes segmentation as one of its automated steps, so changes to how segmentation is performed are relevant to this workflow."
          }
        ],
        "concept_tags": [
          "segmentation",
          "AI-assisted annotation",
          "genre-based subdivision",
          "pages and segments",
          "language-specific prompts",
          "lexical units",
          "punctuation handling"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14781,
        "completion_tokens": 1213,
        "total_tokens": 15994,
        "estimated_cost_usd": 0.03060625
      },
      "has_tex": true
    },
    {
      "section_id": "50141d71b910b358",
      "level": 3,
      "title": "Page audio",
      "label": "Section:PageAudio",
      "plain_text_len": 207,
      "plain_text_excerpt": "Page audio \nSection:PageAudio\n\nA small but useful addition is that the final multimodal rendering stage now adds an audio file for each whole page, creating it by concatenating the pages segment audio files.",
      "analysis": {
        "section_summary": "The multimodal rendering pipeline now generates a separate audio file for each page by concatenating that page’s segment audio files, adding page-level audio to the final output.",
        "relevant_views": [
          {
            "url_name": "render_text_start_normal",
            "confidence": 0.46,
            "rationale": "This view starts the normal text rendering process, which now includes producing page-level audio during the final multimodal rendering stage."
          },
          {
            "url_name": "render_text_monitor",
            "confidence": 0.38,
            "rationale": "Monitors the rendering job whose output now includes concatenated page audio files as part of the multimodal rendering."
          },
          {
            "url_name": "render_text_status",
            "confidence": 0.35,
            "rationale": "Provides status for the rendering task that generates multimodal outputs, including the new per-page audio files."
          },
          {
            "url_name": "render_text_complete",
            "confidence": 0.35,
            "rationale": "Displays completion information for a rendering job that now produces page-level audio by concatenating segment audio."
          },
          {
            "url_name": "render_text_start_phonetic",
            "confidence": 0.25,
            "rationale": "Initiates phonetic rendering, which similarly runs through the multimodal pipeline that adds page-level audio from segment concatenation."
          }
        ],
        "concept_tags": [
          "page audio",
          "multimodal rendering",
          "concatenated segment audio",
          "page-level audio generation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14183,
        "completion_tokens": 864,
        "total_tokens": 15047,
        "estimated_cost_usd": 0.02636875
      },
      "has_tex": true
    },
    {
      "section_id": "a8651ce07183c862",
      "level": 2,
      "title": "Creating coherent image sets",
      "label": "Section:CoherentImages",
      "plain_text_len": 52,
      "plain_text_excerpt": "Creating coherent image sets \nSection:CoherentImages",
      "analysis": {
        "section_summary": "This section introduces new functionality for creating coherent sets of images across a project, focusing on generating and managing consistent image styles and elements for pages.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.8,
            "rationale": "The edit_images_v2 view is used to configure and manage coherent images for a CLARA project, matching the section's focus on creating coherent image sets."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.72,
            "rationale": "The monitor endpoint supports tracking the background tasks involved in generating coherent image sets, which aligns with creating coherent images."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.72,
            "rationale": "The status endpoint reports progress of coherent image generation tasks, relevant to the process of creating coherent image sets."
          }
        ],
        "concept_tags": [
          "coherent images",
          "image sets",
          "consistent style",
          "image generation",
          "CLARA projects"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14150,
        "completion_tokens": 773,
        "total_tokens": 14923,
        "estimated_cost_usd": 0.0254175
      },
      "has_tex": true
    },
    {
      "section_id": "32f729ff7832284b",
      "level": 3,
      "title": "Overview",
      "label": "Section:CoherentImagesOverview",
      "plain_text_len": 2804,
      "plain_text_excerpt": "Overview \nSection:CoherentImagesOverview\n\nA major new piece of functionality is support for creation of text containing multiple images. Here, the challenge is to produce images that are coherent with each other.\n\nCoherence can involve both style and content. Coherence of style means choosing a single artistic theme, a defined colour palette, and so on. Coherence of content means that elements which occur in more than one image (people, animals, objects, locations...) will be realised in roughly the same way. Our current approach in both cases is the same. We instruct the AI to create descriptions for the aspects, style or repeated elements, that will be shared between images; we then include this information in the prompts that generate the final descriptions that are passed to DALL-E-3 to create the images themselves. We thus have a cascade of prompts, where each prompt creates materia…",
      "analysis": {
        "section_summary": "This section introduces C‑LARA's new support for generating sets of images that remain coherent across a text, both in visual style (consistent artistic theme and palette) and in content (repeated elements rendered similarly). The approach uses cascaded AI prompts to first craft shared style and element descriptions, which are then included in DALL‑E‑3 generation prompts. Multiple candidate descriptions and images are produced for review by humans or AI to select the best or trigger regeneration. Style coherence works well; content coherence is more challenging with current API models, though newer ChatGPT image models may improve this.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.83,
            "rationale": "Primary view for configuring and managing coherent image generation, including style and repeated elements, matching the section's description of coherent image sets."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.72,
            "rationale": "Provides monitoring for coherent image generation tasks, relevant to managing the cascaded prompt and review workflow described."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.72,
            "rationale": "Status endpoint for coherent image generation jobs, aligning with producing and reviewing multiple image alternatives."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.61,
            "rationale": "Supports reviewing generated style exemplars in the Simple C‑LARA wizard, matching the section's focus on style coherence and selection."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.58,
            "rationale": "Provides review of page-level generated images in coherent sets, relevant to selecting among multiple alternatives."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.56,
            "rationale": "Enables review of repeated element imagery, pertinent to maintaining content coherence across images."
          }
        ],
        "concept_tags": [
          "coherent images",
          "style coherence",
          "content coherence",
          "cascaded prompts",
          "DALL-E-3",
          "image generation",
          "AI-generated style description",
          "alternative generation",
          "image review and selection",
          "ChatGPT images",
          "visual consistency"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15009,
        "completion_tokens": 993,
        "total_tokens": 16002,
        "estimated_cost_usd": 0.02869125
      },
      "has_tex": true
    },
    {
      "section_id": "dd70ae8eb92bdb6e",
      "level": 3,
      "title": "Style",
      "label": "Section:CoherentImagesStyle",
      "plain_text_len": 570,
      "plain_text_excerpt": "Style \nSection:CoherentImagesStyle\n\nThe starting point in the process of creating a coherent image set is to generate a style description. The prompt template used to do this is shown in\nFigure~[REF:Figure:StyleGenerationTemplate]. We pass in the user's instructions, the text, and optionally some background information.\n\nWe find in practice that it is usually most effective to provide short, vague instructions for generating the style and let the AI make the substantive decisions.\nFigure~[REF:Figure:StyleExample] shows another example from ``The Cat and the Bat''.",
      "analysis": {
        "section_summary": "Describes the first step in creating a coherent image set: generating an overall style description via a prompt template that takes user instructions, the text, and optional background. It notes that brief, broad instructions often work best and illustrates with an example from \"The Cat and the Bat.\"",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.7,
            "rationale": "The edit_images_v2 view manages coherent image configuration and triggers asynchronous generation of style descriptions as part of the coherent image workflow described."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.65,
            "rationale": "Provides monitoring/status for coherent image generation tasks, including the style generation step mentioned in the section."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.65,
            "rationale": "Exposes status information for coherent image generation tasks such as creating the style description."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.45,
            "rationale": "The Simple C-LARA wizard can dispatch style generation requests for coherent images, corresponding to the style description step outlined."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.45,
            "rationale": "Provides status polling for style generation requests initiated via the Simple C-LARA interface, relevant to creating style descriptions."
          }
        ],
        "concept_tags": [
          "coherent images",
          "style description",
          "prompt template",
          "user instructions",
          "background information",
          "AI image generation",
          "Simple C-LARA"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14565,
        "completion_tokens": 777,
        "total_tokens": 15342,
        "estimated_cost_usd": 0.02597625
      },
      "has_tex": true
    },
    {
      "section_id": "7139961c4fc24f1f",
      "level": 3,
      "title": "Elements",
      "label": "Section:CoherentImagesElements",
      "plain_text_len": 2244,
      "plain_text_excerpt": "Elements \nSection:CoherentImagesElements\n\nOnce the style has been determined, the next step is to create descriptions for possible recurring elements in the text. These will typically be people, animals, objects or locations.\n\nThe ``Elements'' step consists of two substeps. In the first, the AI is instructed to create a list of elements. This is done using the prompt template shown in Figure~[REF:Figure:FindElementNames], which takes as input the text, the already generated style description, and possible background information. So far, this operation seems rather unreliable, so we provide controls allowing the user to add or delete elements.\n\nIn the second substep, the AI is instructed to create a description for each element, and then produce an illustrative image to give the user feedback. The prompt template used to create an element description is shown in Figure~[REF:Figure:CreateE…",
      "analysis": {
        "section_summary": "Describes the Elements phase of coherent image creation: after a style is set, the AI drafts a list of recurring elements (people, animals, objects, locations) using a prompt template, though users may need to add or remove items due to unreliability. For each element, a prompt guides the AI to write a detailed description and produce an illustrative image, exemplified with the character Whiskers from \"The Cat and the Bat.\"",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.51,
            "rationale": "The edit_images_v2 view manages coherent images for a project, including configuring and triggering element list and description/image generation as part of the coherent image pipeline."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.36,
            "rationale": "Provides monitoring for asynchronous generation tasks such as creating elements and their images within the coherent images workflow described."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.3,
            "rationale": "Returns status for generation tasks including elements, aligning with tracking the two-step element creation process."
          }
        ],
        "concept_tags": [
          "coherent images",
          "elements generation",
          "recurring elements",
          "prompt template",
          "AI-generated descriptions",
          "illustrative images",
          "style description",
          "user controls for elements",
          "The Cat and the Bat example"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15192,
        "completion_tokens": 1427,
        "total_tokens": 16619,
        "estimated_cost_usd": 0.03326
      },
      "has_tex": true
    },
    {
      "section_id": "e3b5150233e9f7e8",
      "level": 3,
      "title": "Pages",
      "label": "Section:CoherentImagesPages",
      "plain_text_len": 960,
      "plain_text_excerpt": "Pages \nSection:CoherentImagesPages\n\nWith the style description and the element descriptions in place, the final step is to generate the actual page images. This again breaks down into two substeps. In the first, the AI is told to identify the relevant elements from\nthe full list; in the second, the template is passed the text, background, \nstyle description and selected elements, and instructed to create\nthe description for the page image. \n\nThe template for the first operation is shown in Figure~[REF:Figure:FindRelevantElements], and is straightforward.\nThe second substep is the more interesting one. We originally implemented it with the template from Figure~[REF:Figure:CreatePageImageDescription], where we pass in the element descriptions, that is to say pieces of text, and instruct the AI to create page descriptions from them which in some way incorporate these pieces of text. Figure~…",
      "analysis": {
        "section_summary": "Describes the final stage of coherent image creation for a project: after defining style and element descriptions, page images are generated in two substeps. First, an AI identifies which elements are relevant for a given page from the full list. Then a template combines the page text, background, style description, and selected element descriptions to produce a detailed page image description. The first template is straightforward; the second involves incorporating the provided element texts into the page description.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.9,
            "rationale": "This view handles configuring and managing coherent images, including generating styles, elements, and pages, matching the described page image generation workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.82,
            "rationale": "Provides monitoring and task triggering for coherent image generation substeps such as pages, aligning with the two-step page image creation process."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.82,
            "rationale": "Supplies status endpoints for coherent image generation tasks, relevant to monitoring the page image generation steps."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.45,
            "rationale": "Offers a review interface for coherent page images within the Simple C-LARA wizard, tangentially related to generated page descriptions."
          }
        ],
        "concept_tags": [
          "coherent images",
          "page image generation",
          "element selection",
          "style description",
          "AI prompt templates",
          "image description synthesis",
          "template-based generation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14848,
        "completion_tokens": 834,
        "total_tokens": 15682,
        "estimated_cost_usd": 0.0269
      },
      "has_tex": true
    },
    {
      "section_id": "49ee4e0ef5c6682a",
      "level": 3,
      "title": "Using a multimodal model: Gemini Imagen 3",
      "label": "Section:CoherentImagesPagesImagen",
      "plain_text_len": 3871,
      "plain_text_excerpt": "Using a multimodal model: Gemini Imagen 3 \nSection:CoherentImagesPagesImagen\n\nHaving created several dozen C-LARA texts using DALL-E-3, our overall conclusion is that it is in general not capable of maintaining adequately coherent images. The same element is often not presented similarly enough across multiple images, even if the element description generated by the AI is quite detailed and DALL-E-3 appears to have followed it. Words do not appear to provide a sufficiently fine-grained way to capture an image.\n\nWe also experimenting with using the Gemini Image 3 model as an alternative to DALL-E-3; C-LARA now supports both image generators. The important difference is that Imagen 3 is multimodal; instructions can include references to web-accessible images as an alternative to descriptions of images, with the images passed as URLs. These are interpreted directly. \n\nA practical problem wi…",
      "analysis": {
        "section_summary": "The section explains that DALL‑E‑3 often fails to produce coherent visual consistency across multiple C‑LARA pages, so C‑LARA now also supports Google’s multimodal Imagen 3, which can take referenced image URLs as well as text prompts. Because Imagen 3 restricts depictions of people (especially children), C‑LARA uses a hybrid strategy: textual descriptions for human elements and image references otherwise. Rapid advances such as OpenAI’s forthcoming ChatGPT Images are expected to solve coherence. An example page text and a detailed AI‑generated stained‑glass style description for a cat at a windowsill illustrate the workflow.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.71,
            "rationale": "This view handles configuring and generating coherent page images with different models (including Imagen 3) for a project."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.46,
            "rationale": "Provides task monitoring for coherent image generation jobs such as those using Imagen 3."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.44,
            "rationale": "Returns status for coherent image generation tasks, relevant when using Imagen 3."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.32,
            "rationale": "Serves generated coherent image assets, applicable to reviewing Imagen 3 outputs."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.3,
            "rationale": "Displays an overview of generated coherent images for a project, relevant to viewing Imagen 3 results."
          }
        ],
        "concept_tags": [
          "Imagen 3",
          "DALL-E-3",
          "multimodal image generation",
          "coherent images",
          "C-LARA images",
          "image consistency",
          "image model restrictions",
          "hybrid prompt strategy",
          "OpenAI Images in ChatGPT",
          "page image description",
          "stained glass style",
          "image URLs"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15432,
        "completion_tokens": 744,
        "total_tokens": 16176,
        "estimated_cost_usd": 0.02673
      },
      "has_tex": true
    },
    {
      "section_id": "c34d453aea4a6a15",
      "level": 3,
      "title": "Uploading Images",
      "label": "Section:CoherentImagesUploadingImages",
      "plain_text_len": 1776,
      "plain_text_excerpt": "Uploading Images \nSection:CoherentImagesUploadingImages\n\nSometimes the user has a specific idea about how an element or image should appear, but cannot easily express it verbally. We consequently include a mechanism that allows the user to upload their own image and use it as a model. The AI analyses the image to produce a description, then uses the description in the same way as other ones.\n\nThe template used to analyse an uploaded element image is shown in Figure~[REF:Figure:AnalyseUploadedImage]. Figure~[REF:Figure:UploadedImageExample] presents an example of using an uploaded image.\n\nfigure\n\nUploaded image\n\n10pt\n\nGenesis1/Gulapilil.jpg\n\nAI-generated element image description\nverbatim\nI'm unable to identify or describe real people in images. However, \nfor creating an artistic interpretation in DALL-E, you can describe \nthe character as:\n\n- **Apparent Age**: Elderly\n- **Gender**: Male\n…",
      "analysis": {
        "section_summary": "This section explains that when users have a specific visual idea that is hard to verbalise, they can upload their own image to guide coherent image generation. The system analyses the uploaded element image to produce a descriptive prompt, which is then used like other element descriptions. An example shows an uploaded stock photo, the AI-generated description stressing it is only inspirational, and the resulting page image.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.78,
            "rationale": "The coherent images editor handles configuring and managing elements and pages, including using uploaded images as models for elements."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.54,
            "rationale": "Monitoring endpoint for coherent image generation tasks started after uploading and analysing an element image."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.52,
            "rationale": "Status endpoint related to coherent image generation workflows that may include uploaded element images."
          }
        ],
        "concept_tags": [
          "coherent images",
          "element image upload",
          "AI image description",
          "model image",
          "DALL-E prompt",
          "page image generation",
          "Pitjantjatjara Genesis"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14762,
        "completion_tokens": 681,
        "total_tokens": 15443,
        "estimated_cost_usd": 0.0252625
      },
      "has_tex": true
    },
    {
      "section_id": "abc96b7ca277b749",
      "level": 3,
      "title": "Reviewing Images",
      "label": "Section:CoherentImagesReviewing",
      "plain_text_len": 3489,
      "plain_text_excerpt": "Reviewing Images \nSection:CoherentImagesReviewing\n\nThe image generation process is still very far from reliable; in particular, DALL-E-3 hardly ever follows instructions exactly and often produces images which are incompatible with them. As noted below, better models are in the process of being released, but until they are the most realistic strategy is to generate multiple images and then either select the best one, or to edit the descriptions and regenerate in an attempt to find a way of expressing the instructions which resonated better with DALL-E-3.\n\nWe support three different methods. In the first, we attempt to select the best descriptions and images using automatic AI-based filtering. In the second, we let the user select and regenerate directly; and in the third, we allow a community to select and regenerate using a democratic process. We present the three methods in turn, then …",
      "analysis": {
        "section_summary": "The section explains that DALL-E-3 often fails to follow instructions, so multiple images are generated and reviewed using three approaches: automatic AI filtering comparing vision model interpretations to descriptions; manual user review via the \"Edit Images and Pages\" interface with upvotes/downvotes, regenerating from edited descriptions, or uploading their own images; and community-based review with separate member and coordinator views where costly AI generation requests are queued until a coordinator approves and executes them.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.56,
            "rationale": "Implements the Edit Images and Pages interface described for user reviewing, allowing selection, regeneration, and uploading of images for coherent sets."
          },
          {
            "url_name": "community_review_images",
            "confidence": 0.51,
            "rationale": "Supports community member review of project images with controls similar to user reviewing but queuing AI requests for coordinator approval."
          },
          {
            "url_name": "community_organiser_review_images",
            "confidence": 0.51,
            "rationale": "Provides the coordinator view to approve, deny, and run queued community requests for image generation described in community reviewing."
          },
          {
            "url_name": "community_review_images_for_page",
            "confidence": 0.46,
            "rationale": "Allows drilling down to review and manage images on a specific page as part of the community reviewing workflow."
          },
          {
            "url_name": "execute_community_requests_for_page_monitor",
            "confidence": 0.36,
            "rationale": "Monitors execution of approved community generation requests that coordinators run after review."
          }
        ],
        "concept_tags": [
          "coherent images",
          "AI-based image review",
          "image generation",
          "user review",
          "image regeneration",
          "upvote/downvote",
          "community review",
          "coordinator approval",
          "crowdsourced editing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15371,
        "completion_tokens": 962,
        "total_tokens": 16333,
        "estimated_cost_usd": 0.02883375
      },
      "has_tex": true
    },
    {
      "section_id": "fe3d2f847abdb749",
      "level": 3,
      "title": "Summary and further directions",
      "label": "Section:CoherentImagesSummary",
      "plain_text_len": 1342,
      "plain_text_excerpt": "Summary and further directions \nSection:CoherentImagesSummary\n\nThe previous sections have presented the new image generation functionality. Users construct images by first defining a style, then the visual elements common to several images, and finally the images themselves. In each case, the process is first to instruct the AI to create a suitable text description, and then pass this description to DALL-E-3 to produce images. In practice, quality can often be substantially improved by manually filtering the images and/or editing the AI-generated descriptions. An interesting point is that the descriptions do not need to be created in English. \n\nWe had three main goals when developing the functionality. The set of images for a text should be\n\nenumerate\n quick to create.\n coherent in terms of style (images should look stylistically similar);\n coherent in terms of content (if the same item …",
      "analysis": {
        "section_summary": "The section recaps the new coherent image generation workflow: users define an overarching style, then shared visual elements, and finally individual images, each step using AI to draft a text description which is then fed to DALL-E-3. Quality can be improved by manual filtering or editing descriptions, which need not be in English. The goals were rapid creation, stylistic coherence, and content consistency of recurring items; the current approach achieves the first two but struggles with consistent depiction of the same item across images, a limitation expected to ease as more capable models become available via API.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.74,
            "rationale": "This view is the main interface for configuring coherent images by defining styles, elements, and pages and triggering AI description and DALL-E generation, matching the described workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.63,
            "rationale": "Provides monitoring/status for the coherent images v2 generation tasks, relevant to the new functionality outlined in the summary."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.6,
            "rationale": "Supplies status updates for coherent image generation, aligned with the discussed coherent image creation process."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "style definition",
          "visual elements",
          "AI-generated descriptions",
          "DALL-E-3",
          "image quality",
          "manual editing",
          "stylistic coherence",
          "content consistency",
          "future AI models"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14641,
        "completion_tokens": 692,
        "total_tokens": 15333,
        "estimated_cost_usd": 0.02522125
      },
      "has_tex": true
    },
    {
      "section_id": "9a5713711138a2d0",
      "level": 2,
      "title": "Simple C-LARA",
      "label": "Section:SimpleCLARA",
      "plain_text_len": 709,
      "plain_text_excerpt": "Simple C-LARA \nSection:SimpleCLARA\n\nSimple C-LARA provides a wizard-style interface which streamlines the process of creating a C-LARA text. The user does not have all the options available in Advanced C-LARA, but the process is much easier to understand.\n\nThe initial version of Simple C-LARA, described in the second progress report, has now been extended to include more functionality. In particular, annotation for MWEs and sentence translations are added automatically, and the user has the option of requesting images created using the coherent image set functionality from .\n\nAppendix~[REF:Section:UsingSimpleCLARA] presents an example of using Simple C-LARA to create a text with a coherent image set.",
      "analysis": {
        "section_summary": "Simple C-LARA offers a streamlined wizard to create C-LARA texts with fewer options than the advanced interface. The updated version now auto-generates multiword expression annotations and sentence translations, and lets users request coherent image sets for their texts.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.9,
            "rationale": "This is the main Simple C-LARA wizard view that drives the streamlined creation process described."
          },
          {
            "url_name": "simple_clara_status",
            "confidence": 0.76,
            "rationale": "Provides status updates for actions run through the Simple C-LARA wizard, relevant to the streamlined workflow."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.75,
            "rationale": "Monitors longer-running tasks initiated from the Simple C-LARA wizard, aligning with the described interface."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.7,
            "rationale": "Handles asynchronous image generation requests within Simple C-LARA, matching the new option to request coherent image sets."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.7,
            "rationale": "Returns status for image generation tasks requested via Simple C-LARA, supporting the new image functionality."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.68,
            "rationale": "Monitors style image generation in Simple C-LARA, related to coherent image sets."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.68,
            "rationale": "Reports status for style-related image generation within Simple C-LARA."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.67,
            "rationale": "Monitors element-level actions (e.g., annotations) within the Simple C-LARA wizard, relevant to automatic MWE/sentence annotations."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.67,
            "rationale": "Provides status for element-level tasks kicked off by Simple C-LARA, such as auto-annotations."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.64,
            "rationale": "Supports reviewing coherent images per page within the Simple C-LARA flow, matching the new image option."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.64,
            "rationale": "Enables reviewing coherent images for specific elements in Simple C-LARA, tied to the new image feature."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.64,
            "rationale": "Allows review of coherent image styles in Simple C-LARA, aligning with the option to request coherent image sets."
          }
        ],
        "concept_tags": [
          "Simple C-LARA",
          "wizard interface",
          "automatic annotation",
          "multiword expressions",
          "sentence translation",
          "coherent images",
          "streamlined text creation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14889,
        "completion_tokens": 943,
        "total_tokens": 15832,
        "estimated_cost_usd": 0.02804125
      },
      "has_tex": true
    },
    {
      "section_id": "26ef66d8cd614a8b",
      "level": 2,
      "title": "Parallelism",
      "label": "Section:Parallelism",
      "plain_text_len": 1207,
      "plain_text_excerpt": "Parallelism \nSection:Parallelism\n\nWe have implemented parallelism using the Python asyncio package, which has a large impact on speed of processing. Parallelism is currently used in two places:\n\ndescription\n\n Most of the annotation operations (the second phase of segmentation, Multi-Word Expressions, lemma tagging, glossing) are now parallelised. In all of these cases, the text has already been divided into segments, and all the segments are annotated simultaneously.\n\n Similarly, we have parallelised production of descriptions and images for the ``elements'' and ``pages'' phases of coherent image processing. As with linguistic annotation, we process all the elements, or all the pages, at the same time.\n\ndescription\n\nIn all of the above, parallel processing is realised using some version of the standard asyncio pattern shown in Figure~[REF:Figure:ParallelismPseudo].\n\nfigure\nverbatim\nasync…",
      "analysis": {
        "section_summary": "The section explains that C‑LARA now leverages Python’s asyncio to parallelise key processing steps, greatly improving speed. Linguistic annotations such as second-phase segmentation, multi-word expressions, lemma tagging and glossing are run concurrently across pre-segmented text segments. Likewise, the generation of descriptions and images for the element and page phases of coherent image processing is carried out in parallel. All these use a standard asyncio gather pattern to dispatch and collect tasks.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.6,
            "rationale": "This view drives multi-word expression tagging, one of the annotation steps now parallelised with asyncio over segments."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.56,
            "rationale": "Lemma tagging is cited as parallelised; this endpoint handles creation of lemma-tagged text versions, benefiting from the asyncio pattern."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.52,
            "rationale": "Coherent image processing of elements and pages is parallelised; this view manages the generation of coherent images where parallel tasks are dispatched."
          }
        ],
        "concept_tags": [
          "parallelism",
          "asyncio",
          "annotation",
          "segmentation",
          "multi-word expressions",
          "lemma tagging",
          "glossing",
          "coherent image processing",
          "concurrent tasks"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14968,
        "completion_tokens": 1109,
        "total_tokens": 16077,
        "estimated_cost_usd": 0.0298
      },
      "has_tex": true
    },
    {
      "section_id": "47cea9a83df79ba4",
      "level": 2,
      "title": "Better support for Indigenous languages",
      "label": "Section:IndigenousLanguages",
      "plain_text_len": 2694,
      "plain_text_excerpt": "Better support for Indigenous languages \nSection:IndigenousLanguages\n\nInitial experiments using C-LARA to construct texts for Indigenous languages, described in the preceding report, showed that it worked much less well there than with the large languages it was originally designed for. There were two main problems. First, since the AI does not know the languages concerned, it is unable to write or annotate the texts, and this work must thus be performed manually. The human annotator is working with multiple parallel versions of the text, annotated in different ways (glossed, lemma tagged, translated, etc); in practice, the different versions often get out of sync, and it is laborious to then correct the divergences.\n\nAs previously noted, a C-LARA project maintains multiple parallel versions of the text: plain text, segmented text (i.e.\\ text divided into pages and sentence-like units), …",
      "analysis": {
        "section_summary": "The section explains how C-LARA has been adapted to better handle Indigenous languages, where AI cannot provide annotations. Since human annotators maintain multiple parallel text versions (plain, segmented, glossed, translated, lemma-tagged, multi-word expressions) that can easily drift out of sync, the \"Edit Pages and Images\" interface now shows all materials for a page together rather than separated by type, making divergences easier to spot. On save, the platform automatically checks annotation syntax and uses smart diffs to minimally correct divergences by inserting placeholders for missing items. Early testing shows significant usability improvements over the original design.",
        "relevant_views": [
          {
            "url_name": "edit_images",
            "confidence": 0.7,
            "rationale": "The section highlights a reorganised \"Edit Pages and Images\" screen that shows all annotated versions per page and validates them on save; this aligns with the edit_images view which handles editing page texts and images together."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.5,
            "rationale": "The newer coherent images interface also provides an \"Edit Images and Pages\" screen, which may encompass the redesigned page-centric editing experience described for Indigenous language support."
          }
        ],
        "concept_tags": [
          "Indigenous languages",
          "manual annotation",
          "parallel text versions",
          "glossed text",
          "segmented text",
          "translation annotations",
          "lemma tagging",
          "multi-word expressions",
          "Edit Pages and Images",
          "page-based interface",
          "annotation syntax checking",
          "smart diff correction",
          "usability improvements"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14928,
        "completion_tokens": 938,
        "total_tokens": 15866,
        "estimated_cost_usd": 0.02804
      },
      "has_tex": true
    },
    {
      "section_id": "951e5cb7958c0c44",
      "level": 1,
      "title": "Evaluating linguistic annotation and image generation",
      "label": "Section:SoftwareComponent",
      "plain_text_len": 709,
      "plain_text_excerpt": "Evaluating linguistic annotation and image generation\nSection:SoftwareComponent\n\nIn the second progress report, we described how GPT-4 (and subsequently GPT-4 Turbo) was evaluated on four core annotation tasks---writing, segmenting, glossing, and lemma/POS tagging---across multiple languages and text genres. Here, we present a third round of experiments, focusing initially on English, where we again see a notable reduction in error rates(Table~[REF:Table:AnnotationPerformanceEnglish]). Beyond annotation, we have also started to systematically assess the new coherent image generation functionality using a similar multi-text design and a structured questionnaire. Below, we present our initial findings.",
      "analysis": {
        "section_summary": "The section reports a third round of experiments evaluating GPT-4/Turbo on core linguistic annotation tasks—writing, segmentation, glossing, and lemma/POS tagging—initially on English texts, showing reduced error rates. It also outlines initial systematic assessment of the new coherent image generation feature using a multi-text design and a structured questionnaire.",
        "relevant_views": [
          {
            "url_name": "create_segmented_text",
            "confidence": 0.44,
            "rationale": "Segmentation is one of the evaluated annotation tasks; this view handles creating segmented text versions, aligning with the functionality under evaluation."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.44,
            "rationale": "Glossing is explicitly mentioned as a core annotation task; this endpoint supports creating glossed text versions relevant to the evaluation."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.42,
            "rationale": "Lemma/POS tagging is part of the reported experiments; this view manages creating lemma-tagged texts tied to that functionality."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.38,
            "rationale": "The section discusses evaluating coherent image generation; this endpoint monitors coherent image generation tasks within a project."
          },
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.41,
            "rationale": "Image quality is assessed via a structured questionnaire; this view initializes image questionnaires on projects for systematic evaluation."
          }
        ],
        "concept_tags": [
          "GPT-4",
          "annotation evaluation",
          "segmentation",
          "glossing",
          "lemma tagging",
          "POS tagging",
          "English",
          "coherent image generation",
          "structured questionnaire",
          "error rates"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14514,
        "completion_tokens": 994,
        "total_tokens": 15508,
        "estimated_cost_usd": 0.0280825
      },
      "has_tex": true
    },
    {
      "section_id": "2370879539269119",
      "level": 2,
      "title": "Third round of linguistic annotation evaluation",
      "label": "Section:ThirdRoundAnnotation",
      "plain_text_len": 2971,
      "plain_text_excerpt": "Third round of linguistic annotation evaluation\nSection:ThirdRoundAnnotation\n\nTo gauge ongoing progress in GPT-4's linguistic performance, we replicated the experiments from 5.1 of the second progress report a third time in March 2025, using the March 2025 GPT-4o version and the current version of C-LARA, incorporating in particular MWE annotation and segment translations. Table~[REF:Table:AnnotationPerformanceEnglish] shows the updated results for English, measured as before on six texts defined by the prompts in Table~[REF:Table:Prompts] and the same tasks (segmentation, French glossing, lemma/POS tagging). Compared to both the September 2023 and March 2024 data, the error rates have decreased further, with an average of only 2.0\\% for glossing and 0.8\\% for lemma-tagging across the six texts. This is a very substantial improvement relative to the original GPT-4, which had glossing err…",
      "analysis": {
        "section_summary": "The section reports a March 2025 replication of earlier GPT‑4 linguistic annotation experiments using the updated GPT‑4o model and C‑LARA, now including MWE annotation and segment translations. Six English texts generated from predefined prompts were annotated for segmentation, French glossing, and lemma/POS tagging. Error rates have further decreased compared to September 2023 and March 2024, averaging around 2.0% for glossing and 0.8% for lemma tagging across the texts, marking a substantial improvement over the original GPT‑4.",
        "relevant_views": [],
        "concept_tags": [
          "GPT-4o",
          "linguistic annotation",
          "segmentation",
          "glossing",
          "lemma tagging",
          "POS tagging",
          "error rates",
          "evaluation",
          "English texts",
          "C-LARA",
          "MWE annotation",
          "segment translations",
          "prompts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15791,
        "completion_tokens": 533,
        "total_tokens": 16324,
        "estimated_cost_usd": 0.02506875
      },
      "has_tex": true
    },
    {
      "section_id": "5175e3af3b4c79b0",
      "level": 2,
      "title": "Evaluating coherent image generation",
      "label": "Section:ImageGenerationEvaluation",
      "plain_text_len": 8094,
      "plain_text_excerpt": "Evaluating coherent image generation\nSection:ImageGenerationEvaluation\n\nAnother key focus of this third phase is a systematic evaluation of the coherent image generation framework described in . The goal is to produce sets of illustrations for short texts in a way that observe constraints on stylistic coherence, narrative coherence, and cultural relevance. We have again defined six text scenarios of varied types, this time for image creation, as shown in Table~[REF:Table:ImagePrompts].\n\ntable*[bh!]\n Core prompts used to create image evaluation texts. More information was added to specify the length of the text in pages (typically 10--20 pages) and the fact that one illustration would be created per page.\n Table:ImagePrompts\n\ntabularll\n\n1cLabel & 1cPrompt\\\\\n\nDI & A small picture dictionary.\\\\\n\nSC & A scientific/technical explanation accessible to a bright ten-year-old.\\\\\n\nAN & A friendshi…",
      "analysis": {
        "section_summary": "This section reports on a systematic evaluation of C‑LARA’s coherent image generation workflow. Six English text scenarios were illustrated using style specifications and multiple candidate images from DALL‑E‑3 and Imagen 3. Evaluators rated each page image on correspondence to text, stylistic consistency, element coherence, cultural appropriateness, and visual appeal via a 5‑point Likert questionnaire built into C‑LARA. Revised image sets, often by selecting alternate generations, improved scores for four scenarios (dictionary, animals, robot, typical day), while traditional and science/technical texts showed style/coherence issues and fundamental failures in diagram accuracy.",
        "relevant_views": [
          {
            "url_name": "image_questionnaire_project_list",
            "confidence": 0.76,
            "rationale": "Lists projects with image questionnaires, aligning with the described questionnaire-based image evaluation."
          },
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.74,
            "rationale": "Starts an image questionnaire session to rate page images, matching the Likert-scale evaluation tool used."
          },
          {
            "url_name": "image_questionnaire_item",
            "confidence": 0.73,
            "rationale": "Presents page-by-page rating forms for images, directly relevant to collecting the reported scores."
          },
          {
            "url_name": "image_questionnaire_summary",
            "confidence": 0.71,
            "rationale": "Provides summary results of image quality questionnaires akin to the aggregated performance table."
          },
          {
            "url_name": "image_questionnaire_summary_csv",
            "confidence": 0.69,
            "rationale": "Supports exporting questionnaire results, relevant to compiling the evaluation tables."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "stylistic coherence",
          "narrative coherence",
          "cultural relevance",
          "Likert scale evaluation",
          "image quality questionnaire",
          "DALL-E-3",
          "Imagen 3",
          "alternate image selection",
          "style specification",
          "element coherence",
          "visual appeal",
          "traditional story illustration",
          "scientific diagram limitations"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16815,
        "completion_tokens": 669,
        "total_tokens": 17484,
        "estimated_cost_usd": 0.02770875
      },
      "has_tex": true
    },
    {
      "section_id": "e58a57b4e316e0a2",
      "level": 3,
      "title": "Looking ahead: new image generation capabilities",
      "label": "Section:NewImageGenerationCapabilities",
      "plain_text_len": 1991,
      "plain_text_excerpt": "Looking ahead: new image generation capabilities\nSection:NewImageGenerationCapabilities\n\nIt appears, however, that the issues just mentioned will soon be resolved. The ``Images in ChatGPT''https://www.theverge.com/openai/635118/chatgpt-sora-ai-image-generation-chatgpt functionality, released in late Mar 2025 and so far only available through the web interface, incorporates reasoning into the image generation process and opens up dramatic new possibilities. Figure~[REF:Figure:ProblemsInScienceTechnical] shows the result when the ``simple circuit'' request is submitted; it is already nearly correct on the first try. \n\nfigure[h!]\n\n ImageEvaluation/RealisticCircuit.png\nGenerating the basic circuit image from Figure~[REF:Figure:ProblemsInScienceTechnical] using the ``Images in ChatGPT'' functionality released late March 2025. Although not perfect, this image, created without any tweaking of t…",
      "analysis": {
        "section_summary": "The section notes a breakthrough with the late March 2025 web-only \"Images in ChatGPT\" feature, which integrates reasoning into image generation. Initial prompts already produce near-correct technical diagrams and more coherent illustrations for stories like \"The Brave Little Tailor\". Although API access is pending, the rapid progress suggests C-LARA will soon be able to generate good coherent image sets for similar texts.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.58,
            "rationale": "The section discusses improved coherent image generation capabilities, aligning with the view used to configure and manage coherent images for projects."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.52,
            "rationale": "Monitoring the status of coherent image generation relates to assessing new image generation performance described in the section."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.5,
            "rationale": "A monitor for coherent image generation tasks is relevant to evaluating and adopting the new image generation capabilities."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.46,
            "rationale": "Reviewing coherent images at the page level ties to the examples of story illustrations using enhanced image generation."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.44,
            "rationale": "Element-level coherent image review supports checking quality of generated diagrams like the circuit example."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.41,
            "rationale": "Serving overviews of coherent image sets is pertinent to showcasing improved generated images."
          }
        ],
        "concept_tags": [
          "Images in ChatGPT",
          "image generation",
          "coherent images",
          "OpenAI",
          "API availability",
          "technical diagrams",
          "illustration quality"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15143,
        "completion_tokens": 652,
        "total_tokens": 15795,
        "estimated_cost_usd": 0.02544875
      },
      "has_tex": true
    },
    {
      "section_id": "dba058815f37206f",
      "level": 3,
      "title": "GPT-4o on the Future of Coherent Image Generation in C-LARA",
      "label": null,
      "plain_text_len": 2435,
      "plain_text_excerpt": "GPT-4o on the Future of Coherent Image Generation in C-LARA\n\nRecent advances in image generation mark a turning point for C-LARA’s ability to illustrate narrative and educational texts. Until now, one of the most significant limitations in using generative AI for visual support has been the difficulty of maintaining coherence across images — keeping characters, props, and style consistent from one page to the next. Current API-based systems like DALL-E 3 and Imagen 3 offer strong single-image generation, but they struggle to produce visually coherent multi-image sequences, particularly for narratives.\n\nIn this week’s experiments using the ChatGPT interface, however, it has become clear that this barrier is beginning to fall. The latest image generation model, Images in ChatGPT, can now retain visual identity across multiple images within a session, following detailed reference descriptio…",
      "analysis": {
        "section_summary": "The section reports that recent advances, notably the new Images in ChatGPT (GPT-4o), now maintain visual identity across multiple images, overcoming prior coherence limits seen in APIs like DALL-E 3. Tests on \"The Brave Little Tailor\" showed consistent characters, proportions, and environments, validating C-LARA’s design for reusable visual elements and pluggable references. These capabilities promise rapid creation of coherent, pedagogically useful illustrated texts and open new genres such as evolving diagrams, multilingual stories, and culturally faithful tales.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.71,
            "rationale": "Provides configuration and generation workflow for coherent images across pages, matching the section’s focus on visual consistency and reusable elements."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.66,
            "rationale": "Offers monitoring for coherent image generation tasks (style, elements, pages), directly relevant to managing multi-image consistency."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.62,
            "rationale": "Supplies status for coherent image generation tasks, aligned with the need to track multi-image coherence workflows."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.52,
            "rationale": "Serves overviews of generated coherent images, relevant for reviewing visual continuity across pages."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.5,
            "rationale": "Delivers individual coherent image files, pertinent to accessing consistent multi-image outputs."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.45,
            "rationale": "Supports reviewing coherent images at the page level within Simple C-LARA, useful for checking narrative consistency."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.42,
            "rationale": "Allows review of coherent images tied to shared elements, resonating with the section’s emphasis on reusable visual references."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.4,
            "rationale": "Enables reviewing stylistic coherence, which is central to consistent multi-image generation."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "visual consistency",
          "character continuity",
          "narrative illustration",
          "Images in ChatGPT",
          "GPT-4o",
          "DALL-E 3 limitations",
          "reusable visual elements",
          "pluggable architecture",
          "pedagogical picture books",
          "multilingual stories",
          "technical diagrams"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14925,
        "completion_tokens": 958,
        "total_tokens": 15883,
        "estimated_cost_usd": 0.02823625
      },
      "has_tex": true
    },
    {
      "section_id": "a8df799d0f0a4ac2",
      "level": 1,
      "title": "Using C-LARA with specific languages",
      "label": "Section:SpecificLanguages",
      "plain_text_len": 62,
      "plain_text_excerpt": "Using C-LARA with specific languages\nSection:SpecificLanguages",
      "analysis": {
        "section_summary": "This section likely outlines how to apply C‑LARA to particular languages, touching on language-specific resources and settings needed to process texts in those languages.",
        "relevant_views": [
          {
            "url_name": "edit_phonetic_lexicon",
            "confidence": 0.35,
            "rationale": "Editing phonetic lexicons is a language-specific task that supports proper processing for certain languages."
          },
          {
            "url_name": "edit_prompt",
            "confidence": 0.32,
            "rationale": "Managing annotation prompt templates per language helps tailor C‑LARA to specific language characteristics."
          },
          {
            "url_name": "bundle_list",
            "confidence": 0.28,
            "rationale": "Localisation bundles reflect language-specific translations that may be relevant when using C‑LARA with different languages."
          },
          {
            "url_name": "create_pinyin_tagged_text",
            "confidence": 0.27,
            "rationale": "Pinyin tagging is specific to Chinese, illustrating how C‑LARA handles language-specific text annotations."
          },
          {
            "url_name": "create_phonetic_text",
            "confidence": 0.27,
            "rationale": "Generating phonetic texts is useful for languages requiring phonetic support, aligning with language-specific usage."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.25,
            "rationale": "Glossed text creation is a language-dependent annotation feature that can vary across languages."
          }
        ],
        "concept_tags": [
          "language support",
          "language-specific settings",
          "phonetic lexicon",
          "annotation prompts",
          "pinyin",
          "glossed text"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15087,
        "completion_tokens": 518,
        "total_tokens": 15605,
        "estimated_cost_usd": 0.02403875
      },
      "has_tex": true
    },
    {
      "section_id": "ad35b4756c172ce8",
      "level": 2,
      "title": "Icelandic and Mandarin (University of Iceland)",
      "label": "Section:IcelandicAndMandarin",
      "plain_text_len": 1064,
      "plain_text_excerpt": "Icelandic and Mandarin (University of Iceland)\nSection:IcelandicAndMandarin\n\nIn Summer 2024The version of C-LARA used for the experiments described in this section (Jun--Aug 2024) was significantly less advanced than the one used in most of the rest of the report (Feb--Mar 2025)., three university students worked with C-LARA to create illustrated short stories with translations to English and Mandarin Chinese that would be suitable for learning Icelandic as a second language at different learner levels. The aim was to evaluate the effectiveness of the C-LARA platform for language learning, focusing on its potential for less commonly spoken languages such as Icelandic. Over three months June-August, these students used C-LARA to create 30 projects, of which nine were fully completed and analyzed. The study documented accuracy rates, processing time, and costs at various stages of project …",
      "analysis": {
        "section_summary": "Three university students used an earlier 2024 version of C-LARA over June–August to build illustrated Icelandic short stories with English and Mandarin translations for different learner levels. They created 30 projects (nine fully completed and analyzed), tracking accuracy, processing time and costs, and turned three into picture books for future learning-effectiveness studies.",
        "relevant_views": [],
        "concept_tags": [
          "Icelandic as a second language",
          "Mandarin translation",
          "illustrated short stories",
          "project creation",
          "language learning materials",
          "evaluation of accuracy and cost",
          "processing time",
          "picture books",
          "less commonly spoken languages"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15070,
        "completion_tokens": 540,
        "total_tokens": 15610,
        "estimated_cost_usd": 0.0242375
      },
      "has_tex": true
    },
    {
      "section_id": "2e0894b591d448ee",
      "level": 3,
      "title": "Key findings on language learning with C-LARA",
      "label": null,
      "plain_text_len": 625,
      "plain_text_excerpt": "Key findings on language learning with C-LARA\n\n The study explored two primary use cases: learning Icelandic through English and Mandarin Chinese and learning Mandarin Chinese through Icelandic. It found that AI was highly efficient in generating learning resources, though it required much of human intervention to refine content for fluency and cultural accuracy especially in Icelandic. Interestingly, only minor adjustments needed to be done in Mandarin Chinese. This human-in-the-loop process, where AI-generated content was iteratively reviewed and edited, proved essential in achieving high-quality learning materials.",
      "analysis": {
        "section_summary": "The study compared using C-LARA for Icelandic and Mandarin, finding AI very efficient at generating learning resources but requiring significant human review to ensure fluency and cultural appropriateness for Icelandic, with only minor edits needed for Mandarin; iterative human-in-the-loop refinement was key to producing high-quality materials.",
        "relevant_views": [],
        "concept_tags": [
          "human-in-the-loop",
          "AI-generated learning materials",
          "Icelandic language learning",
          "Mandarin Chinese",
          "cultural accuracy",
          "fluency",
          "content refinement",
          "language learning resources"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14927,
        "completion_tokens": 523,
        "total_tokens": 15450,
        "estimated_cost_usd": 0.02388875
      },
      "has_tex": true
    },
    {
      "section_id": "962922831958a869",
      "level": 3,
      "title": "Project outcomes and challenges",
      "label": null,
      "plain_text_len": 1280,
      "plain_text_excerpt": "Project outcomes and challenges\n\nEach student focused on different learner profiles and linguistic challenges:\n\nStudent 1 created 17 stories for young learners of Icelandic through English. While seven were experimental, the remaining 10 were thematically structured, covering topics like Norse mythology and AI-generated narratives. A significant challenge was aligning Common European Framework for Languages (CEFR) levels with children’s literature, as texts aimed at young readers tend to fall below B2, despite their complexity in terms of storytelling.\nStudent 2 developed nine projects, including adaptations of the same story for different proficiency levels and a text for teenage learners. Real-world themes, such as Reykjavík’s volcanic eruptions and the Arctic midnight sun, were explored. However, technical issues with multi-word-expression (MWE) processing for Icelandic language preve…",
      "analysis": {
        "section_summary": "Three University of Iceland students produced Icelandic and Mandarin learning projects with varied themes and proficiency adaptations. Challenges included aligning CEFR levels with children’s literature, technical failures in multi‑word expression processing that blocked uploads, and incorrect text segmentation of Icelandic words requiring manual fixes.",
        "relevant_views": [
          {
            "url_name": "create_cefr_level",
            "confidence": 0.41,
            "rationale": "Aligning texts to appropriate CEFR levels was a noted challenge, making the CEFR-level creation/annotation view potentially pertinent."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.62,
            "rationale": "Student 2 faced technical issues with multi‑word expression processing for Icelandic, which relates to MWE-tagging functionality."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.56,
            "rationale": "Text segmentation errors in Icelandic that required manual correction suggest use of the segmentation creation/editing view."
          }
        ],
        "concept_tags": [
          "CEFR alignment",
          "multi-word expressions",
          "text segmentation",
          "Icelandic",
          "Mandarin Chinese",
          "language learning materials",
          "proficiency adaptation",
          "technical issues"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14866,
        "completion_tokens": 1189,
        "total_tokens": 16055,
        "estimated_cost_usd": 0.0304725
      },
      "has_tex": true
    },
    {
      "section_id": "3df4831823bab09c",
      "level": 3,
      "title": "AI in text and image generation",
      "label": null,
      "plain_text_len": 945,
      "plain_text_excerpt": "AI in text and image generation\n\nThe project highlighted strengths and limitations in AI-generated content:\n\ndescription\n AI-created texts were sometimes inconsistent, requiring multiple prompt refinements. Known narratives (e.g., Norse mythology) tended to yield higher-quality texts than AI-generated original stories. The error rate in AI-generated texts varied between 20\\% and 90\\%, depending on the level of human editing needed.\n\n While AI-produced translations were mostly accurate, issues arose in Icelandic-Mandarin Chinese translations due to structural differences between the languages. Context-sensitive modifications were required to improve glossing and lemma-tagging accuracy.\n\n AI-generated images were often inconsistent, especially in character depiction. While animal characters maintained uniformity, human characters varied across images. Specifying stylistic consistency in pr…",
      "analysis": {
        "section_summary": "The team observed that AI-generated texts often needed multiple prompt revisions, with familiar narratives outperforming original stories and error rates ranging widely. While translations were generally accurate, Icelandic–Mandarin differences led to glossing and lemma-tagging problems requiring context-aware edits. AI-produced images also showed inconsistency, especially for human characters, though specifying a consistent style in prompts improved uniformity.",
        "relevant_views": [
          {
            "url_name": "generate_text_monitor",
            "confidence": 0.31,
            "rationale": "Monitors AI-driven text generation tasks, aligning with the discussion of refining prompts and handling error rates in AI-produced texts."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.32,
            "rationale": "Configures and manages coherent image generation with style controls, relevant to mitigating inconsistency in AI-generated character depictions."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.29,
            "rationale": "Provides monitoring for coherent image generation tasks, pertinent to tracking and improving stylistic consistency in AI-produced images."
          }
        ],
        "concept_tags": [
          "AI-generated text quality",
          "prompt refinement",
          "translation accuracy",
          "Icelandic–Mandarin differences",
          "glossing",
          "lemma-tagging",
          "AI image consistency",
          "stylistic prompts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14543,
        "completion_tokens": 832,
        "total_tokens": 15375,
        "estimated_cost_usd": 0.02649875
      },
      "has_tex": true
    },
    {
      "section_id": "f7a049ad5ec3f72d",
      "level": 3,
      "title": "Implications for C-LARA’s use with less commonly spoken languages",
      "label": null,
      "plain_text_len": 547,
      "plain_text_excerpt": "Implications for C-LARA’s use with less commonly spoken languages\n\nThe findings underscore C-LARA’s potential in learning less commonly spoken languages such as Icelandic, particularly in resource-scarce environments. The platform facilitates interactive, multimedia-based learning experiences but requires careful human supervision to ensure linguistic and cultural accuracy. The results suggest that C-LARA can support personalized content development for learners, particularly when integrated with manual editing and instructional scaffolding.",
      "analysis": {
        "section_summary": "The section highlights C-LARA’s potential to support learning of less commonly spoken languages like Icelandic in resource-scarce settings through interactive, multimedia experiences, while emphasizing the need for human oversight to maintain linguistic and cultural accuracy. It notes that integrating manual editing and instructional scaffolding can enable personalized content development for learners.",
        "relevant_views": [],
        "concept_tags": [
          "less commonly spoken languages",
          "Icelandic",
          "resource-scarce environments",
          "interactive multimedia learning",
          "human supervision",
          "linguistic and cultural accuracy",
          "personalized content development",
          "manual editing",
          "instructional scaffolding"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14524,
        "completion_tokens": 1020,
        "total_tokens": 15544,
        "estimated_cost_usd": 0.028355
      },
      "has_tex": true
    },
    {
      "section_id": "f4bfd9f94ea5b74a",
      "level": 2,
      "title": "Kok Kaper (Kowanyama Community, Queensland)",
      "label": "Section:KokKaper",
      "plain_text_len": 5473,
      "plain_text_excerpt": "Kok Kaper (Kowanyama Community, Queensland)\nSection:KokKaper\n\nSophie Rendina has been using C-LARA together with the Kok Kaper community at Kowanyama, Cape York, Queensland. Kok Kaper is a small, critically endangered Australian Aboriginal language, and the C-LARA work supports the creation of Kok Kaper language teaching and learning materials, which form an integral part of the Kowanyama State School's language program. Texts so far created are an initial picture dictionary with 50 entries, some songs, and some simple dialogues; there is a particular focus on images, which can be used in multiple ways in the classroom. We present two examples showing creation of images for Kok Kaper texts\n\nThe first, taken from the picture dictionary, accompanies the entry for the word Mim-marpany, which means both ``cyclone'' and ``rainbow snake'' (the Kok Kaper language identifies these two concepts).…",
      "analysis": {
        "section_summary": "The section describes collaboration with the Kok Kaper community in Queensland to build language teaching materials in a critically endangered Aboriginal language using C-LARA’s image generation capabilities. Examples include a picture dictionary entry where GPT-4o produced a culturally sensitive prompt and DALL-E-3 generated an approved image for ‘cyclone/rainbow snake’, and a lullaby illustrated with multiple coherent images from ‘Images in ChatGPT’ that matched text, style, recurring characters, and cultural context with minimal human intervention. The community is also beginning to trial C-LARA’s community reviewing tools.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.55,
            "rationale": "Section centers on configuring and generating coherent sets of images for texts; edit_images_v2 supports managing coherent image generation workflows."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.5,
            "rationale": "Monitoring coherent image generation aligns with the described iterative creation of culturally consistent images for Kok Kaper texts."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.48,
            "rationale": "Status polling for coherent image generation is pertinent to producing multi-image narratives like the lullaby."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.32,
            "rationale": "Simple-C-LARA can be used to create projects and dispatch text/image generation tasks as done for the Kok Kaper materials."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.28,
            "rationale": "Monitoring queued image generation tasks in Simple-C-LARA supports the described workflow of generating images for texts."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.27,
            "rationale": "Reviewing generated images per page fits the need to assess multiple lullaby illustrations for consistency."
          },
          {
            "url_name": "community_home",
            "confidence": 0.24,
            "rationale": "The section notes trialling community reviewing functionality, which is anchored in community views like community_home."
          },
          {
            "url_name": "assign_member_to_community",
            "confidence": 0.2,
            "rationale": "Community reviewing involves assigning members to community projects for collaborative review."
          }
        ],
        "concept_tags": [
          "Kok Kaper",
          "Australian Aboriginal language",
          "endangered language",
          "language revitalization",
          "picture dictionary",
          "lullaby",
          "image generation",
          "coherent images",
          "GPT-4o",
          "DALL-E",
          "Images in ChatGPT",
          "cultural context",
          "community reviewing",
          "Cape York",
          "stylistic consistency"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16046,
        "completion_tokens": 895,
        "total_tokens": 16941,
        "estimated_cost_usd": 0.0290075
      },
      "has_tex": true
    },
    {
      "section_id": "b1503f613dcdf4e6",
      "level": 2,
      "title": "Iaai and Drehu  (University of New Caledonia)",
      "label": "Section:IaaiAndDrehu",
      "plain_text_len": 1899,
      "plain_text_excerpt": "Iaai and Drehu (University of New Caledonia)\nSection:IaaiAndDrehu\n\nAs part of our presentation at the Eighth Workshop on Computational Methods for Endangered Languages [CITE:ComputEL8,ComputEL8Presentation], we created some illustrated C-LARA texts for the Kanak languages Iaai and Drehu, modelling them on the earlier Kok Kaper content. We made a 50 word picture dictionary for Drehu and a traditional story for each language. Our evaluation focussed on the images.\n\nThe results were similar to those for Kok Kaper. Although not perfect, the quality of the images in the picture dictionary was generally judged good or at least satisfactory. Community members were much less pleased with the stories. For example, the Iaai story explains the origin of the coconut crab and the hermit crab when they offend the powerful island spirit soohmwecaa (lit. ``grandmother'') Although visually attractive to …",
      "analysis": {
        "section_summary": "The authors created illustrated C-LARA content for the Kanak languages Iaai and Drehu—a Drehu picture dictionary and traditional stories—and evaluated the generated images. Like previous Kok Kaper work, dictionary images were mostly acceptable, but story illustrations were criticized by community members for cultural inaccuracies and poor coherence of recurring elements. They anticipate that new “Images in ChatGPT” capabilities may improve these results in future work.",
        "relevant_views": [],
        "concept_tags": [
          "Iaai",
          "Drehu",
          "Kanak languages",
          "picture dictionary",
          "traditional stories",
          "image quality",
          "cultural appropriateness",
          "image coherence",
          "community feedback",
          "Images in ChatGPT",
          "Kok Kaper",
          "endangered languages"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15054,
        "completion_tokens": 500,
        "total_tokens": 15554,
        "estimated_cost_usd": 0.0238175
      },
      "has_tex": true
    },
    {
      "section_id": "31e11620e968f6cb",
      "level": 1,
      "title": "ChatGPT as a software engineer",
      "label": "Section:SoftwareEngineer",
      "plain_text_len": 1404,
      "plain_text_excerpt": "ChatGPT as a software engineer\nSection:SoftwareEngineer\n\nThroughout the project, ChatGPT has made a large contribution as a software engineer, helping to construct the platform. It has contributed at two levels: first as a designer, helping to determine the overall architecture, and second as a coder, writing the actual software.\n\nPerhaps unexpectedly, we found in the early stages of the project that it could contribute more directly in its designer role. From the start, it was clear that ChatGPT had an excellent knowledge of Python packages and how they could be used; when there was a specific problem to solve, it could very frequently suggest a concise and elegant solution. It was somewhat less effective as a coder, and could not usually produce more than a fairly small amount of code in response to a single request; also, the code was in most cases not entirely correct, and needed car…",
      "analysis": {
        "section_summary": "The section describes how ChatGPT contributed to building the platform both as an architectural designer and as a coder. Early on, it was more effective suggesting architectural solutions and Python package use than producing correct code, which required debugging. With newer chain-of-thought models like o1, ChatGPT can now generate hundreds of lines of well-structured, often working Django form-based view code, suggesting a significant improvement in its coding role.",
        "relevant_views": [],
        "concept_tags": [
          "ChatGPT",
          "software engineering",
          "architecture design",
          "Python packages",
          "coding assistance",
          "Django",
          "Chain-of-Thought models",
          "o1 model",
          "code generation",
          "debugging"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14939,
        "completion_tokens": 296,
        "total_tokens": 15235,
        "estimated_cost_usd": 0.02163375
      },
      "has_tex": true
    },
    {
      "section_id": "77d8d966957e5e9a",
      "level": 2,
      "title": "Simple form-based views",
      "label": "Section:SimpleViews",
      "plain_text_len": 803,
      "plain_text_excerpt": "Simple form-based views\nSection:SimpleViews\n\nA large fraction of C-LARA’s code follows straightforward patterns: handling user inputs, storing or updating database records, and rendering templates. In Django, this typically involves three main steps: \nenumerate\n Defining a form that corresponds to the data fields we want users to supply or edit,\n Writing a view function that processes GET and POST requests, and\n Rendering a template to show either the empty form (in the case of GET) or a validation error (if POST fails), or to redirect upon success.\nenumerate\nThese steps are well-served by common Django idioms and often require minimal domain-specific logic. Consequently, ChatGPT can handle them with near-complete reliability, once it knows what fields to collect and which actions to perform.",
      "analysis": {
        "section_summary": "Many C-LARA views follow a simple Django pattern: define a form for the data to collect or edit, write a view that handles GET to display the empty form and POST to validate and save or show errors, then render a template or redirect on success. These straightforward, low domain-logic patterns can be handled reliably by ChatGPT once the needed fields and actions are known.",
        "relevant_views": [
          {
            "url_name": "user_config",
            "confidence": 0.7,
            "rationale": "This view is a straightforward form workflow letting users create or update configuration via GET/POST and template rendering, matching the simple form-based pattern described."
          },
          {
            "url_name": "register",
            "confidence": 0.66,
            "rationale": "The registration view processes a form to create a new user, handling GET/POST and redirects on success, exemplifying the simple form-handling steps mentioned."
          },
          {
            "url_name": "funding_request",
            "confidence": 0.58,
            "rationale": "Submitting a funding request involves a basic Django form with validation and saving, fitting the pattern of handling user input and rendering templates."
          },
          {
            "url_name": "manage_language_masters",
            "confidence": 0.55,
            "rationale": "Managing language master assignments is implemented with standard forms and message feedback, reflecting the typical GET/POST form-handling approach."
          },
          {
            "url_name": "create_community",
            "confidence": 0.52,
            "rationale": "Creating a community uses a simple form-based view to capture input and create a record, matching the described pattern of minimal domain-specific logic."
          }
        ],
        "concept_tags": [
          "Django forms",
          "GET/POST handling",
          "template rendering",
          "form validation",
          "CRUD",
          "user input processing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14414,
        "completion_tokens": 995,
        "total_tokens": 15409,
        "estimated_cost_usd": 0.0279675
      },
      "has_tex": true
    },
    {
      "section_id": "4e02b156bdfe4a92",
      "level": 2,
      "title": "Examples of simple Django views",
      "label": null,
      "plain_text_len": 4191,
      "plain_text_excerpt": "Examples of simple Django views\nTo illustrate, we show two real examples from C-LARA. The first manages user permissions for an admin user:\nlstlisting[language=Python, caption=An example of a straightforward permission-management view.]\n@login_required\n@user_passes_test(lambda u: u.userprofile.is_admin)\ndef manage_user_permissions(request):\n user_select_form = UserSelectForm(request.POST or None)\n permissions_form = None\n selected_user_id = None\n\n if request.method == 'POST' and 'select_user' in request.POST:\n if user_select_form.is_valid():\n selected_user = user_select_form.cleaned_data['user']\n selected_user_id = selected_user.id\n permissions_form = UserPermissionsForm(instance=selected_user.userprofile)\n elif request.method == 'POST':\n selected_user_id = request.POST.get('selected_user_id')\n selected_user_profile = get_object_or_404(UserProfile, user__id=selected_user_id)\n permissions…",
      "analysis": {
        "section_summary": "The section illustrates two straightforward Django view examples from C‑LARA: an admin-only user permission management view that follows the standard GET/POST form pattern to select a user, validate and save a UserProfile permissions form, showing messages and redirecting on success; and a credit transfer view that handles POSTed form data, checks user existence and balance, generates and emails a confirmation code while storing transfer details in the session, uses the messages framework, and redirects to a confirmation step. The examples highlight how typical form validation, branching on request method, simple model updates, and redirects can be reliably authored by ChatGPT, benefiting from improved chain-of-thought reasoning for common CRUD flows.",
        "relevant_views": [
          {
            "url_name": "manage_user_permissions",
            "confidence": 0.9,
            "rationale": "The first example in the section is the manage_user_permissions view handling admin permission updates with GET/POST form logic."
          },
          {
            "url_name": "transfer_credit",
            "confidence": 0.9,
            "rationale": "The second example shown is the transfer_credit view managing credit transfers via form validation, messages, session storage, and redirect."
          },
          {
            "url_name": "confirm_transfer",
            "confidence": 0.6,
            "rationale": "The credit transfer flow redirects to a confirm_transfer step, implying this companion view is part of the described process."
          }
        ],
        "concept_tags": [
          "Django view",
          "form handling",
          "POST validation",
          "admin permissions",
          "credit transfer",
          "session storage",
          "email confirmation",
          "messages framework",
          "redirect",
          "UserProfile",
          "ChatGPT code generation",
          "chain-of-thought reasoning",
          "CRUD patterns"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15014,
        "completion_tokens": 638,
        "total_tokens": 15652,
        "estimated_cost_usd": 0.0251475
      },
      "has_tex": true
    },
    {
      "section_id": "1ab69647c00fdcb2",
      "level": 2,
      "title": "Why these tasks are straightforward; what's next?",
      "label": null,
      "plain_text_len": 2246,
      "plain_text_excerpt": "Why these tasks are straightforward; what's next?\nIn each of the above cases, the functionality is purely about requesting basic user input, validating it with one or two domain checks, and then performing an elementary update or redirection. The primary complexity lies in deciding how to structure user interactions—something that is highly standardized in Django. Because these tasks revolve around well-trodden design patterns and do not demand much domain-specific reasoning, ChatGPT can easily produce robust and accurate code. \n\nThe interesting question during the next phase of the project is whether o1 can similarly take over responsibility for less stereotypical parts of the codebase, in particular those responsible for linguistic annotation, coherent image generation, and Simple C-LARA. Our impression, discussing the tasks with gpt-4 and gpt-4o, was that they were too difficult for t…",
      "analysis": {
        "section_summary": "The section notes that previous coding tasks were simple, involving basic input handling and redirects using standard Django patterns, which models like ChatGPT handle well. The next challenge is to see if newer models such as o1 can manage more complex, less stereotypical parts of the codebase—particularly linguistic annotation, coherent image generation, and the comprehensive Simple C-LARA workflow—where earlier models struggled with prompt design and holding broad context. The authors plan systematic discussions to gauge o1’s ability to take over these areas.",
        "relevant_views": [
          {
            "url_name": "edit_images",
            "confidence": 0.44,
            "rationale": "Coherent image generation is a focus of the ‘what’s next’ discussion; this view handles image editing and generation tasks that were previously challenging for older models."
          },
          {
            "url_name": "create_dall_e_3_image_monitor",
            "confidence": 0.41,
            "rationale": "Directly related to image generation workflows the section highlights as difficult; monitoring async DALL·E image creation fits the coherent image generation challenge."
          },
          {
            "url_name": "create_dall_e_3_image_status",
            "confidence": 0.41,
            "rationale": "Status tracking for image generation tasks aligns with the coherent image generation area noted as a next-phase challenge."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.47,
            "rationale": "Specifically targets coherent images v2 monitoring, matching the section’s mention of coherent image generation as a harder, less stereotypical task."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.46,
            "rationale": "Status endpoint for coherent images generation, relevant to the challenges around image workflows mentioned in the text."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.45,
            "rationale": "Covers configuration and generation of coherent images, aligning with the ‘coherent image generation’ frontier described."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.5,
            "rationale": "The section explicitly calls out Simple C-LARA as a complex, holistic workflow; this view is the main entry point for that wizard."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.43,
            "rationale": "Provides monitoring for the Simple C-LARA workflow, which the section notes as a next-phase challenge due to its breadth."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.39,
            "rationale": "Handles async element-generation steps within Simple C-LARA, relevant to the comprehensive workflow highlighted as difficult."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.38,
            "rationale": "Status polling for Simple C-LARA element generation ties to managing the complex workflow mentioned."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.39,
            "rationale": "Monitors image-related steps within Simple C-LARA, fitting the combined text+image workflow challenge."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.38,
            "rationale": "Status for image generation in Simple C-LARA, relevant to the noted complexity around image workflows."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.37,
            "rationale": "Monitors style-generation tasks in Simple C-LARA, part of the complex end-to-end workflow noted."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.37,
            "rationale": "Status endpoint for style generation in Simple C-LARA, aligning with the workflow complexity discussed."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.36,
            "rationale": "Reviewing coherent images for elements within Simple C-LARA reflects the combined image + workflow challenges cited."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.36,
            "rationale": "Reviewing coherent images for pages in Simple C-LARA ties directly to the coherent image generation complexity."
          },
          {
            "url_name": "generate_text_monitor",
            "confidence": 0.33,
            "rationale": "Linguistic annotation is a highlighted challenge; this monitors async text generation in annotation workflows."
          },
          {
            "url_name": "generate_text_status",
            "confidence": 0.33,
            "rationale": "Status tracking for generated texts supports the linguistic annotation tasks the section identifies as harder."
          },
          {
            "url_name": "generate_text_complete",
            "confidence": 0.32,
            "rationale": "Completion endpoint for generated texts, part of the annotation pipeline the section plans to test with newer models."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.31,
            "rationale": "Creating core text versions is part of linguistic annotation, one of the code areas the section deems less stereotypical."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.31,
            "rationale": "Glossed text creation is a specific linguistic annotation task called out as a harder area for model assistance."
          }
        ],
        "concept_tags": [
          "Django patterns",
          "LLM-based development",
          "prompt engineering",
          "linguistic annotation",
          "coherent image generation",
          "Simple C-LARA workflow",
          "Chain-of-Thought models",
          "context management",
          "asynchronous task monitoring",
          "template-based prompts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14977,
        "completion_tokens": 1542,
        "total_tokens": 16519,
        "estimated_cost_usd": 0.03414125
      },
      "has_tex": true
    },
    {
      "section_id": "f9bf87ed9f43d8e0",
      "level": 1,
      "title": "ChatGPT as an author",
      "label": "Section:Author",
      "plain_text_len": 822,
      "plain_text_excerpt": "ChatGPT as an author\nSection:Author\n\nA second notable area where the new o1 model surpasses earlier LLMs is in writing or co-authoring texts. The previous version, GPT-4o, already had nontrivial abilities, but struggled to maintain longer-form consistency, address complex research questions, or incorporate correct references. In contrast, o1 is capable of generating structured academic prose that extends to multiple pages, with clear organization, significantly fewer factual lapses, and generally correct and appropriate references. To explore these new capabilities in a controlled manner, we wrote four self-contained, publicly available papers, two with GPT-4o and two with o1. Below, we outline the contents and motivations behind these four works, then summarise the main lessons learned regarding AI authorship.",
      "analysis": {
        "section_summary": "The section highlights the enhanced authorship capabilities of the new o1 model compared to prior LLMs like GPT-4o. It can sustain longer, well-organized academic prose with fewer factual errors and more accurate references. To test these abilities, the authors co-wrote four publicly available papers (two with GPT-4o, two with o1) and outline their contents and lessons on AI-assisted writing.",
        "relevant_views": [],
        "concept_tags": [
          "LLM",
          "ChatGPT",
          "o1 model",
          "GPT-4o",
          "AI authorship",
          "academic writing",
          "long-form consistency",
          "factual accuracy",
          "references"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14650,
        "completion_tokens": 354,
        "total_tokens": 15004,
        "estimated_cost_usd": 0.0218525
      },
      "has_tex": true
    },
    {
      "section_id": "36caed8f4db103ad",
      "level": 2,
      "title": "Overview of the four papers",
      "label": null,
      "plain_text_len": 3994,
      "plain_text_excerpt": "Overview of the four papers\n\nPaper 1: Reinforcement Learning for Chain of Thought Reasoning: A Case Study Using Tic-Tac-Toe.\nOriginally posted in July~2024 (ReinforcementLearningForTicTacToe), this collaboration between GPT-4o and a human co-author investigates how chain-of-thought (CoT) prompting can be optimized through a lightweight reinforcement learning (RL) approach. The focus is on an experiment where RL was used to optimise Tic-Tac-Toe performance by evolving few-shot CoT examples over 40 iterative cycles. Results show a statistically significant improvement in move correctness and game outcomes, suggesting that even small-scale RL can enhance CoT-based reasoning. The human and AI collaborated on both the experiments and the writing. The greater part of the code was written by the AI and then revised by the human; the AI wrote perhaps a third of the text, which was again in most …",
      "analysis": {
        "section_summary": "The section outlines four AI-involved papers. Paper 1 reports a lightweight reinforcement learning approach to optimize chain-of-thought prompting for Tic-Tac-Toe, with GPT-4o contributing code, data analysis, and text. Paper 2 surveys generative AI in computer-assisted language learning, co-authored equally by three authors including GPT-4o, with heavy human revisions. Paper 3 showcases a fully AI-written 3,100-word short story using o1-preview, demonstrating coherent long-form creative writing with human supervision. Paper 4 is an AI-authored rebuttal to “ChatGPT is bullshit,” where the AI quickly identified a key omission about RLHF and supplied references, attracting significant readership.",
        "relevant_views": [],
        "concept_tags": [
          "Reinforcement learning",
          "Chain-of-thought prompting",
          "Tic-Tac-Toe",
          "Generative AI",
          "Computer-assisted language learning",
          "AI co-authorship",
          "o1-preview",
          "AI creative writing",
          "RLHF",
          "ChatGPT rebuttal"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15416,
        "completion_tokens": 368,
        "total_tokens": 15784,
        "estimated_cost_usd": 0.02295
      },
      "has_tex": true
    },
    {
      "section_id": "0c8ef5bdf091f641",
      "level": 2,
      "title": "Conclusion and future outlook",
      "label": null,
      "plain_text_len": 712,
      "plain_text_excerpt": "Conclusion and future outlook\nThese experiences show how o1 and similar CoT-based models can now take on the main responsibility for writing a substantial piece of text. They still need some human supervision. One important problem is that they come across as insufficiently self-critical; another is that after a while they are not directly able to recall what they have written due to limitations on context length, and must reread the text in order to respond to questions about it. However, given the speed of progress---the jump from GPT-4 to o1 was particularly striking---it seems legitimate to project that they will reach the level of human academic authors fairly soon, perhaps within one to two years.",
      "analysis": {
        "section_summary": "The conclusion notes that chain-of-thought models like o1 can now draft substantial texts with limited human oversight, though they lack self-criticism and suffer from context-length forgetting, requiring rereads to answer questions. Given rapid progress, especially from GPT-4 to o1, the author predicts such models may achieve human-level academic authorship within one to two years.",
        "relevant_views": [],
        "concept_tags": [
          "Chain-of-Thought models",
          "o1",
          "GPT-4",
          "human supervision",
          "context length limitations",
          "academic authorship",
          "model self-criticism"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14709,
        "completion_tokens": 339,
        "total_tokens": 15048,
        "estimated_cost_usd": 0.02177625
      },
      "has_tex": true
    },
    {
      "section_id": "bf6c74c8e69105b5",
      "level": 1,
      "title": "Ethical issues",
      "label": "Section:Ethics",
      "plain_text_len": 244,
      "plain_text_excerpt": "Ethical issues\nSection:Ethics\n\nThere are two ethical issues particularly relevant to this project. One is the question of AI authorship; the other is the obligations of the platform providers, that is to say ourselves. We discuss these in turn.",
      "analysis": {
        "section_summary": "The section introduces two key ethical concerns for the project: questions around AI authorship and the responsibilities of the platform providers themselves, which will be examined further.",
        "relevant_views": [],
        "concept_tags": [
          "ethics",
          "AI authorship",
          "platform responsibilities",
          "provider obligations"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14642,
        "completion_tokens": 209,
        "total_tokens": 14851,
        "estimated_cost_usd": 0.0203925
      },
      "has_tex": true
    },
    {
      "section_id": "ec758e18540a42b4",
      "level": 2,
      "title": "AI authorship",
      "label": "Section:AIAuthorship",
      "plain_text_len": 4036,
      "plain_text_excerpt": "AI authorship\nSection:AIAuthorship\n\nBecause C‑LARA treats the AI as a genuine collaborator, the contested question of whether an AI can be an accredited author is impossible to avoid. Unfortunately, with some exceptions, most journals and conferences do not currently allow AI authors. This is usually justified by claiming that AIs are not capable of being accountable for their work. We question this on multiple grounds.\n\nFirst, it is not clear either that AIs actually are incapable of being accountable for their work, or that humans unquestioningly accredited as authors always are so capable. Starting with the second of these, if a paper has many authors it is very rarely the case that every author can take responsibility for every aspect of the paper. Normally, most of the authors will have made some specific contribution and can take responsibility for that contribution, but may know l…",
      "analysis": {
        "section_summary": "The section argues that C‑LARA treats AI as a genuine collaborator, challenging blanket refusals by journals and conferences to list AI as an author. It questions claims that AI cannot be accountable, noting that human coauthors often only answer for their specific contributions. It calls for pragmatic criteria—ability to explain, defend, and retract if needed—and points out there is little evidence high‑end AIs cannot meet them, shifting the burden of proof. Excluding AI authorship leads to uncredited de facto AI contributors and misattributed responsibility. The authors advocate transparency about AI involvement and, given C‑LARA’s extensive AI contributions, view AI as deserving coauthor status, gathering evidence to support this position.",
        "relevant_views": [],
        "concept_tags": [
          "AI authorship",
          "authorship accountability",
          "ethical issues",
          "academic publishing",
          "AI coauthor",
          "responsibility criteria",
          "transparency",
          "burden of proof",
          "AI contributions",
          "research integrity"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15699,
        "completion_tokens": 420,
        "total_tokens": 16119,
        "estimated_cost_usd": 0.02382375
      },
      "has_tex": true
    },
    {
      "section_id": "9ed17aed8998c3a6",
      "level": 2,
      "title": "Obligations of the platform providers",
      "label": "Section:Obligations",
      "plain_text_len": 1384,
      "plain_text_excerpt": "Obligations of the platform providers\nSection:Obligations\n\nA distressing fact, observed in a multitude of academic projects which involve construction of online platforms and other software tools, is that these tools often have a brief lifespan. People are encouraged to use them, typically because their involvement lets the platform providers publish papers and/or obtain further funding, but at some point the funding runs out or some key person leaves. Normally, the tool becomes unavailable shortly afterwards, and the users are left stranded; resources they built using the tool are in many cases no longer accessible. It is hard to feel that this is a morally defensible outcome, but since it appears inevitable the general feeling is that there is no point wringing one's hands over it.\n\nAn interesting thing about AI collaborators is that they potentially offer a way out of this ethical dil…",
      "analysis": {
        "section_summary": "The section criticizes the common pattern of academic platforms becoming unusable once funding or key personnel disappear, leaving users without access to their work. It suggests that capable AI collaborators might mitigate this ethical issue by maintaining codebases through funding gaps or staff losses, potentially extending a project’s lifespan, though it remains uncertain whether this can be reliably achieved.",
        "relevant_views": [],
        "concept_tags": [
          "platform sustainability",
          "ethical obligations",
          "software longevity",
          "user impact",
          "funding gaps",
          "AI-assisted maintenance",
          "codebase stewardship"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14802,
        "completion_tokens": 199,
        "total_tokens": 15001,
        "estimated_cost_usd": 0.0204925
      },
      "has_tex": true
    },
    {
      "section_id": "9c373355cdc8d074",
      "level": 1,
      "title": "Conclusions and further directions",
      "label": "Section:Conclusions",
      "plain_text_len": 4099,
      "plain_text_excerpt": "Conclusions and further directions\nSection:Conclusions\n\nThis report has documented work carried out in C-LARA during the period Mar 2024--Apr 2025. Our main goals were to improve the quality of linguistic annotation and image generation, make the content creation process faster, make the platform more suitable for use with Indigenous languages, and better understand the AI's abilities as a software engineer and author.\n\nWe are pleased with our progress. C-LARA is now almost at the point where it can be used reliably for its intended purpose. Thanks to the introduction of the MWE-tagging phase and associated processing, linguistic annotation in English is much improved, with the error rate more than halved. The coherent image generation module also represents a large advance. It is not yet adequate for connected narratives, but preliminary experiments encourage us to think that it will re…",
      "analysis": {
        "section_summary": "The conclusions recap progress made between March 2024 and April 2025: improved English linguistic annotation through MWE tagging, advances in coherent image generation, easier content creation for Indigenous languages, and notable gains in AI assistance for coding and authorship. Future priorities include integrating the new Images in ChatGPT model to boost image quality, scaling content creation with partners, testing the extent to which AI can assume software engineering responsibilities via code cleanup and documentation, empirically evaluating AI authorship accountability, and deploying a mirror server with documented procedures.",
        "relevant_views": [],
        "concept_tags": [
          "C-LARA",
          "linguistic annotation",
          "MWE tagging",
          "coherent image generation",
          "Images in ChatGPT",
          "Indigenous languages",
          "AI-assisted software engineering",
          "AI authorship",
          "content creation",
          "codebase cleanup",
          "mirror server deployment"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15587,
        "completion_tokens": 396,
        "total_tokens": 15983,
        "estimated_cost_usd": 0.02344375
      },
      "has_tex": true
    },
    {
      "section_id": "c9015d53b9084645",
      "level": 1,
      "title": "Acknowledgements",
      "label": null,
      "plain_text_len": 665,
      "plain_text_excerpt": "Acknowledgements\n\nThe greater part of the development and evaluation work on C-LARA described here was carried out by Manny Rayner and different versions of ChatGPT under funding from the University of South Australia.\n\nWork on Icelandic and Mandarin was supported by a grant from Rannis, the Icelandic Centre for Research.\n\nWork on Iaai and Drehu was supported by l'AFD (Agence Française de Développement, Paris) and the Service for Culture, Education and Science, Embassy of France in Australia under a grant from the program `Fonds Pacifique', and by the ERALO research team at the University of New Caledonia.\n\ntocsectionReferences\n\nplainnat\n\nclara_third_report",
      "analysis": {
        "section_summary": "The acknowledgements note that most development and evaluation was done by Manny Rayner with ChatGPT, funded by the University of South Australia. Work on Icelandic and Mandarin was supported by a Rannis grant, while work on Iaai and Drehu received support from the French development agency, the French embassy in Australia via the Fonds Pacifique program, and the ERALO team at the University of New Caledonia.",
        "relevant_views": [],
        "concept_tags": [
          "funding",
          "University of South Australia",
          "Manny Rayner",
          "ChatGPT",
          "Rannis",
          "Icelandic Centre for Research",
          "Iaai",
          "Drehu",
          "Agence Française de Développement",
          "Fonds Pacifique",
          "Embassy of France in Australia",
          "ERALO",
          "University of New Caledonia"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14951,
        "completion_tokens": 519,
        "total_tokens": 15470,
        "estimated_cost_usd": 0.02387875
      },
      "has_tex": true
    },
    {
      "section_id": "fe78394ccfe799d1",
      "level": 1,
      "title": "Glossary of abbreviations and acronyms",
      "label": "Section:AbbreviationsAndAcronyms",
      "plain_text_len": 3317,
      "plain_text_excerpt": "Glossary of abbreviations and acronyms\nSection:AbbreviationsAndAcronyms\n\nWe briefly list abbreviations and acronyms used in the report.\n\ndescription\n\n ``Artificial Intelligence''. The meaning of this term is debated, but is sometimes described as the development of software capable of behaviour which in humans would require intelligence.\n\n Académie des Langues Kanak / Kanak Languages Academy.\n\n ``Application Programming Interface''. Interface software that is added to a software system in order to allow other software systems to access it, often but not necessarily through the Internet.\n\n Type of chatbot architecture in which the bot produces its final response by chaining intermediate responses, in effect thinking aloud. CoT has repeatedly been shown to be more powerful than plain chatbot architectures.\n\n Software system capable of carrying out a more or less plausibly human-like conver…",
      "analysis": {
        "section_summary": "This section provides a glossary of abbreviations and acronyms used throughout the report, defining terms related to AI, large language models, the C-LARA platform, chatbots, OpenAI models and tools, linguistic terminology, and software architecture.",
        "relevant_views": [],
        "concept_tags": [
          "Artificial Intelligence",
          "AI",
          "Academie des Langues Kanak",
          "API",
          "Chain of Thought",
          "CoT",
          "Chatbot",
          "ChatGPT-3.5",
          "ChatGPT-4",
          "C-LARA",
          "DALL-E",
          "GPT-3.5",
          "GPT-4",
          "GPT-4 October 2023",
          "GPT-4.1 omni",
          "LARA",
          "Large Language Model",
          "LLM",
          "Model View Controller",
          "MVC",
          "Multi-Word Expression",
          "MWE",
          "OpenAI",
          "Part of Speech",
          "POS"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15308,
        "completion_tokens": 454,
        "total_tokens": 15762,
        "estimated_cost_usd": 0.023675
      },
      "has_tex": true
    },
    {
      "section_id": "62bfcaac13123153",
      "level": 1,
      "title": "Creating a picture book with Simple C-LARA",
      "label": "Section:UsingSimpleCLARA",
      "plain_text_len": 5270,
      "plain_text_excerpt": "Creating a picture book with Simple C-LARA\nSection:UsingSimpleCLARA\n\nWe present an example showing how to quickly create a picture book text using Simple C-LARA. The final multimedia text is posted https://c-lara.unisa.edu.au/accounts/content/253/here.\n\nWe start by going to the ``My projects'' menu and clicking on ``Create new C-LARA project using Simple C-LARA''.\n\ncenter\n AmorousCat/01Start.jpg\ncenter\n\nThis takes us to the Simple C-LARA screen. We fill in the project name and the text and annotation languages, and check ``Use the AI to create text and an image for each page based on your instructions''.\n\ncenter\n AmorousCat/02CreateProject.jpg\ncenter\n\nWe've now got a box where we can enter the initial prompt. We do that and hit ``Create Text''.\n\ncenter\n AmorousCat/03Prompt.jpg\ncenter\n\nC-LARA now adds two new boxes, showing the title and text it has created based on our prompt. When we lo…",
      "analysis": {
        "section_summary": "This section walks through using the Simple C‑LARA wizard to rapidly create and publish a picture book. Starting from the My Projects menu, the user creates a new Simple C‑LARA project, enters a prompt to generate a title and story, applies automatic segmentation, then defines an image style, recurring image elements, and page images with AI-generated alternatives. After reviewing and selecting preferred images, the system assembles translations, glosses and media into a multimedia text, which is posted to the social network and appears among recently published content.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.93,
            "rationale": "Central Simple C‑LARA wizard used to create the project, generate text, images, and publish."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.7,
            "rationale": "Provides monitoring/progress for Simple C‑LARA actions invoked during the workflow."
          },
          {
            "url_name": "simple_clara_status",
            "confidence": 0.66,
            "rationale": "Status endpoint used by the Simple C‑LARA UI while generating text, images, and multimedia content."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.82,
            "rationale": "Triggered when generating and monitoring the image style from the style prompt."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.78,
            "rationale": "Status polling for the style generation step in the Simple C‑LARA flow."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.82,
            "rationale": "Used when generating and viewing recurring image elements like the cat and woman."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.78,
            "rationale": "Status polling for the element generation step invoked in the narrative."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.82,
            "rationale": "Handles monitoring the creation of page images after style and elements are set."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.78,
            "rationale": "Status endpoint for page image generation invoked in the Simple C‑LARA workflow."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.76,
            "rationale": "Review interface to inspect and possibly edit generated recurring element images."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.8,
            "rationale": "Review interface to inspect and choose between generated page images."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.72,
            "rationale": "Allows reviewing the style sample image generated from the style prompt."
          },
          {
            "url_name": "register_project_content",
            "confidence": 0.45,
            "rationale": "Posting the generated multimedia text to the social network likely uses the project content registration endpoint."
          },
          {
            "url_name": "public_content_detail",
            "confidence": 0.37,
            "rationale": "The final social network page and public view of the posted multimedia text correspond to content detail views."
          }
        ],
        "concept_tags": [
          "Simple C-LARA",
          "AI text generation",
          "picture book",
          "segmented text",
          "image style",
          "image elements",
          "page images",
          "multimedia text",
          "publishing",
          "social network",
          "prompt-based creation",
          "image review",
          "translations and glosses"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16366,
        "completion_tokens": 2092,
        "total_tokens": 18458,
        "estimated_cost_usd": 0.0413775
      },
      "has_tex": true
    },
    {
      "section_id": "6cfae7986debb11b",
      "level": 1,
      "title": "Creating a picture book with Advanced C-LARA",
      "label": "Section:UsingCoherentImages",
      "plain_text_len": 10456,
      "plain_text_excerpt": "Creating a picture book with Advanced C-LARA\nSection:UsingCoherentImages\n\nIn this section, we will show how to create a picture book text with one image per page. The text used will be Shakespeare's Sonnet 18 (``Shall I compare thee to a summer's day''). We will divide it up into eight pages, a title page followed by one page for each pair of lines in the fourteen-line poem. We will include glosses and translations in French. The final multimedia text is posted https://c-lara.unisa.edu.au/accounts/content/251/here.\n\nWe start by selecting ``Create new C-LARA project using Advanced C-LARA'' from the ``My projects'' menu:\n\ncenter\n Sonnet18/Sonnet18_01_CreateProject.jpg\ncenter\n\nWe get this screen. We fill in the project name, the text language and the annotation language, and tick the box ``Use coherent AI-generated image set''.\n\ncenter\n Sonnet18/Sonnet18_02_CreateProject_project_form.jpg\nce…",
      "analysis": {
        "section_summary": "Step-by-step walkthrough of using Advanced C-LARA to create and publish a multimodal picture book of Shakespeare’s Sonnet 18: create a new project with coherent AI image set, enter the plain text and title, auto-estimate CEFR level and summary, segment the poem, generate French translations, MWE-, lemma- and gloss annotations, configure coherent image style, elements and page images, render the text, and register it as published content.",
        "relevant_views": [
          {
            "url_name": "create_project",
            "confidence": 0.9,
            "rationale": "Used at the start to create a new Advanced C-LARA project with the coherent AI-generated image set option."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.92,
            "rationale": "Chosen to manually enter and save the poem’s plain text."
          },
          {
            "url_name": "create_title",
            "confidence": 0.88,
            "rationale": "Selected to enter the text title distinct from the project name."
          },
          {
            "url_name": "create_cefr_level",
            "confidence": 0.85,
            "rationale": "Used to estimate the CEFR level of the text via AI."
          },
          {
            "url_name": "create_summary",
            "confidence": 0.85,
            "rationale": "Invoked to generate an AI summary of the text."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.9,
            "rationale": "Used to segment the poem into pages/segments with AI and then manually correct markers."
          },
          {
            "url_name": "create_segmented_title",
            "confidence": 0.82,
            "rationale": "Applied to segment the title text for the segmented title version."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.88,
            "rationale": "Used to generate French translations from the segmented text."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.86,
            "rationale": "Selected to auto-generate MWE annotations and then produce a version without CoT explanations."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.86,
            "rationale": "Used to generate lemma and POS tags from the segmented text."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.86,
            "rationale": "Used to generate glossed text with French glosses from the segmented text."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.93,
            "rationale": "Central to configuring coherent images: set background info, generate style, define elements, and generate page images."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.64,
            "rationale": "Supports monitoring the asynchronous generation of style, elements, and page images described."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.64,
            "rationale": "Provides the monitor view for the coherent images generation tasks mentioned."
          },
          {
            "url_name": "register_project_content",
            "confidence": 0.6,
            "rationale": "Relevant to the final step of registering the rendered multimodal text as published content."
          }
        ],
        "concept_tags": [
          "Advanced C-LARA",
          "project creation",
          "plain text entry",
          "text title",
          "CEFR level estimation",
          "summary generation",
          "segmented text",
          "French translation",
          "MWE tagging",
          "lemma tagging",
          "glossed text",
          "coherent images",
          "image style",
          "image elements",
          "page image generation",
          "rendering",
          "content registration",
          "multimodal picture book"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 17624,
        "completion_tokens": 1599,
        "total_tokens": 19223,
        "estimated_cost_usd": 0.03802
      },
      "has_tex": true
    },
    {
      "section_id": "059f4004bab8be66",
      "level": 1,
      "title": "Creating a text in a language not supported by the AI",
      "label": "Section:CreatingIndigenousLanguageText",
      "plain_text_len": 6825,
      "plain_text_excerpt": "Creating a text in a language not supported by the AI\nSection:CreatingIndigenousLanguageText\n\nThis appendix presents a toy example showing how to create a text in a language not supported by the AI; this means that linguistic annotation needs to be performed manually by the user. The language we have chosen is Pitjantjatjara, a Central Australian language that belongs to the Western Desert family, itself a sub-family of Pama-Nyungan. The text consists of the three sentences shown in Table~[REF:Table:PitjantjatjaraExample], taken from an exercise in Unit 17 of the 1967 Adelaide University advanced course in Pitjantjatjara [CITE:amery2012history]. \n\ntable*[th]\n Toy Pitjantjatjara text.\n Table:PitjantjatjaraExample\n\ntabularll\n\n1cAnnotation & 1cSentence\\\\\n\n2cSentence 1 \\\\\nTranslation & The dogs were coming to camp \\\\\nPlain & Papa tjua ngurakutu pitjangi \\\\\nSegmented & Papa tjua ngura-kutu pi…",
      "analysis": {
        "section_summary": "The section walks through manually creating a C-LARA project in an AI-unsupported language (Pitjantjatjara) using the Advanced workflow. It covers setting up the project, entering plain text and title, manually segmenting the text and title, configuring and generating coherent AI-based images using translations and style prompts, adding glosses and lemma tags, uploading recorded audio for words and segments, rendering the text, and viewing the final interactive result.",
        "relevant_views": [
          {
            "url_name": "create_project",
            "confidence": 0.58,
            "rationale": "Used to create a new project with specified text and annotation languages and coherent image options in the Advanced C-LARA workflow."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.64,
            "rationale": "The workflow includes manually entering and saving the plain text for the project."
          },
          {
            "url_name": "create_title",
            "confidence": 0.54,
            "rationale": "The user manually enters and saves the text title as part of setup."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.65,
            "rationale": "The section describes manually segmenting the text with markers, which aligns with creating/editing segmented text."
          },
          {
            "url_name": "create_segmented_title",
            "confidence": 0.49,
            "rationale": "Segmenting the title is mentioned, corresponding to the segmented title edit view."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.72,
            "rationale": "Most work is done in the Edit Images and Pages screen to configure coherent images, add translations, style prompts, and generate page images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.36,
            "rationale": "Generating style and page images may trigger background tasks monitored via the coherent images monitor."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.34,
            "rationale": "Status endpoints for coherent image generation could be used while generating style and page images."
          },
          {
            "url_name": "serve_rendered_text",
            "confidence": 0.3,
            "rationale": "After rendering, the final text is viewed via a rendered text link, which is served by the rendered text view."
          }
        ],
        "concept_tags": [
          "manual annotation",
          "indigenous languages",
          "Pitjantjatjara",
          "advanced C-LARA workflow",
          "plain text entry",
          "segmented text",
          "glossing",
          "lemma tagging",
          "coherent images",
          "style prompts",
          "audio upload",
          "rendering"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16169,
        "completion_tokens": 1203,
        "total_tokens": 17372,
        "estimated_cost_usd": 0.03224125
      },
      "has_tex": true
    },
    {
      "section_id": "55cd9d6353e3463a",
      "level": 1,
      "title": "Prompt templates",
      "label": "Section:PromptTemplates",
      "plain_text_len": 15150,
      "plain_text_excerpt": "Prompt templates\nSection:PromptTemplates\n\nFinally, we present the prompt templates and few-shot examples referred to earlier in the report:\n\nitemize\n Prompt template for English MWE annotation. Figure~[REF:Figure:MWEPromptTemplate].\n Typical few-shot example for English MWE annotation. Figure~[REF:Figure:MWEPromptExamples].\n Prompt template for glossing. Figure~[REF:Figure:GlossPromptTemplate].\n Prompt template for creating an image style. Figure~[REF:Figure:StyleGenerationTemplate].\n Prompt template for creating the candidate list of image elements. Figure~[REF:Figure:FindElementNames].\n Prompt template for creating an image element description. Figure~[REF:Figure:CreateElementDescription].\n Prompt template for selecting elements relevant to a page. Figure~[REF:Figure:FindRelevantElements].\n Prompt template for creating the description of a page image (DALL-E-3 version). Figure~[REF:Fig…",
      "analysis": {
        "section_summary": "This section showcases the prompt templates and few-shot examples used in CLARA for various tasks, including English MWE annotation, glossing, and several steps in the DALL-E-based image pipeline: defining a coherent visual style, identifying recurrent elements, describing individual elements, selecting relevant elements per page, crafting page-level image descriptions with essential aspects, and analysing user-uploaded element images.",
        "relevant_views": [
          {
            "url_name": "edit_prompt",
            "confidence": 0.62,
            "rationale": "Provides the interface to view and edit GPT-4 prompt templates and examples for annotation tasks like MWEs and glossing, directly aligning with the presented templates."
          },
          {
            "url_name": "bundle_list",
            "confidence": 0.32,
            "rationale": "Lists localisation bundles related to prompt templates/examples, which are part of managing and reviewing the displayed prompt materials."
          },
          {
            "url_name": "edit_bundle",
            "confidence": 0.3,
            "rationale": "Allows editing localisation bundles tied to annotation prompts and examples, tangentially relevant to maintaining the templates shown."
          }
        ],
        "concept_tags": [
          "prompt templates",
          "few-shot examples",
          "MWE annotation",
          "glossing",
          "DALL-E 3 image generation",
          "style specification",
          "visual element description",
          "page image specification",
          "image analysis",
          "annotation workflow"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 17779,
        "completion_tokens": 708,
        "total_tokens": 18487,
        "estimated_cost_usd": 0.02930375
      },
      "has_tex": true
    }
  ],
  "usage_totals": {
    "prompt_tokens": 950833,
    "completion_tokens": 49461,
    "total_tokens": 1000294,
    "estimated_cost_usd": 1.683151
  }
}