{
  "publication_id": "clara_third_report_2025",
  "created_at": "2025-12-29T11:51:58.039842+00:00",
  "source_zip": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\data\\overleaf_zips\\ThirdReport.zip",
  "source_zip_sha256": "74eb329c994795fbf391d98716debfcb53784ca2df6d40143fc09be25bf76633",
  "root_tex": "clara_third_report.tex",
  "flattened_tex": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\publications\\clara_third_report_2025\\flattened.tex",
  "model": "gpt-5.1-codex-max",
  "sections_count": 63,
  "candidate_top_k": 100,
  "sections": [
    {
      "section_id": "d21b4a64a2d8656a",
      "level": 1,
      "title": "'Abstract'",
      "label": null,
      "plain_text_len": 1947,
      "plain_text_excerpt": "Abstract\n\nabstract\n\nThis report presents an overview of progress during the period March 2024--April 2025 on ChatGPT-Based Learning And Reading Assistant (C-LARA), an open source online platform which supports creation of multimodal texts for language learners that integrate audio, images, glosses and other annotations. Building on earlier work, we use GPT-4o and other Lange Language Models to automate most or all of the annotation, guided by pedagogical needs and exploiting new AI capabilities. A central goal of the project is to explore how modern AI can act as a collaborative partner in research projects of this kind.\n\nOver the past year, our principle achievements are the following:\n\nitemize\n More accurate annotation: A principled treatment of multi-word expressions (MWEs) integrating segment-level translation now halves error rates in English glossing.\n Flexible image generation: We…",
      "analysis": {
        "section_summary": "An abstract outlining progress in C-LARA from March 2024 to April 2025, highlighting advances in automated multimodal text creation for language learners using GPT-4o and other LLMs. Key achievements include improved annotation through principled multi‑word expression handling, a coherent image generation pipeline, faster processing via parallelisation, enhanced support for Indigenous languages with manual annotation modes, and leveraging AI for software engineering and academic writing. The project aims to make AI a collaborative partner and foresees near-term capability to produce high-quality annotated multimedia content autonomously.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.44,
            "rationale": "The abstract mentions introducing a pipeline for generating coherent sets of images; edit_images_v2 handles configuring and managing coherent images for a CLARA project."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.38,
            "rationale": "Monitoring and status endpoints for coherent image generation align with the abstract’s discussion of coherent image pipelines."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.38,
            "rationale": "Status reporting for coherent image generation supports the flexible image generation pipeline described."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.32,
            "rationale": "Improved annotation via principled treatment of multi-word expressions is reflected in views creating MWE-tagged texts."
          }
        ],
        "concept_tags": [
          "C-LARA",
          "multimodal texts",
          "language learning",
          "GPT-4o",
          "large language models",
          "automated annotation",
          "multi-word expressions",
          "coherent image generation",
          "parallel processing",
          "Indigenous languages",
          "AI-assisted software engineering",
          "AI academic writing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15048,
        "completion_tokens": 985,
        "total_tokens": 16033,
        "estimated_cost_usd": 0.02866
      },
      "has_tex": true
    },
    {
      "section_id": "04304c6b44dd461d",
      "level": 1,
      "title": "'How to access C-LARA'",
      "label": null,
      "plain_text_len": 94,
      "plain_text_excerpt": "How to access C-LARA\n\nIf you want to experiment with C-LARA, we recommend doing the following:",
      "analysis": {
        "section_summary": "This brief section introduces how to get started with C-LARA, inviting readers to experiment with the system and outlining initial steps to access it.",
        "relevant_views": [
          {
            "url_name": "home",
            "confidence": 0.18,
            "rationale": "The section is about accessing and starting with C-LARA; the home view provides the main landing page to begin exploring the system."
          }
        ],
        "concept_tags": [
          "access",
          "getting started",
          "C-LARA",
          "experiment"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14799,
        "completion_tokens": 451,
        "total_tokens": 15250,
        "estimated_cost_usd": 0.02300875
      },
      "has_tex": true
    },
    {
      "section_id": "ae43069630e030ff",
      "level": 2,
      "title": "'How to access C-LARA > Create an account'",
      "label": null,
      "plain_text_len": 310,
      "plain_text_excerpt": "Create an account\n\nGo to the C-LARA login page, https://c-lara.unisa.edu.au/accounts/login/. Use the Register tab to create an account. You need to supply an email address to be able to use password retrieval and email confirmation, and receive email updates. We will not share your address with third parties.",
      "analysis": {
        "section_summary": "This section explains how to create a C-LARA account by going to the login page and using the Register tab. It notes you must provide an email address to enable password retrieval, email confirmation, and updates, and reassures that the address will not be shared.",
        "relevant_views": [
          {
            "url_name": "register",
            "confidence": 0.93,
            "rationale": "The section instructs users to use the Register tab on the login page to create an account, which corresponds to the registration view handling new user signup and email-based setup."
          }
        ],
        "concept_tags": [
          "account registration",
          "C-LARA login",
          "email address",
          "password retrieval",
          "email confirmation",
          "privacy"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14995,
        "completion_tokens": 304,
        "total_tokens": 15299,
        "estimated_cost_usd": 0.02178375
      },
      "has_tex": true
    },
    {
      "section_id": "f6d4837a0bd1ddaa",
      "level": 2,
      "title": "'How to access C-LARA > Register an API key or submit a funding request'",
      "label": null,
      "plain_text_len": 1550,
      "plain_text_excerpt": "Register an API key or submit a funding request\n\nCreating an account is free and language learners can use it to read existing C-LARA content and do a lot of other things. But the AI-based functionality which permits content creation and is the heart of the platform uses GPT-4 and other OpenAI models, which costs money. In order to access these functions, in particular to use the AI to create multimodal content, you need to be able to pay for the OpenAI calls. Independent language learners may well wish to create their own tailored content. It is easy and fast as the AI does most of the work. \n\nThe preferred solution is to use an OpenAI API key valid for GPT-4. Open a ChatGPT account at https://chat.openai.com/auth/login if you do not already have one and get an API key from https://platform.openai.com/api-keys. Then on C-LARA, go to User profile etc $>$ Edit configuration information an…",
      "analysis": {
        "section_summary": "Explains that C-LARA accounts are free, but AI-based content creation requires paying for GPT-4/OpenAI calls. Users should obtain an OpenAI API key (after creating a ChatGPT/OpenAI account) and enter it in their C-LARA configuration so charges go to their own account. If obtaining an API key isn’t possible, users can submit a small funding request via the Social network tab to receive limited credit for experimenting.",
        "relevant_views": [
          {
            "url_name": "user_config",
            "confidence": 0.74,
            "rationale": "The section instructs users to enter their OpenAI API key under “Edit configuration information,” which is handled by the user configuration view."
          },
          {
            "url_name": "funding_request",
            "confidence": 0.64,
            "rationale": "For users unable to use their own API key, the section directs them to submit a funding request via the Social network tab, matching the funding_request view."
          }
        ],
        "concept_tags": [
          "OpenAI API key",
          "GPT-4",
          "AI-based content creation",
          "User configuration",
          "ChatGPT account",
          "Funding request",
          "Credit for OpenAI calls"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15099,
        "completion_tokens": 470,
        "total_tokens": 15569,
        "estimated_cost_usd": 0.02357375
      },
      "has_tex": true
    },
    {
      "section_id": "6a53ea0e82ea11de",
      "level": 2,
      "title": "\"How to access C-LARA > Use ``Simple C-LARA'' to create a piece of content\"",
      "label": null,
      "plain_text_len": 566,
      "plain_text_excerpt": "Use ``Simple C-LARA'' to create a piece of content\n\nFollow the steps in Appendix~[REF:Section:UsingSimpleCLARA] to enter ``Simple C-LARA'' and create a piece of content. You just need to specify the languages and supply an initial prompt saying what you want C-LARA to write, and the AI does the rest for you.\n\nWhen you have created some content, consider filling out the satisfaction questionnaire at the end. It should take a couple of minutes. Don't feel you have to be nice. If you tell us about things that did not work, it is more likely that we will fix them.",
      "analysis": {
        "section_summary": "The section advises entering the Simple C‑LARA wizard to generate a piece of content: choose your languages, provide an initial prompt, and let the AI create the text. Afterward, you're encouraged to complete a brief satisfaction questionnaire, sharing honest feedback on what worked and what did not.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.8,
            "rationale": "The Simple C-LARA view launches the wizard where users specify languages and a prompt to create AI-generated content as described."
          },
          {
            "url_name": "satisfaction_questionnaire",
            "confidence": 0.46,
            "rationale": "Users are asked to fill out a satisfaction questionnaire after creating content, which is handled by this questionnaire view."
          }
        ],
        "concept_tags": [
          "Simple C-LARA",
          "AI-generated content",
          "Language selection",
          "User prompt",
          "Satisfaction questionnaire",
          "User feedback"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15189,
        "completion_tokens": 483,
        "total_tokens": 15672,
        "estimated_cost_usd": 0.02381625
      },
      "has_tex": true
    },
    {
      "section_id": "f50d0540bc2a95ed",
      "level": 1,
      "title": "'Introduction and overview'",
      "label": "Section:Introduction",
      "plain_text_len": 46,
      "plain_text_excerpt": "Introduction and overview\nSection:Introduction",
      "analysis": {
        "section_summary": "This section serves as the introduction and overview to the report, likely outlining the context, purpose, and structure of the document.",
        "relevant_views": [],
        "concept_tags": [
          "Introduction",
          "Overview",
          "Report structure",
          "Context setting"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14059,
        "completion_tokens": 136,
        "total_tokens": 14195,
        "estimated_cost_usd": 0.01893375
      },
      "has_tex": true
    },
    {
      "section_id": "8aeb5f98a233f1c3",
      "level": 2,
      "title": "'Introduction and overview > Background and overall goals'",
      "label": "Section:Background",
      "plain_text_len": 4402,
      "plain_text_excerpt": "Background and overall goals\nSection:Background\n\nChatGPT-Based Learning And Reading Assistant (C-LARA; https://www.c-lara.org/) is an international open source project inaugurated in March 2023. Building on the earlier LARA project [CITE:LARA2019SLaTE], one of the top-level goals was the same: we aimed to create a online platform that would support construction of multimodal texts useful for language learners who wished to improve their reading and listening skills. A text of this kind would contain annotations typically including audio files, images, word glosses, a concordance, and maybe other things.\n\nThe recent release of GPT-4, however, opened up new possibilities. In LARA, a large part of the work involved in building a high-quality multimodal text had to be performed manually. It seemed to us that GPT-4's intelligence was at a completely different level to previous AI software, an…",
      "analysis": {
        "section_summary": "This introductory section outlines C-LARA as a GPT-4–based, open-source platform for building multimodal language learning texts with audio, images, glosses and concordances, expanding on the earlier LARA project. It highlights ambitions to leverage GPT-4 both for automating annotation and software development, notes early deployment and experience, and frames key challenges for the next phase: handling multi-word expressions in annotations, producing coherent image sets, improving processing speed, supporting manual work (especially for Indigenous languages), assessing AI on large codebases, and exploring longer-form academic text generation.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.49,
            "rationale": "The section identifies multi-word expressions as a major obstacle in annotation quality, pointing to functionality for creating MWE-tagged texts."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.42,
            "rationale": "Addressing the need for coherent, styled images across texts aligns with views for configuring and managing coherent image generation."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.35,
            "rationale": "The core goal is to build multimodal C-LARA projects; the Simple C-LARA wizard is a primary entry point for creating such projects."
          }
        ],
        "concept_tags": [
          "C-LARA",
          "LARA",
          "GPT-4",
          "multimodal texts",
          "language learning",
          "reading skills",
          "listening skills",
          "annotations",
          "audio",
          "images",
          "word glosses",
          "concordance",
          "multi-word expressions",
          "MWEs",
          "image coherence",
          "processing speed",
          "Indigenous languages",
          "manual annotation",
          "AI-assisted software engineering",
          "long-form text generation",
          "academic writing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15652,
        "completion_tokens": 1325,
        "total_tokens": 16977,
        "estimated_cost_usd": 0.032815
      },
      "has_tex": true
    },
    {
      "section_id": "9f258a8d9557401b",
      "level": 2,
      "title": "'Introduction and overview > Main results'",
      "label": "Section:Results",
      "plain_text_len": 2180,
      "plain_text_excerpt": "Main results\nSection:Results\n\nDuring the last year, we made good progress on all of the above issues.\n\ndescription\n\n We have implemented a principled treatment of Multi-Word Expressions, annotated using a Chain of Thought method (). Combining the MWE information with segment translation annotations, also added during this phase, we reduce the average glossing annotation error rate in English by more than 50\\%. MWEs are now displayed as single units in the final multimodal form.\n\n We have implemented a method for creation of coherent sets of images (), which in turn defines the style, the recurrent visual elements, and finally the page images. Coherence in terms of style is generally good. Coherence in terms of content---the recurrent visual elements---is still unsatisfactory with the deployed version of the system. It seems likely, however, that improved models currently in the process o…",
      "analysis": {
        "section_summary": "The report highlights progress over the past year: a principled chain-of-thought annotation for multi-word expressions combined with segment translations halved glossing errors and now renders MWEs as single units; a new pipeline for generating coherent image sets defines styles, recurring elements and page images, with good stylistic coherence and pending improvements in content coherence; parallelisation of costly tasks yields roughly tenfold speedups; a page-organised editing mode tailored for Indigenous languages adds automatic consistency checks; and rapid gains in AI capability mean newer models now produce high-quality Django code and academic prose with minimal human correction.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.47,
            "rationale": "Implements creation and maintenance of multi-word expression tagged texts, aligning with the reported MWE annotation improvements."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.53,
            "rationale": "Provides the interface for configuring coherent image sets (styles, elements, page images), matching the new coherent image generation method described."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.38,
            "rationale": "Supports monitoring asynchronous generation of styles, elements, and pages in the coherent images workflow mentioned in the section."
          }
        ],
        "concept_tags": [
          "multi-word expressions",
          "chain-of-thought annotation",
          "glossing error reduction",
          "segment translation",
          "coherent image generation",
          "style consistency",
          "recurrent visual elements",
          "parallel processing",
          "indigenous language support",
          "annotation consistency checks",
          "AI code generation",
          "GPT-4o",
          "o1 model",
          "academic writing automation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15156,
        "completion_tokens": 1103,
        "total_tokens": 16259,
        "estimated_cost_usd": 0.029975
      },
      "has_tex": true
    },
    {
      "section_id": "c53f81fc1f9723d2",
      "level": 1,
      "title": "'Related work'",
      "label": "Section:RelatedWork",
      "plain_text_len": 554,
      "plain_text_excerpt": "Related work\nSection:RelatedWork\n\nA growing body of research in computer-assisted language learning (CALL) highlights the importance of annotated, multimodal texts to increase reading comprehension and vocabulary acquisition [CITE:plonsky2016call]. Traditional platforms have long offered dictionary-based or teacher-curated annotations, while more recent systems incorporate artificial intelligence (AI) to automate tasks such as glossing, text generation, and image creation. Below, we summarize key developments and representative tools in this space.",
      "analysis": {
        "section_summary": "The related work surveys computer-assisted language learning systems that use annotated, multimodal texts to boost comprehension and vocabulary, noting a shift from manually curated annotations to AI-driven automation for glossing, text generation, and image creation.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.44,
            "rationale": "Provides AI- or tagger-assisted creation of glossed text versions, aligning with work on automated glossing for annotated reading."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.43,
            "rationale": "Supports automatic lemmatization and glossing, reflecting tools that generate annotated texts to aid comprehension."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.41,
            "rationale": "Enables automated lemma tagging of texts, relevant to systems using linguistic annotation to scaffold reading."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.37,
            "rationale": "Handles multiword expression tagging as part of automated annotation pipelines highlighted in CALL research."
          },
          {
            "url_name": "create_pinyin_tagged_text",
            "confidence": 0.35,
            "rationale": "Generates pinyin-tagged texts, an example of automated phonetic glossing to support vocabulary acquisition."
          },
          {
            "url_name": "create_phonetic_text",
            "confidence": 0.36,
            "rationale": "Creates phonetic annotations, matching the emphasis on annotated multimodal texts in related work."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.34,
            "rationale": "Automates text segmentation, a common preprocessing step for annotated reading materials."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.33,
            "rationale": "Supports generation of translated text versions, aligning with AI-assisted annotation and glossing."
          },
          {
            "url_name": "generate_text_status",
            "confidence": 0.38,
            "rationale": "Monitors AI-driven text generation tasks, directly tied to automated text creation discussed in the section."
          },
          {
            "url_name": "generate_text_monitor",
            "confidence": 0.38,
            "rationale": "Provides status monitoring for AI text generation, reflecting systems that automate text production."
          },
          {
            "url_name": "generate_text_complete",
            "confidence": 0.38,
            "rationale": "Handles completion of AI text generation workflows, relevant to automated content creation noted in related work."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.4,
            "rationale": "Supports configuring and generating coherent images via AI, matching the mention of AI-based image creation."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.39,
            "rationale": "Status endpoint for AI image generation tasks, aligning with automated image creation in multimodal CALL tools."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.39,
            "rationale": "Monitors AI-driven image generation processes, reflecting the section’s focus on automated image creation."
          },
          {
            "url_name": "create_dall_e_3_image_status",
            "confidence": 0.37,
            "rationale": "Tracks DALL-E-based image generation, an example of AI image creation noted in related work."
          },
          {
            "url_name": "create_dall_e_3_image_monitor",
            "confidence": 0.37,
            "rationale": "Monitors OpenAI image generation tasks, relevant to automated image creation in annotated multimodal systems."
          },
          {
            "url_name": "edit_images",
            "confidence": 0.35,
            "rationale": "Enables image editing and triggering AI generation tasks, supporting the multimodal text paradigm discussed."
          }
        ],
        "concept_tags": [
          "CALL",
          "annotated texts",
          "glossing",
          "lemmatization",
          "phonetic annotation",
          "text segmentation",
          "AI-assisted text generation",
          "image generation",
          "multimodal learning",
          "reading comprehension",
          "vocabulary acquisition"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14423,
        "completion_tokens": 1937,
        "total_tokens": 16360,
        "estimated_cost_usd": 0.03739875
      },
      "has_tex": true
    },
    {
      "section_id": "48746c04f4e25bf8",
      "level": 2,
      "title": "'Related work > Annotated Reading and Glossing Tools'",
      "label": null,
      "plain_text_len": 1613,
      "plain_text_excerpt": "Annotated Reading and Glossing Tools\nSeveral open-source platforms focus on providing word- or phrase-level annotations to help learners read authentic texts with minimal interruption. Learning With Texts (LWT)https://learning-with-texts.sourceforge.io/ allows users to import any target-language text, tokenize it automatically, and click on unfamiliar words to look them up in an external dictionary. Translations and notes can then be saved to build a personal vocabulary database. A similar approach underpins LingQhttps://www.lingq.com/ and Readlanghttps://readlang.com/, both of which offer browser-based or mobile interfaces where users can translate words on the fly and create flashcards for spaced repetition review. By significantly reducing “lookup friction,” these systems encourage extensive reading in the target language.\n\nAnother example is Clilstorehttps://multidict.net/clilstore/,…",
      "analysis": {
        "section_summary": "The section reviews open-source annotated reading tools that provide word- or phrase-level glosses to ease comprehension of authentic texts. It highlights platforms like Learning With Texts, LingQ, and Readlang, which allow on-the-fly dictionary lookups and vocabulary building, and Clilstore with Wordlink for linking each word in transcripts to online dictionaries. These tools reduce lookup friction and encourage extensive reading, but static glosses can struggle with idiomatic or context-sensitive expressions.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.45,
            "rationale": "This view supports creating glossed versions of texts in C-LARA, aligning with tools that provide word-level annotations to aid reading."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.4,
            "rationale": "Generates texts tagged with lemmas and glosses, similar to dictionary-linked annotations discussed in the section."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.35,
            "rationale": "Allows creation of lemma-tagged texts, which underpin word-level lookup and glossing functionality akin to the reviewed tools."
          }
        ],
        "concept_tags": [
          "annotated reading",
          "glossing",
          "dictionary lookup",
          "Learning With Texts",
          "LingQ",
          "Readlang",
          "Clilstore",
          "Wordlink",
          "Content and Language Integrated Learning",
          "spaced repetition"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14902,
        "completion_tokens": 992,
        "total_tokens": 15894,
        "estimated_cost_usd": 0.0285475
      },
      "has_tex": true
    },
    {
      "section_id": "4aec95d317339757",
      "level": 2,
      "title": "'Related work > Multimodal Approaches'",
      "label": null,
      "plain_text_len": 1212,
      "plain_text_excerpt": "Multimodal Approaches\nTo provide richer context, platforms increasingly integrate audio and visuals alongside text. Many systems, including Clilstore and LingQ, support synchronized audio playback, allowing learners to listen to a native or text-to-speech (TTS) recording while following the transcript. Some tools highlight each sentence or phrase in “karaoke” style, improving comprehension and aiding pronunciation. Commercial apps such as Beelinguapphttps://beelinguapp.com/ focus on parallel text with audiobook narration, whereas projects like Duolingo Storieshttps://blog.duolingo.com/introducing-duolingo-stories/ embed illustrated narrative panels with native-speaker audio, eliciting users’ comprehension through mini-quizzes.\n\nImage and video integration often relies on manual embedding or linking; however, research suggests that visual context can boost learner engagement and recall [C…",
      "analysis": {
        "section_summary": "This section surveys how language learning tools increasingly combine text with audio and visuals. It notes synchronized audio playback and karaoke-style highlighting in platforms like Clilstore and LingQ, parallel texts with audiobook narration as in Beelinguapp, and illustrated, audio-supported stories with comprehension quizzes such as Duolingo Stories. It highlights research on the benefits of visual context and emerging experiments with AI-driven, automatic image generation tied to textual segments, pointing to more immersive, multimodal reading experiences.",
        "relevant_views": [
          {
            "url_name": "serve_audio_file",
            "confidence": 0.48,
            "rationale": "Provides direct delivery of audio assets, enabling synchronized playback alongside text as described for multimodal reading."
          },
          {
            "url_name": "human_audio_processing",
            "confidence": 0.36,
            "rationale": "Handles collecting and processing human audio for projects, supporting native recordings that align with transcripts for listening practice."
          },
          {
            "url_name": "generate_audio_metadata",
            "confidence": 0.35,
            "rationale": "Generates metadata needed to align audio with text, facilitating karaoke-style highlighting and synchronized playback."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.55,
            "rationale": "Manages creation and configuration of coherent images for projects, relevant to embedding illustrations alongside text segments."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.42,
            "rationale": "Monitors AI-driven coherent image generation keyed to text, aligning with trends toward automatic visual augmentation."
          },
          {
            "url_name": "serve_clara_image",
            "confidence": 0.4,
            "rationale": "Serves generated images over HTTP, enabling visual context to be displayed with text in multimodal interfaces."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.38,
            "rationale": "Delivers coherent image assets produced for texts, supporting the integration of automatically generated visuals."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.35,
            "rationale": "Monitors background tasks for generating images in Simple C-LARA, reflecting experimentation with automatic illustration."
          }
        ],
        "concept_tags": [
          "multimodal learning",
          "synchronized audio playback",
          "karaoke highlighting",
          "text-to-speech",
          "native speaker audio",
          "parallel text",
          "audiobook narration",
          "illustrated narratives",
          "automatic image generation",
          "visual context",
          "learner engagement",
          "immersive reading"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14639,
        "completion_tokens": 1429,
        "total_tokens": 16068,
        "estimated_cost_usd": 0.03258875
      },
      "has_tex": true
    },
    {
      "section_id": "bf85f524049b4442",
      "level": 2,
      "title": "'Related work > AI-Enhanced Tools and Content Creation'",
      "label": null,
      "plain_text_len": 1550,
      "plain_text_excerpt": "AI-Enhanced Tools and Content Creation\nWhile dictionary-based or rule-based annotations remain popular, AI-driven features are growing rapidly. Commercial services like Quizlet Q-Chathttps://quizlet.com/blog/q-chat and Duolingo’s AI chatbots demonstrate how large language models (LLMs) can handle learner queries, explain tricky phrases, or create practice dialogues on the fly. However, few platforms go as far as providing entirely AI-generated reading content with automated annotations, which is the territory of emerging systems such as StoryWizard AIhttps://www.storywizard.ai/ (illustrated children’s stories) and advanced open-source prototypes like Multimodal-GPThttps://github.com/open-mmlab/Multimodal-GPT. These approaches aim to reduce teacher workload by generating text, glosses, and even morphological analysis without extensive human intervention.\n\nEarly evaluations of AI-based glo…",
      "analysis": {
        "section_summary": "The section surveys the rise of AI-driven tools in language learning, highlighting how LLMs power conversational helpers and can generate reading texts with automated annotations, glosses, and images. It notes emerging systems that create end-to-end illustrated content, and early findings that AI glossing is promising but requires human review, especially for complex morphology or idioms, suggesting a hybrid human–AI workflow to balance accuracy and nuance while mitigating hallucinations.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.72,
            "rationale": "Supports creating glossed text versions with options for AI-driven generation and correction, aligning with AI-based glossing and automated annotations discussed."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.68,
            "rationale": "Enables generating lemma-tagged texts via AI/taggers, relevant to automated morphological analysis mentioned in AI annotation workflows."
          },
          {
            "url_name": "edit_prompt",
            "confidence": 0.64,
            "rationale": "Manages GPT-4 prompt templates for annotations, connecting to AI-driven glossing and content generation workflows."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.56,
            "rationale": "Simple C-LARA orchestrates AI-based creation of texts and images, similar to systems generating end-to-end illustrated content."
          }
        ],
        "concept_tags": [
          "AI-generated content",
          "large language models",
          "automated glossing",
          "morphological analysis",
          "hybrid human–AI workflow",
          "computer-assisted language learning",
          "context-sensitive explanations",
          "hallucinations",
          "teacher workload reduction",
          "multimodal AI"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14983,
        "completion_tokens": 783,
        "total_tokens": 15766,
        "estimated_cost_usd": 0.02655875
      },
      "has_tex": true
    },
    {
      "section_id": "44a72c653c4106fc",
      "level": 2,
      "title": "'Related work > Summary'",
      "label": null,
      "plain_text_len": 696,
      "plain_text_excerpt": "Summary\nAcross open-source and commercial solutions, the consensus is that annotated, multimodal texts can significantly enhance reading comprehension and vocabulary uptake. Platforms like LWT and LingQ have validated the utility of rapid dictionary lookups and integrated flashcards, while Clilstore and Duolingo demonstrate how audio or video can bolster textual understanding. With the emergence of AI-assisted generation and glossing, tools are beginning to reduce the time and expertise required to create high-quality learning materials. The next sections discuss how our project builds on these ideas to offer a more scalable, community-driven approach to AI-facilitated annotated reading.",
      "analysis": {
        "section_summary": "The section highlights that both open-source and commercial platforms show annotated, multimodal texts improve reading comprehension and vocabulary by enabling quick dictionary lookups, flashcards, and embedding audio or video. AI-assisted generation and glossing are emerging to lower the effort and expertise needed to produce quality learning materials, setting the stage for the project’s scalable, community-driven approach to AI-supported annotated reading.",
        "relevant_views": [],
        "concept_tags": [
          "annotated texts",
          "multimodal learning",
          "reading comprehension",
          "vocabulary acquisition",
          "dictionary lookup",
          "flashcards",
          "audio integration",
          "video integration",
          "AI-assisted generation",
          "glossing",
          "community-driven learning",
          "language learning platforms"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14691,
        "completion_tokens": 621,
        "total_tokens": 15312,
        "estimated_cost_usd": 0.02457375
      },
      "has_tex": true
    },
    {
      "section_id": "4cbabc5d2368223c",
      "level": 1,
      "title": "'New functionality'",
      "label": "Section:NewFunctionality",
      "plain_text_len": 888,
      "plain_text_excerpt": "New functionality\nSection:NewFunctionality\n\nIn this section, we describe new functionality added between March 2024 and March 2025. Specifically, we consider the following:\ndescription\n\n Linguistic annotation has been much improved. In particular, it now includes a systematic treatment of Multi-Word Expressions.\n\n There is now good support for creation of texts with multiple images, where the images are aligned to be reasonably coherent with each other in terms of style and content.\n\n Both linguistic annotation and creation of images have been now use parallel processing, making them much faster.\n\n Simple C-LARA has been upgraded to incorporate most of the above functionality. In particular, it can be used to create texts that include annotations of MWEs and coherent image sets.\n\n The interface for editing of Indigenous language texts has been completely revised.\n\ndescription",
      "analysis": {
        "section_summary": "Between March 2024 and March 2025 the platform added much faster, parallelised linguistic annotation with systematic handling of multi‑word expressions, introduced coherent multi‑image generation aligned in style and content, upgraded Simple C‑LARA to support these annotations and coherent image sets, and completely revamped the interface for editing Indigenous language texts.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.78,
            "rationale": "Provides configuration and generation of coherent image sets with aligned style/content, matching the new multi‑image support."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.7,
            "rationale": "Monitors asynchronous generation tasks for coherent images, reflecting the parallelised image creation mentioned."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.68,
            "rationale": "Returns status for coherent image generation tasks, relevant to the new faster, parallel processing of images."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.8,
            "rationale": "Simple C‑LARA was upgraded to include coherent images and MWE annotations; this view drives the Simple C‑LARA workflow."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.65,
            "rationale": "Monitors Simple C‑LARA background tasks, aligning with the added parallel processing of annotation and image creation."
          },
          {
            "url_name": "edit_prompt",
            "confidence": 0.5,
            "rationale": "Allows language masters to customise annotation prompts, relevant to improved linguistic annotation including MWEs."
          }
        ],
        "concept_tags": [
          "linguistic annotation",
          "multi-word expressions",
          "parallel processing",
          "coherent images",
          "Simple C-LARA",
          "Indigenous language texts",
          "annotation prompts",
          "image generation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15148,
        "completion_tokens": 1602,
        "total_tokens": 16750,
        "estimated_cost_usd": 0.034955
      },
      "has_tex": true
    },
    {
      "section_id": "dfb5c34d45418172",
      "level": 2,
      "title": "'New functionality > Improvements to linguistic annotation'",
      "label": "Section:BetterAnnotation",
      "plain_text_len": 593,
      "plain_text_excerpt": "Improvements to linguistic annotation \nSection:BetterAnnotation\n\nIn the second C-LARA report, we evaluated annotation performance and identified poor handling of Multi-Word Expressions (MWEs) as the dominant issue. Here, we describe the principled solution we have implemented, which involves addition of a new annotation phase specifically for identification of MWEs. We have also added an annotation phase for addition of segment translations. We explain how these two new phases feed into processing for lemma-tagging and glossing, and also summarise improvements to segmentation and audio.",
      "analysis": {
        "section_summary": "The report outlines enhancements to linguistic annotation by adding new dedicated phases: one for identifying multi‑word expressions and another for adding translations at the segment level. These feed into improved lemma tagging and glossing workflows, alongside refinements to segmentation and audio handling.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.6,
            "rationale": "Supports creation of MWE-tagged text, directly addressing the new phase for identifying multi-word expressions."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.45,
            "rationale": "Creates lemma-tagged versions of texts, which the new MWE and segment translation phases feed into."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.45,
            "rationale": "Handles glossed text generation, benefiting from improved lemma tagging and segment translation inputs."
          },
          {
            "url_name": "create_glossed_text_from_lemma",
            "confidence": 0.4,
            "rationale": "Generates glossed text from lemma annotations, aligning with the described pipeline improvements."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.4,
            "rationale": "Combines lemma tagging and glossing in one step, reflecting the integrated annotation workflow enhancements."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.35,
            "rationale": "Facilitates text segmentation, relevant to the noted improvements to segmentation and segment translations."
          }
        ],
        "concept_tags": [
          "linguistic annotation",
          "multi-word expressions",
          "MWE identification",
          "segment translations",
          "segmentation",
          "lemma tagging",
          "glossing",
          "annotation pipeline"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15120,
        "completion_tokens": 1131,
        "total_tokens": 16251,
        "estimated_cost_usd": 0.03021
      },
      "has_tex": true
    },
    {
      "section_id": "6683657665748368",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Annotating Multi-Wor'",
      "label": "Section:MWEs",
      "plain_text_len": 882,
      "plain_text_excerpt": "Annotating Multi-Word Expressions \nSection:MWEs\n\nAfter some initial experimentation, we determined that a Chain of Thought approach seemed most suitable for annotating MWEs. We process each segment separately, using the prompt template shown in Figure~[REF:Figure:MWEPromptTemplate]. The prompt template contains slots for the text language, the text in JSON form, and a list of few-shot examples; a typical few-shot example for English is shown in Figure~[REF:Figure:MWEPromptExamples]. The few shot-examples are created in a bootstrapped manner by running the prompt on gpt-4o with an initial set of two handcrafted examples and editing the resulting output.\n\nInitial testing and prompt engineering was done using two annotated English Sherlock Holmes stories from the NTU-MC []bond-etal-2021-teaching. These include MWEs from an extended version of wordnet [CITE:_Fellbaum:1998].",
      "analysis": {
        "section_summary": "This section describes a chain-of-thought prompting approach to annotate multi-word expressions. Each text segment is processed with a prompt template that specifies the language, JSON-formatted text, and bootstrapped few-shot examples (initially handcrafted and refined via GPT-4o). Initial experiments used two annotated English Sherlock Holmes stories (NTU-MC), leveraging MWEs from an extended WordNet.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.86,
            "rationale": "This view handles creation and maintenance of MWE-tagged text versions, matching the section’s focus on MWE annotation workflows."
          }
        ],
        "concept_tags": [
          "multi-word expressions",
          "chain of thought prompting",
          "few-shot learning",
          "prompt engineering",
          "GPT-4o",
          "segment-level annotation",
          "bootstrapping examples",
          "Sherlock Holmes corpus",
          "NTU-MC dataset",
          "WordNet"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14733,
        "completion_tokens": 614,
        "total_tokens": 15347,
        "estimated_cost_usd": 0.02455625
      },
      "has_tex": true
    },
    {
      "section_id": "2d81bb7abbc66856",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Segment translations'",
      "label": "Section:Translations",
      "plain_text_len": 464,
      "plain_text_excerpt": "Segment translations \nSection:Translations\n\nWe have also added a new annotation phase which creates translations for segments. The segments are divided into groups, currently of about 250 words each, and submitted to gpt-4o using a prompt which provides minimal instructions about format. As well as being useful in itself, the segment translation are also used for glossing, as described in the immediately following section, and sometimes for creation of images.",
      "analysis": {
        "section_summary": "The report introduces a new annotation phase that batches text segments of roughly 250 words and submits them to GPT-4o with minimal formatting instructions to generate translations. These segment-level translations are valuable on their own and also feed into subsequent glossing and sometimes image generation workflows.",
        "relevant_views": [
          {
            "url_name": "create_translated_text",
            "confidence": 0.58,
            "rationale": "This view handles creating and maintaining translated text annotations for CLARA projects, aligning with the new segment translation phase described."
          }
        ],
        "concept_tags": [
          "segment translations",
          "GPT-4o",
          "annotation phase",
          "segment batching",
          "glossing",
          "image generation",
          "translation workflow"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14909,
        "completion_tokens": 432,
        "total_tokens": 15341,
        "estimated_cost_usd": 0.02295625
      },
      "has_tex": true
    },
    {
      "section_id": "82c12d39cb6773e1",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Glossing and lemma t'",
      "label": "Section:GlossingLemmaTagging",
      "plain_text_len": 899,
      "plain_text_excerpt": "Glossing and lemma tagging \nSection:GlossingLemmaTagging\n\nThe point of adding MWE information is that it gives us options for improving glossing and lemma tagging. If a word is part of an MWE, we want the associated gloss and lemma to refer to the MWE, not the word itself. For glossing, we find it is also useful to make available the segment translation information and the full text of the document, in order to provide context.\n\nWe perform the glossing and lemma tagging operations on each segment separately. We experimented with several different ways of passing in the MWE information; the one that worked best was to present the segment as a JSON list with one element per lexical item, and the MWE information paired with the words it refers to. The prompt template is shown in Figure~[REF:Figure:GlossPromptTemplate].\n\nThe prompt template for lemma tagging is similar but a little simpler.",
      "analysis": {
        "section_summary": "Describes how multiword expression annotations are used to improve glossing and lemma tagging: if a token is part of an MWE, its gloss and lemma should target the whole expression. Glossing also benefits from segment translations and full-document context. Both glossing and lemma tagging are run per segment, with the best input format being a JSON list of lexical items with attached MWE info; the gloss prompt template reflects this, and the lemma prompt is similar but simpler.",
        "relevant_views": [
          {
            "url_name": "create_glossed_text",
            "confidence": 0.85,
            "rationale": "Provides the workflow to generate or edit glossed text versions, aligning with the section’s focus on improved glossing using MWE and context."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.83,
            "rationale": "Supports creating lemma-tagged text, matching the described lemma tagging improvements informed by MWE data."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.84,
            "rationale": "Combines glossing and lemma tagging in one process, directly relevant to jointly applying the MWE-aware prompts discussed."
          },
          {
            "url_name": "create_glossed_text_from_lemma",
            "confidence": 0.7,
            "rationale": "Derives glossing from existing lemma annotations, connected to the improved glossing pipeline described."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.67,
            "rationale": "Handles MWE tagging, which supplies the MWE information used to drive the improved glossing and lemma tagging."
          }
        ],
        "concept_tags": [
          "mwe",
          "glossing",
          "lemma tagging",
          "json input",
          "annotation prompt",
          "segment context"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14915,
        "completion_tokens": 1070,
        "total_tokens": 15985,
        "estimated_cost_usd": 0.02934375
      },
      "has_tex": true
    },
    {
      "section_id": "c3b54ce18e8b33c8",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Displaying the final'",
      "label": "Section:DisplayingMultimedia",
      "plain_text_len": 742,
      "plain_text_excerpt": "Displaying the final multimedia text \nSection:DisplayingMultimedia\n\nTo present the MWEs in an intuitive way, the multimedia text contains JavaScript which modifies the highlighting behaviour to show MWEs as units; thus clicking on one word in an MWE highlights the other words as well. Similarly, when creating audio associated with words, a word that is a component of an MWE is linked to audio for the whole MWE, and when compiling the concordance the lemma that appears is one for the whole MWE.\n\nFigure~[REF:Figure:MWEHighlightingExample] illustrates.\n\nfigure\n \n TheCatAndTheBat/MWEHighlightingExample.jpg\n Example from ``The Cat and the Bat'' illustrating presentation of MWEs in the multimodal text\n Figure:MWEHighlightingExample\nfigure",
      "analysis": {
        "section_summary": "The final multimedia text now treats multiword expressions as unified units: JavaScript ensures that clicking one component highlights all, audio links a component to the whole expression’s audio, and concordance lemmas are those of the entire MWE, enhancing intuitive presentation.",
        "relevant_views": [
          {
            "url_name": "serve_rendered_text",
            "confidence": 0.64,
            "rationale": "Rendered multimedia texts with custom highlighting and audio are delivered via the rendered text serving endpoint, which would include the JavaScript behavior described for MWEs."
          }
        ],
        "concept_tags": [
          "multiword expressions",
          "MWE highlighting",
          "multimedia text",
          "audio linking",
          "concordance lemma",
          "JavaScript behavior"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14303,
        "completion_tokens": 689,
        "total_tokens": 14992,
        "estimated_cost_usd": 0.02476875
      },
      "has_tex": true
    },
    {
      "section_id": "1486c3f4b0ab6554",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Better segmentation'",
      "label": "Section:Segmentation",
      "plain_text_len": 870,
      "plain_text_excerpt": "Better segmentation \nSection:Segmentation\n\nThe segmentation phase is now divided into two subphases. In the first subphase, the AI is used to divide the text into pages, with each page divided into segments. The prompt recommends that the AI first determines the genre of the text (story, essay, poem, dictionary, etc), and then makes an appropriate subdivision. For example, in a story a typical page would be 1--3 paragraphs and a typical segment would be a sentence, while in a poem a typical page might be 1--3 verses and a typical segment would be a line. In the second phase, each segment is processed separately, using a language-dependent prompt which gives examples of how to subdivide words or mark words containing punctuation marks (hyphens, apostrophes) as single lexical units.\n\nThe new scheme gives much better results, particularly in the first subphase.",
      "analysis": {
        "section_summary": "The segmentation process now runs in two steps: first, an AI divides the text into pages and segments, choosing appropriate page and segment sizes based on the detected genre (e.g. paragraphs and sentences for stories, verses and lines for poems). Then each segment is processed with a language-specific prompt to split into lexical units and handle punctuation such as hyphens and apostrophes. This two-stage approach yields noticeably better segmentation results.",
        "relevant_views": [
          {
            "url_name": "create_segmented_text",
            "confidence": 0.69,
            "rationale": "This view handles creating and managing the segmented version of a CLARA text, which aligns with the improved two-phase segmentation described."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.27,
            "rationale": "The Simple C-LARA wizard orchestrates automatic project steps including segmentation, so its workflow is relevant to the described segmentation improvements."
          }
        ],
        "concept_tags": [
          "segmentation",
          "AI-assisted segmentation",
          "text genre detection",
          "page segmentation",
          "sentence segmentation",
          "lexical unit handling",
          "punctuation handling",
          "language-dependent prompts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14781,
        "completion_tokens": 670,
        "total_tokens": 15451,
        "estimated_cost_usd": 0.02517625
      },
      "has_tex": true
    },
    {
      "section_id": "50141d71b910b358",
      "level": 3,
      "title": "'New functionality > Improvements to linguistic annotation > Page audio'",
      "label": "Section:PageAudio",
      "plain_text_len": 207,
      "plain_text_excerpt": "Page audio \nSection:PageAudio\n\nA small but useful addition is that the final multimodal rendering stage now adds an audio file for each whole page, creating it by concatenating the pages segment audio files.",
      "analysis": {
        "section_summary": "The multimodal rendering pipeline now produces a complete audio file per page by stitching together that page's segment audio clips, adding page-level audio output.",
        "relevant_views": [
          {
            "url_name": "serve_audio_file",
            "confidence": 0.69,
            "rationale": "This endpoint serves generated audio assets, so the newly created per-page audio files would be accessed through it."
          },
          {
            "url_name": "render_text_complete",
            "confidence": 0.4,
            "rationale": "The rendering completion view relates to the final multimodal rendering stage where page-level audio is now added."
          }
        ],
        "concept_tags": [
          "page audio",
          "audio concatenation",
          "multimodal rendering",
          "segment audio"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14183,
        "completion_tokens": 950,
        "total_tokens": 15133,
        "estimated_cost_usd": 0.02722875
      },
      "has_tex": true
    },
    {
      "section_id": "a8651ce07183c862",
      "level": 2,
      "title": "'New functionality > Creating coherent image sets'",
      "label": "Section:CoherentImages",
      "plain_text_len": 52,
      "plain_text_excerpt": "Creating coherent image sets \nSection:CoherentImages",
      "analysis": {
        "section_summary": "Introduces new support for generating and managing coherent sets of images so that pages in a CLARA project share a consistent style and elements.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.62,
            "rationale": "The edit_images_v2 view is used to configure and manage coherent images for a CLARA project, matching the creation of coherent image sets."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.54,
            "rationale": "Provides monitoring for the coherent images v2 generation tasks, relevant when creating coherent image sets."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.53,
            "rationale": "Supplies status information for coherent images generation, fitting the coherent image sets workflow."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.4,
            "rationale": "Serves an overview of coherent images v2 outputs, likely used to review generated coherent image sets."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.38,
            "rationale": "Serves files for coherent images v2, which may be needed when handling coherent image sets."
          }
        ],
        "concept_tags": [
          "coherent images",
          "image generation",
          "style consistency",
          "CLARA projects"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14150,
        "completion_tokens": 682,
        "total_tokens": 14832,
        "estimated_cost_usd": 0.0245075
      },
      "has_tex": true
    },
    {
      "section_id": "32f729ff7832284b",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Overview'",
      "label": "Section:CoherentImagesOverview",
      "plain_text_len": 2804,
      "plain_text_excerpt": "Overview \nSection:CoherentImagesOverview\n\nA major new piece of functionality is support for creation of text containing multiple images. Here, the challenge is to produce images that are coherent with each other.\n\nCoherence can involve both style and content. Coherence of style means choosing a single artistic theme, a defined colour palette, and so on. Coherence of content means that elements which occur in more than one image (people, animals, objects, locations...) will be realised in roughly the same way. Our current approach in both cases is the same. We instruct the AI to create descriptions for the aspects, style or repeated elements, that will be shared between images; we then include this information in the prompts that generate the final descriptions that are passed to DALL-E-3 to create the images themselves. We thus have a cascade of prompts, where each prompt creates materia…",
      "analysis": {
        "section_summary": "Introduces C‑LARA’s new coherent image generation: when creating texts with multiple illustrations, the system enforces coherence in both style (shared artistic theme, palette) and content (consistent depiction of recurring characters/objects). It uses a cascade of AI prompts to first produce shared style and element descriptions, then feeds these into final DALL‑E‑3 prompts. Multiple alternative descriptions and images are generated and can be reviewed by humans or AI. Style coherence works well; content coherence is harder with current API models, though newer ChatGPT image models may improve this. An example shows a user style request and the AI’s expanded style description.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.7,
            "rationale": "Core view for configuring and generating coherent images (style, elements, pages) via cascaded prompts aligns with the described coherent image workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.55,
            "rationale": "Provides monitoring for coherent image generation tasks, relevant to overseeing cascaded prompt runs and reviewing alternatives."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.55,
            "rationale": "Exposes status of coherent image generation jobs, fitting the multi-step generation described."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.42,
            "rationale": "Simple C-LARA wizard monitor for style-related image generation matches the style coherence and prompt cascade for styles."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.42,
            "rationale": "Status polling for style generation in the wizard supports the style coherence flow."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.48,
            "rationale": "Review interface for style-related coherent images aligns with reviewing multiple style alternatives."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.4,
            "rationale": "Enables reviewing element-level coherent images, relevant to content coherence for repeated elements."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.35,
            "rationale": "Serves overview pages for coherent images, potentially displaying the multi-image set and examples."
          }
        ],
        "concept_tags": [
          "coherent images",
          "style coherence",
          "content coherence",
          "cascaded prompts",
          "DALL-E-3",
          "style description",
          "repeated elements",
          "alternative generations",
          "AI review",
          "image sets"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15009,
        "completion_tokens": 881,
        "total_tokens": 15890,
        "estimated_cost_usd": 0.02757125
      },
      "has_tex": true
    },
    {
      "section_id": "dd70ae8eb92bdb6e",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Style'",
      "label": "Section:CoherentImagesStyle",
      "plain_text_len": 570,
      "plain_text_excerpt": "Style \nSection:CoherentImagesStyle\n\nThe starting point in the process of creating a coherent image set is to generate a style description. The prompt template used to do this is shown in\nFigure~[REF:Figure:StyleGenerationTemplate]. We pass in the user's instructions, the text, and optionally some background information.\n\nWe find in practice that it is usually most effective to provide short, vague instructions for generating the style and let the AI make the substantive decisions.\nFigure~[REF:Figure:StyleExample] shows another example from ``The Cat and the Bat''.",
      "analysis": {
        "section_summary": "This section explains that creating a coherent image set begins by generating a style description using a prompt template that incorporates user instructions, the source text, and optional background information. It notes that short, vague instructions often yield the best results, allowing the AI to decide the stylistic details, and points to an example style from \"The Cat and the Bat\".",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.68,
            "rationale": "This view lets users configure and manage coherent images for a project, including initiating style generation as the first step using the prompt template described."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.61,
            "rationale": "Provides a monitoring interface for the asynchronous generation tasks involved in coherent images, including the style description creation mentioned in the section."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.59,
            "rationale": "Supplies status updates for coherent image generation jobs, which would include tracking the progress of the style description generation outlined here."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.41,
            "rationale": "In the Simple C-LARA workflow, this monitors style-related requests for coherent images, aligning with the style generation step described."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.4,
            "rationale": "Reports status for Simple C-LARA style generation requests, relevant to the process of creating a style description for coherent images."
          }
        ],
        "concept_tags": [
          "coherent images",
          "style description",
          "prompt template",
          "AI-generated style",
          "user instructions",
          "image generation",
          "background information"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14565,
        "completion_tokens": 1447,
        "total_tokens": 16012,
        "estimated_cost_usd": 0.03267625
      },
      "has_tex": true
    },
    {
      "section_id": "7139961c4fc24f1f",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Elements'",
      "label": "Section:CoherentImagesElements",
      "plain_text_len": 2244,
      "plain_text_excerpt": "Elements \nSection:CoherentImagesElements\n\nOnce the style has been determined, the next step is to create descriptions for possible recurring elements in the text. These will typically be people, animals, objects or locations.\n\nThe ``Elements'' step consists of two substeps. In the first, the AI is instructed to create a list of elements. This is done using the prompt template shown in Figure~[REF:Figure:FindElementNames], which takes as input the text, the already generated style description, and possible background information. So far, this operation seems rather unreliable, so we provide controls allowing the user to add or delete elements.\n\nIn the second substep, the AI is instructed to create a description for each element, and then produce an illustrative image to give the user feedback. The prompt template used to create an element description is shown in Figure~[REF:Figure:CreateE…",
      "analysis": {
        "section_summary": "The Elements step in creating coherent image sets follows style generation by identifying recurring entities such as characters, animals, objects or locations. It has two parts: first, the AI proposes a list of elements from the text, style description and background, but since this is unreliable users can add or delete items. Second, for each element the AI produces a detailed description and an illustrative image to provide feedback, as shown in the example of the cat \"Whiskers\" for \"The Cat and the Bat\".",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.37,
            "rationale": "This view configures and manages coherent images for a project, including triggering AI generation of styles, elements, and pages as described in the Elements step."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.3,
            "rationale": "Provides monitoring for coherent images generation tasks such as element lists and descriptions, aligning with the AI-driven elements creation process."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.3,
            "rationale": "Supplies status information for asynchronous generation of elements and their illustrative images within coherent image workflows."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.26,
            "rationale": "Monitors element-generation requests in the Simple C-LARA wizard, relevant to creating and reviewing AI-generated element descriptions and images."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.26,
            "rationale": "Provides status polling for element-generation tasks initiated through the Simple C-LARA interface, matching the Elements step functionality."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.24,
            "rationale": "Offers review interfaces for coherent images for elements, which is pertinent to checking AI-produced element descriptions and their feedback images."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "element identification",
          "AI prompt template",
          "element description",
          "illustrative image feedback",
          "user control over elements",
          "visual style consistency"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15192,
        "completion_tokens": 1023,
        "total_tokens": 16215,
        "estimated_cost_usd": 0.02922
      },
      "has_tex": true
    },
    {
      "section_id": "e3b5150233e9f7e8",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Pages'",
      "label": "Section:CoherentImagesPages",
      "plain_text_len": 960,
      "plain_text_excerpt": "Pages \nSection:CoherentImagesPages\n\nWith the style description and the element descriptions in place, the final step is to generate the actual page images. This again breaks down into two substeps. In the first, the AI is told to identify the relevant elements from\nthe full list; in the second, the template is passed the text, background, \nstyle description and selected elements, and instructed to create\nthe description for the page image. \n\nThe template for the first operation is shown in Figure~[REF:Figure:FindRelevantElements], and is straightforward.\nThe second substep is the more interesting one. We originally implemented it with the template from Figure~[REF:Figure:CreatePageImageDescription], where we pass in the element descriptions, that is to say pieces of text, and instruct the AI to create page descriptions from them which in some way incorporate these pieces of text. Figure~…",
      "analysis": {
        "section_summary": "This section describes the final step in creating coherent image sets: generating page images. The process consists of first using an AI prompt to pick relevant elements from the full list, and then supplying the page text, background, style description, and those selected elements to a template that instructs the AI to produce a detailed page image description. The original implementation used templates that fed element description snippets into the AI to incorporate them into the page descriptions.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.8,
            "rationale": "The edit_images_v2 view supports configuring coherent images for a project, including generating page images based on style and elements, matching the described workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.78,
            "rationale": "This monitor endpoint manages the generation tasks for styles, elements, and pages in coherent image sets, aligning with the page image generation steps outlined."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.76,
            "rationale": "The status endpoint reports on the progress of coherent image generation tasks including page images, relevant to the described two-step AI templating process."
          }
        ],
        "concept_tags": [
          "coherent images",
          "page image generation",
          "element selection",
          "AI templates",
          "style description",
          "element descriptions"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14848,
        "completion_tokens": 584,
        "total_tokens": 15432,
        "estimated_cost_usd": 0.0244
      },
      "has_tex": true
    },
    {
      "section_id": "49ee4e0ef5c6682a",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Using a multimodal model: Gem'",
      "label": "Section:CoherentImagesPagesImagen",
      "plain_text_len": 3871,
      "plain_text_excerpt": "Using a multimodal model: Gemini Imagen 3 \nSection:CoherentImagesPagesImagen\n\nHaving created several dozen C-LARA texts using DALL-E-3, our overall conclusion is that it is in general not capable of maintaining adequately coherent images. The same element is often not presented similarly enough across multiple images, even if the element description generated by the AI is quite detailed and DALL-E-3 appears to have followed it. Words do not appear to provide a sufficiently fine-grained way to capture an image.\n\nWe also experimenting with using the Gemini Image 3 model as an alternative to DALL-E-3; C-LARA now supports both image generators. The important difference is that Imagen 3 is multimodal; instructions can include references to web-accessible images as an alternative to descriptions of images, with the images passed as URLs. These are interpreted directly. \n\nA practical problem wi…",
      "analysis": {
        "section_summary": "The section evaluates image generation for coherent image sets in C-LARA. It finds DALL-E-3 weak at keeping elements consistent across pages and reports adding support for Google's multimodal Gemini Imagen 3, which can take image URLs in prompts. Due to Imagen 3’s restrictions on depicting people (especially children), C-LARA uses a hybrid strategy: textual descriptions for people and image references otherwise. It notes rapid progress with OpenAI’s forthcoming Images in ChatGPT (March 2025) likely to solve coherence, and illustrates the workflow with a sample page text and AI-generated stained glass style image description.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.7,
            "rationale": "This view configures and manages coherent images for a project, including generating page images with alternative models like DALL-E-3 or Gemini Imagen 3."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.64,
            "rationale": "Status endpoint for coherent image generation tasks, relevant when monitoring Imagen 3 or DALL-E-3 page image creation."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.61,
            "rationale": "Monitor view for asynchronous coherent image generation, applicable to workflows using Gemini Imagen 3."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "Gemini Imagen 3",
          "DALL-E-3",
          "multimodal prompts",
          "image URLs",
          "hybrid strategy for people",
          "C-LARA images",
          "OpenAI Images in ChatGPT",
          "page image description"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15432,
        "completion_tokens": 640,
        "total_tokens": 16072,
        "estimated_cost_usd": 0.02569
      },
      "has_tex": true
    },
    {
      "section_id": "c34d453aea4a6a15",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Uploading Images'",
      "label": "Section:CoherentImagesUploadingImages",
      "plain_text_len": 1776,
      "plain_text_excerpt": "Uploading Images \nSection:CoherentImagesUploadingImages\n\nSometimes the user has a specific idea about how an element or image should appear, but cannot easily express it verbally. We consequently include a mechanism that allows the user to upload their own image and use it as a model. The AI analyses the image to produce a description, then uses the description in the same way as other ones.\n\nThe template used to analyse an uploaded element image is shown in Figure~[REF:Figure:AnalyseUploadedImage]. Figure~[REF:Figure:UploadedImageExample] presents an example of using an uploaded image.\n\nfigure\n\nUploaded image\n\n10pt\n\nGenesis1/Gulapilil.jpg\n\nAI-generated element image description\nverbatim\nI'm unable to identify or describe real people in images. However, \nfor creating an artistic interpretation in DALL-E, you can describe \nthe character as:\n\n- **Apparent Age**: Elderly\n- **Gender**: Male\n…",
      "analysis": {
        "section_summary": "This subsection explains that users can upload their own element images when they have a specific visual in mind. The system analyzes the uploaded image to generate a descriptive prompt, which is then used like any other element description for coherent image generation. An example shows an uploaded stock photo and the AI’s cautious description used to inspire a page image.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.73,
            "rationale": "The coherent images v2 editor handles configuring and managing coherent images for a project, including uploading and working with element and page images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.36,
            "rationale": "Provides monitoring/status for coherent image generation tasks started from the edit_images_v2 interface, which may include processing uploaded element images."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.34,
            "rationale": "Status endpoint for coherent image generation workflows related to the coherent images v2 editor where uploaded images are analyzed and used."
          }
        ],
        "concept_tags": [
          "coherent images",
          "element image upload",
          "AI image analysis",
          "prompt generation",
          "DALL-E",
          "user-provided models",
          "image descriptions"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14762,
        "completion_tokens": 501,
        "total_tokens": 15263,
        "estimated_cost_usd": 0.0234625
      },
      "has_tex": true
    },
    {
      "section_id": "abc96b7ca277b749",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Reviewing Images'",
      "label": "Section:CoherentImagesReviewing",
      "plain_text_len": 3489,
      "plain_text_excerpt": "Reviewing Images \nSection:CoherentImagesReviewing\n\nThe image generation process is still very far from reliable; in particular, DALL-E-3 hardly ever follows instructions exactly and often produces images which are incompatible with them. As noted below, better models are in the process of being released, but until they are the most realistic strategy is to generate multiple images and then either select the best one, or to edit the descriptions and regenerate in an attempt to find a way of expressing the instructions which resonated better with DALL-E-3.\n\nWe support three different methods. In the first, we attempt to select the best descriptions and images using automatic AI-based filtering. In the second, we let the user select and regenerate directly; and in the third, we allow a community to select and regenerate using a democratic process. We present the three methods in turn, then …",
      "analysis": {
        "section_summary": "The section explains how to review and refine AI-generated images for coherent image sets. Because models like DALL-E-3 are unreliable, multiple descriptions and images are generated and filtered either automatically, by comparing AI vision interpretations to prompts (with limited success so far), or manually. Users can review styles, elements, and pages via the \"Edit Images and Pages\" interface: upvote/downvote existing images, edit and regenerate descriptions, or upload their own images. A community workflow also exists, with regular members submitting requests that are held for approval, and coordinators approving or denying and triggering costly AI generation.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.86,
            "rationale": "This is the main \"Edit Images and Pages\" interface used for user reviewing of styles, elements, and pages, including selecting, regenerating, and uploading images."
          },
          {
            "url_name": "community_review_images",
            "confidence": 0.84,
            "rationale": "Supports community member reviewing of images with voting and request submission, matching the described community workflow for regular members."
          },
          {
            "url_name": "community_organiser_review_images",
            "confidence": 0.84,
            "rationale": "Provides the coordinator view to approve or deny community image generation requests and execute approved ones, as described for coordinators."
          },
          {
            "url_name": "community_review_images_for_page",
            "confidence": 0.78,
            "rationale": "Allows drilling into a specific page for community-based image selection and regeneration, aligning with the page-level review process."
          },
          {
            "url_name": "execute_community_requests_for_page_monitor",
            "confidence": 0.65,
            "rationale": "Monitors execution of approved community image generation requests, relevant to coordinators running community-held AI actions."
          },
          {
            "url_name": "execute_community_requests_for_page_status",
            "confidence": 0.65,
            "rationale": "Reports status of executing approved community requests, part of the coordinator’s role in the community reviewing process."
          }
        ],
        "concept_tags": [
          "coherent images",
          "AI reviewing",
          "image generation",
          "DALL-E-3",
          "vision model",
          "GPT-4o",
          "automatic filtering",
          "user reviewing",
          "Edit Images and Pages",
          "upvote/downvote",
          "regenerate images",
          "image descriptions",
          "upload custom image",
          "community reviewing",
          "coordinator approval",
          "collaborative editing",
          "Indigenous languages"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15371,
        "completion_tokens": 1412,
        "total_tokens": 16783,
        "estimated_cost_usd": 0.03333375
      },
      "has_tex": true
    },
    {
      "section_id": "fe3d2f847abdb749",
      "level": 3,
      "title": "'New functionality > Creating coherent image sets > Summary and further direction'",
      "label": "Section:CoherentImagesSummary",
      "plain_text_len": 1342,
      "plain_text_excerpt": "Summary and further directions \nSection:CoherentImagesSummary\n\nThe previous sections have presented the new image generation functionality. Users construct images by first defining a style, then the visual elements common to several images, and finally the images themselves. In each case, the process is first to instruct the AI to create a suitable text description, and then pass this description to DALL-E-3 to produce images. In practice, quality can often be substantially improved by manually filtering the images and/or editing the AI-generated descriptions. An interesting point is that the descriptions do not need to be created in English. \n\nWe had three main goals when developing the functionality. The set of images for a text should be\n\nenumerate\n quick to create.\n coherent in terms of style (images should look stylistically similar);\n coherent in terms of content (if the same item …",
      "analysis": {
        "section_summary": "This section recaps the new coherent image workflow, where users define a style, specify common visual elements, and then generate page images by first having the AI write descriptions and passing them to DALL-E-3. Manual curation of descriptions and filtering images can improve quality, and descriptions need not be in English. The design aimed at quick creation, stylistic coherence across images, and consistent depiction of recurring items; the current approach achieves the first two but still struggles with content coherence, likely to be resolved as more powerful models become accessible via APIs.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.69,
            "rationale": "Provides the interface to configure style, elements, and generate coherent images as described in the new workflow."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.58,
            "rationale": "Supports monitoring asynchronous generation tasks for styles, elements, and page images in the coherent images v2 process."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.56,
            "rationale": "Exposes status information for the coherent image generation steps aligning with the style/elements/pages workflow outlined."
          }
        ],
        "concept_tags": [
          "coherent images",
          "style definition",
          "visual elements",
          "AI-generated descriptions",
          "DALL-E-3",
          "manual curation",
          "style coherence",
          "content coherence",
          "image generation",
          "future AI models"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14641,
        "completion_tokens": 737,
        "total_tokens": 15378,
        "estimated_cost_usd": 0.02567125
      },
      "has_tex": true
    },
    {
      "section_id": "9a5713711138a2d0",
      "level": 2,
      "title": "'New functionality > Simple C-LARA'",
      "label": "Section:SimpleCLARA",
      "plain_text_len": 709,
      "plain_text_excerpt": "Simple C-LARA \nSection:SimpleCLARA\n\nSimple C-LARA provides a wizard-style interface which streamlines the process of creating a C-LARA text. The user does not have all the options available in Advanced C-LARA, but the process is much easier to understand.\n\nThe initial version of Simple C-LARA, described in the second progress report, has now been extended to include more functionality. In particular, annotation for MWEs and sentence translations are added automatically, and the user has the option of requesting images created using the coherent image set functionality from .\n\nAppendix~[REF:Section:UsingSimpleCLARA] presents an example of using Simple C-LARA to create a text with a coherent image set.",
      "analysis": {
        "section_summary": "Simple C-LARA is a wizard-style interface to create C-LARA texts with fewer options than Advanced C-LARA but a much simpler workflow. The latest version extends the original wizard to automatically add multiword expression annotations and sentence translations, and lets users request coherent image sets as part of the process.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.73,
            "rationale": "This is the main Simple C-LARA wizard view that streamlines text creation with limited options, matching the described interface."
          },
          {
            "url_name": "simple_clara_status",
            "confidence": 0.65,
            "rationale": "Status polling for Simple C-LARA tasks supports the wizard flow mentioned in the section."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.65,
            "rationale": "Monitoring the Simple C-LARA process aligns with the extended wizard functionality."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.6,
            "rationale": "Handles dispatched Simple C-LARA element requests in the wizard, consistent with the streamlined creation process."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.6,
            "rationale": "Status endpoint for Simple C-LARA element requests, part of the wizard pipeline."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.64,
            "rationale": "Monitors image generation requests initiated from Simple C-LARA, relevant to requesting coherent image sets."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.64,
            "rationale": "Status polling for Simple C-LARA image generation requests, supporting the coherent image option."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.58,
            "rationale": "Monitors style-related image requests in Simple C-LARA, tied to coherent image set creation."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.58,
            "rationale": "Reports status for style requests in Simple C-LARA, part of coherent image workflows."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.56,
            "rationale": "Provides review interfaces for coherent images at element level within Simple C-LARA, matching the added image functionality."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.56,
            "rationale": "Supports reviewing coherent images by page from the Simple C-LARA wizard."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.56,
            "rationale": "Allows reviewing coherent images by style within Simple C-LARA, aligned with the new image option."
          }
        ],
        "concept_tags": [
          "Simple C-LARA",
          "wizard interface",
          "simplified workflow",
          "automatic annotation",
          "multiword expressions",
          "sentence translation",
          "coherent image set",
          "image generation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14889,
        "completion_tokens": 1001,
        "total_tokens": 15890,
        "estimated_cost_usd": 0.02862125
      },
      "has_tex": true
    },
    {
      "section_id": "26ef66d8cd614a8b",
      "level": 2,
      "title": "'New functionality > Parallelism'",
      "label": "Section:Parallelism",
      "plain_text_len": 1207,
      "plain_text_excerpt": "Parallelism \nSection:Parallelism\n\nWe have implemented parallelism using the Python asyncio package, which has a large impact on speed of processing. Parallelism is currently used in two places:\n\ndescription\n\n Most of the annotation operations (the second phase of segmentation, Multi-Word Expressions, lemma tagging, glossing) are now parallelised. In all of these cases, the text has already been divided into segments, and all the segments are annotated simultaneously.\n\n Similarly, we have parallelised production of descriptions and images for the ``elements'' and ``pages'' phases of coherent image processing. As with linguistic annotation, we process all the elements, or all the pages, at the same time.\n\ndescription\n\nIn all of the above, parallel processing is realised using some version of the standard asyncio pattern shown in Figure~[REF:Figure:ParallelismPseudo].\n\nfigure\nverbatim\nasync…",
      "analysis": {
        "section_summary": "The report describes how CLARA now uses Python’s asyncio to parallelise work, greatly speeding up processing. Annotation steps such as the second phase of segmentation, multi‑word expression detection, lemma tagging and glossing now run over all segments concurrently. Coherent image generation has similarly been parallelised so that descriptions and images for all elements or all pages are produced simultaneously using an asyncio task/gather pattern.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.48,
            "rationale": "Handles creation of multi‑word expression annotations, one of the steps now parallelised in the new asyncio-based pipeline."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.46,
            "rationale": "Produces lemma tagging annotations, cited as being parallelised across segments."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.45,
            "rationale": "Glossing is one of the annotation operations now processed concurrently over all segments."
          },
          {
            "url_name": "create_lemma_and_gloss_tagged_text",
            "confidence": 0.41,
            "rationale": "Combines lemma tagging and glossing, both mentioned as parallelised annotation phases."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.44,
            "rationale": "Manages coherent images generation; the section notes parallelising description and image production for elements and pages."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.4,
            "rationale": "Provides status for coherent image generation tasks, which now run in parallel for elements and pages."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.4,
            "rationale": "Monitors coherent image processing, an area described as parallelised using asyncio."
          }
        ],
        "concept_tags": [
          "asyncio",
          "parallel processing",
          "annotation pipeline",
          "segmentation",
          "multi-word expressions",
          "lemma tagging",
          "glossing",
          "coherent images",
          "element/page processing",
          "performance improvement"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14968,
        "completion_tokens": 1213,
        "total_tokens": 16181,
        "estimated_cost_usd": 0.03084
      },
      "has_tex": true
    },
    {
      "section_id": "47cea9a83df79ba4",
      "level": 2,
      "title": "'New functionality > Better support for Indigenous languages'",
      "label": "Section:IndigenousLanguages",
      "plain_text_len": 2694,
      "plain_text_excerpt": "Better support for Indigenous languages \nSection:IndigenousLanguages\n\nInitial experiments using C-LARA to construct texts for Indigenous languages, described in the preceding report, showed that it worked much less well there than with the large languages it was originally designed for. There were two main problems. First, since the AI does not know the languages concerned, it is unable to write or annotate the texts, and this work must thus be performed manually. The human annotator is working with multiple parallel versions of the text, annotated in different ways (glossed, lemma tagged, translated, etc); in practice, the different versions often get out of sync, and it is laborious to then correct the divergences.\n\nAs previously noted, a C-LARA project maintains multiple parallel versions of the text: plain text, segmented text (i.e.\\ text divided into pages and sentence-like units), …",
      "analysis": {
        "section_summary": "The section discusses improving C-LARA’s handling of Indigenous languages, where AI cannot produce annotations and humans must maintain multiple parallel text versions. To reduce errors and divergence between glossed, translated, lemma-tagged and other versions, the \"Edit Pages and Images\" screen now presents all annotations per page together and automatically checks syntax and alignment on save, using smart diffs to minimally correct mismatches.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.37,
            "rationale": "The reorganised \"Edit Pages and Images\" screen is mentioned; edit_images_v2 handles configuring and managing that screen for pages and images in a project."
          },
          {
            "url_name": "edit_images",
            "confidence": 0.3,
            "rationale": "The legacy Edit Images and Pages view corresponds to the screen being redesigned to present page-level material together for manual annotation."
          }
        ],
        "concept_tags": [
          "Indigenous languages",
          "manual annotation",
          "parallel text versions",
          "glossed text",
          "lemma tagging",
          "segmented text",
          "page-based editing",
          "annotation syntax checking",
          "divergence detection",
          "smart diff correction"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14928,
        "completion_tokens": 1050,
        "total_tokens": 15978,
        "estimated_cost_usd": 0.02916
      },
      "has_tex": true
    },
    {
      "section_id": "951e5cb7958c0c44",
      "level": 1,
      "title": "'Evaluating linguistic annotation and image generation'",
      "label": "Section:SoftwareComponent",
      "plain_text_len": 709,
      "plain_text_excerpt": "Evaluating linguistic annotation and image generation\nSection:SoftwareComponent\n\nIn the second progress report, we described how GPT-4 (and subsequently GPT-4 Turbo) was evaluated on four core annotation tasks---writing, segmenting, glossing, and lemma/POS tagging---across multiple languages and text genres. Here, we present a third round of experiments, focusing initially on English, where we again see a notable reduction in error rates(Table~[REF:Table:AnnotationPerformanceEnglish]). Beyond annotation, we have also started to systematically assess the new coherent image generation functionality using a similar multi-text design and a structured questionnaire. Below, we present our initial findings.",
      "analysis": {
        "section_summary": "This section reports a third round of evaluations of GPT-4/GPT-4 Turbo on core linguistic annotation tasks, with English showing notably reduced error rates, and outlines initial systematic assessment of the new coherent image generation feature using a multi-text design and structured questionnaires.",
        "relevant_views": [
          {
            "url_name": "image_questionnaire_project_list",
            "confidence": 0.52,
            "rationale": "Provides access to projects with image questionnaires, aligning with the described structured evaluation of coherent image generation."
          },
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.5,
            "rationale": "Initiates page-by-page rating forms for image questionnaires, supporting the systematic assessment of generated images."
          },
          {
            "url_name": "image_questionnaire_summary",
            "confidence": 0.49,
            "rationale": "Offers summaries of image questionnaire results, relevant to reporting findings from the structured evaluation of image generation."
          }
        ],
        "concept_tags": [
          "GPT-4",
          "GPT-4 Turbo",
          "linguistic annotation",
          "writing annotation",
          "text segmentation",
          "glossing",
          "lemma tagging",
          "POS tagging",
          "error rates",
          "coherent image generation",
          "structured questionnaire",
          "evaluation",
          "multi-text design",
          "English language"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14514,
        "completion_tokens": 1269,
        "total_tokens": 15783,
        "estimated_cost_usd": 0.0308325
      },
      "has_tex": true
    },
    {
      "section_id": "2370879539269119",
      "level": 2,
      "title": "'Evaluating linguistic annotation and image generation > Third round of linguisti'",
      "label": "Section:ThirdRoundAnnotation",
      "plain_text_len": 2971,
      "plain_text_excerpt": "Third round of linguistic annotation evaluation\nSection:ThirdRoundAnnotation\n\nTo gauge ongoing progress in GPT-4's linguistic performance, we replicated the experiments from 5.1 of the second progress report a third time in March 2025, using the March 2025 GPT-4o version and the current version of C-LARA, incorporating in particular MWE annotation and segment translations. Table~[REF:Table:AnnotationPerformanceEnglish] shows the updated results for English, measured as before on six texts defined by the prompts in Table~[REF:Table:Prompts] and the same tasks (segmentation, French glossing, lemma/POS tagging). Compared to both the September 2023 and March 2024 data, the error rates have decreased further, with an average of only 2.0\\% for glossing and 0.8\\% for lemma-tagging across the six texts. This is a very substantial improvement relative to the original GPT-4, which had glossing err…",
      "analysis": {
        "section_summary": "A third replication of the linguistic annotation evaluation was run in March 2025 using the GPT-4o model and the current C-LARA (with MWE annotation and segment translations) on six English texts prompted as in prior studies. Tasks included segmentation, French glossing, and lemma/POS tagging, and error rates have dropped markedly since September 2023 and March 2024, averaging about 2.0% for glossing and 0.8% for lemma tagging across the six texts.",
        "relevant_views": [],
        "concept_tags": [
          "GPT-4o",
          "linguistic annotation evaluation",
          "segmentation",
          "French glossing",
          "lemma tagging",
          "error rates",
          "English prompts",
          "C-LARA",
          "multiword expressions",
          "segment translations",
          "March 2025"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15791,
        "completion_tokens": 900,
        "total_tokens": 16691,
        "estimated_cost_usd": 0.02873875
      },
      "has_tex": true
    },
    {
      "section_id": "5175e3af3b4c79b0",
      "level": 2,
      "title": "'Evaluating linguistic annotation and image generation > Evaluating coherent imag'",
      "label": "Section:ImageGenerationEvaluation",
      "plain_text_len": 8094,
      "plain_text_excerpt": "Evaluating coherent image generation\nSection:ImageGenerationEvaluation\n\nAnother key focus of this third phase is a systematic evaluation of the coherent image generation framework described in . The goal is to produce sets of illustrations for short texts in a way that observe constraints on stylistic coherence, narrative coherence, and cultural relevance. We have again defined six text scenarios of varied types, this time for image creation, as shown in Table~[REF:Table:ImagePrompts].\n\ntable*[bh!]\n Core prompts used to create image evaluation texts. More information was added to specify the length of the text in pages (typically 10--20 pages) and the fact that one illustration would be created per page.\n Table:ImagePrompts\n\ntabularll\n\n1cLabel & 1cPrompt\\\\\n\nDI & A small picture dictionary.\\\\\n\nSC & A scientific/technical explanation accessible to a bright ten-year-old.\\\\\n\nAN & A friendshi…",
      "analysis": {
        "section_summary": "This section reports on a systematic evaluation of C‑LARA’s coherent image generation pipeline. Six English text scenarios (dictionary, science explanation, animal friendship, robot story, traditional tale, typical day) were illustrated using a staged process of style definition, element descriptions and page images, with three variants per page generated by DALL‑E‑3 or Imagen 3. Weaker images were revised by selecting alternate variants or regenerating. A newly integrated questionnaire tool gathered 5‑point Likert ratings on correspondence to text, style consistency, element coherence, cultural appropriateness and visual appeal. Results show strong performance for four scenarios, with high correspondence, style and coherence scores after revision, but style and coherence issues in the traditional story and systematic failures on technical diagrams in the science text, reflecting current model limitations. Figures illustrate successful coherence and problem cases.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.76,
            "rationale": "The evaluation refers to generating coherent image sets via a process of style specification, element definitions and page images, which is handled by the coherent images v2 editing view."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.7,
            "rationale": "Status monitoring of the coherent image generation tasks used in the evaluated pipeline would use the v2 status endpoint."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.68,
            "rationale": "Monitoring ongoing coherent image generation jobs aligns with the evaluation of generated image sets."
          },
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.86,
            "rationale": "The section describes using a questionnaire tool to rate each page image on Likert scales; this view initiates such image questionnaires."
          },
          {
            "url_name": "image_questionnaire_item",
            "confidence": 0.85,
            "rationale": "Per-page image rating as described is supported by the questionnaire item view."
          },
          {
            "url_name": "image_questionnaire_summary",
            "confidence": 0.8,
            "rationale": "Summarising the collected ratings, as shown in the reported average scores, corresponds to this summary view."
          },
          {
            "url_name": "image_questionnaire_summary_csv",
            "confidence": 0.74,
            "rationale": "Exporting or aggregating questionnaire results mirrors the presentation of averaged Likert scores."
          },
          {
            "url_name": "image_questionnaire_project_list",
            "confidence": 0.62,
            "rationale": "Listing projects with image questionnaires relates to setting up and locating the evaluations performed."
          },
          {
            "url_name": "image_questionnaire_all_projects_summary",
            "confidence": 0.6,
            "rationale": "Aggregated summaries across evaluated image sets are akin to the reported results table."
          },
          {
            "url_name": "image_and_text_questionnaire_start",
            "confidence": 0.55,
            "rationale": "If both images and accompanying text are involved in evaluation, this combined questionnaire start view could be used."
          },
          {
            "url_name": "image_only_questionnaire_start",
            "confidence": 0.57,
            "rationale": "The described evaluation focused on images alone, making this image-only questionnaire entry point relevant."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "image prompts",
          "DALL-E-3",
          "Imagen 3",
          "style consistency",
          "narrative coherence",
          "cultural appropriateness",
          "Likert-scale evaluation",
          "image questionnaire",
          "visual appeal",
          "technical diagram limitations",
          "illustration revision",
          "CLARA image pipeline"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16815,
        "completion_tokens": 1539,
        "total_tokens": 18354,
        "estimated_cost_usd": 0.03640875
      },
      "has_tex": true
    },
    {
      "section_id": "e58a57b4e316e0a2",
      "level": 3,
      "title": "'Evaluating linguistic annotation and image generation > Evaluating coherent imag'",
      "label": "Section:NewImageGenerationCapabilities",
      "plain_text_len": 1991,
      "plain_text_excerpt": "Looking ahead: new image generation capabilities\nSection:NewImageGenerationCapabilities\n\nIt appears, however, that the issues just mentioned will soon be resolved. The ``Images in ChatGPT''https://www.theverge.com/openai/635118/chatgpt-sora-ai-image-generation-chatgpt functionality, released in late Mar 2025 and so far only available through the web interface, incorporates reasoning into the image generation process and opens up dramatic new possibilities. Figure~[REF:Figure:ProblemsInScienceTechnical] shows the result when the ``simple circuit'' request is submitted; it is already nearly correct on the first try. \n\nfigure[h!]\n\n ImageEvaluation/RealisticCircuit.png\nGenerating the basic circuit image from Figure~[REF:Figure:ProblemsInScienceTechnical] using the ``Images in ChatGPT'' functionality released late March 2025. Although not perfect, this image, created without any tweaking of t…",
      "analysis": {
        "section_summary": "The section notes rapid advances in OpenAI’s image generation, highlighting the new ChatGPT web-based \"Images in ChatGPT\" feature (released March 2025) that already produces near-correct technical diagrams and more coherent story illustrations with minimal prompting. Although the API is not yet available, the pace suggests C-LARA will soon be able to generate good coherent image sets for similar texts.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.6,
            "rationale": "Supports configuring and generating coherent image sets for C-LARA projects, matching the section’s focus on improved coherent image generation capabilities."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.5,
            "rationale": "Provides status monitoring for coherent image generation tasks, relevant to anticipated future coherent image creation described in the section."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.5,
            "rationale": "Offers monitoring for coherent image generation workflows, aligning with the discussion of producing coherent image sets."
          }
        ],
        "concept_tags": [
          "Images in ChatGPT",
          "coherent image generation",
          "OpenAI",
          "image API",
          "technical diagrams",
          "story illustration",
          "C-LARA"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15143,
        "completion_tokens": 755,
        "total_tokens": 15898,
        "estimated_cost_usd": 0.02647875
      },
      "has_tex": true
    },
    {
      "section_id": "dba058815f37206f",
      "level": 3,
      "title": "'Evaluating linguistic annotation and image generation > Evaluating coherent imag'",
      "label": null,
      "plain_text_len": 2435,
      "plain_text_excerpt": "GPT-4o on the Future of Coherent Image Generation in C-LARA\n\nRecent advances in image generation mark a turning point for C-LARA’s ability to illustrate narrative and educational texts. Until now, one of the most significant limitations in using generative AI for visual support has been the difficulty of maintaining coherence across images — keeping characters, props, and style consistent from one page to the next. Current API-based systems like DALL-E 3 and Imagen 3 offer strong single-image generation, but they struggle to produce visually coherent multi-image sequences, particularly for narratives.\n\nIn this week’s experiments using the ChatGPT interface, however, it has become clear that this barrier is beginning to fall. The latest image generation model, Images in ChatGPT, can now retain visual identity across multiple images within a session, following detailed reference descriptio…",
      "analysis": {
        "section_summary": "The section reports that ChatGPT’s latest image model can now maintain visual coherence across multiple images in a session, successfully reusing character and scene references for narratives like The Brave Little Tailor. This confirms C-LARA’s design choice to structure and reuse shared visual elements, positioning it to exploit API-level coherence as soon as available. The breakthrough enables rapid creation of consistent, high-quality illustrated texts and technical materials that match professional standards.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.81,
            "rationale": "edit_images_v2 handles coherent image configuration and generation (style, elements, pages), aligning with the discussion of structured, reusable visual elements and multi-image coherence."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.78,
            "rationale": "Provides monitoring for coherent image generation tasks, relevant to managing and evaluating coherent multi-image sequences mentioned in the section."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.75,
            "rationale": "Supplies status endpoints for coherent image generation, useful when tracking multi-image coherence progress described in the section."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_page",
            "confidence": 0.48,
            "rationale": "Offers review interfaces for coherent images by page, which supports assessing consistency across narrative pages as discussed."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_element",
            "confidence": 0.46,
            "rationale": "Enables reviewing coherent images per element, aligning with reuse of character/prop references emphasized in the section."
          },
          {
            "url_name": "simple_clara_review_v2_images_for_style",
            "confidence": 0.42,
            "rationale": "Allows review of stylistic coherence, pertinent to maintaining consistent visual style across images."
          },
          {
            "url_name": "serve_coherent_images_v2_overview",
            "confidence": 0.38,
            "rationale": "Serves overviews of coherent image sets, tangentially relevant for presenting and checking multi-image coherence."
          },
          {
            "url_name": "serve_coherent_images_v2_file",
            "confidence": 0.35,
            "rationale": "Delivers coherent image assets; somewhat relevant to consuming and validating the coherent outputs described."
          }
        ],
        "concept_tags": [
          "coherent image generation",
          "multi-image consistency",
          "reference-based visuals",
          "character continuity",
          "scene continuity",
          "ChatGPT Images",
          "DALL-E 3",
          "narrative illustration",
          "pluggable architecture",
          "visual elements reuse",
          "style consistency",
          "educational texts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14925,
        "completion_tokens": 754,
        "total_tokens": 15679,
        "estimated_cost_usd": 0.02619625
      },
      "has_tex": true
    },
    {
      "section_id": "a8df799d0f0a4ac2",
      "level": 1,
      "title": "'Using C-LARA with specific languages'",
      "label": "Section:SpecificLanguages",
      "plain_text_len": 62,
      "plain_text_excerpt": "Using C-LARA with specific languages\nSection:SpecificLanguages",
      "analysis": {
        "section_summary": "Overview guidance on working with particular languages in C‑LARA, including language‑specific resources and settings such as phonetic lexicons and prompt templates managed by language masters.",
        "relevant_views": [
          {
            "url_name": "edit_phonetic_lexicon",
            "confidence": 0.64,
            "rationale": "Editing or importing phonetic lexicon data is a key language-specific task handled by language masters."
          },
          {
            "url_name": "edit_prompt",
            "confidence": 0.52,
            "rationale": "Annotation prompt templates vary by language and are maintained per language by language masters."
          },
          {
            "url_name": "bundle_list",
            "confidence": 0.32,
            "rationale": "Translation/localisation bundles relate to language-specific configuration for annotations and UI."
          }
        ],
        "concept_tags": [
          "language support",
          "language masters",
          "phonetic lexicon",
          "annotation prompts",
          "localisation"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15087,
        "completion_tokens": 428,
        "total_tokens": 15515,
        "estimated_cost_usd": 0.02313875
      },
      "has_tex": true
    },
    {
      "section_id": "ad35b4756c172ce8",
      "level": 2,
      "title": "'Using C-LARA with specific languages > Icelandic and Mandarin (University of Ice'",
      "label": "Section:IcelandicAndMandarin",
      "plain_text_len": 1064,
      "plain_text_excerpt": "Icelandic and Mandarin (University of Iceland)\nSection:IcelandicAndMandarin\n\nIn Summer 2024The version of C-LARA used for the experiments described in this section (Jun--Aug 2024) was significantly less advanced than the one used in most of the rest of the report (Feb--Mar 2025)., three university students worked with C-LARA to create illustrated short stories with translations to English and Mandarin Chinese that would be suitable for learning Icelandic as a second language at different learner levels. The aim was to evaluate the effectiveness of the C-LARA platform for language learning, focusing on its potential for less commonly spoken languages such as Icelandic. Over three months June-August, these students used C-LARA to create 30 projects, of which nine were fully completed and analyzed. The study documented accuracy rates, processing time, and costs at various stages of project …",
      "analysis": {
        "section_summary": "In summer 2024 three University of Iceland students used an earlier version of C‑LARA to produce illustrated Icelandic short stories with English and Mandarin translations for learners at different levels. Over three months they built 30 projects, fully completing nine that were analysed for accuracy, processing time and cost. Three of the projects were turned into picture books for future studies on learning effectiveness, demonstrating the platform’s potential for less commonly taught languages like Icelandic.",
        "relevant_views": [
          {
            "url_name": "create_project",
            "confidence": 0.37,
            "rationale": "Students created multiple C-LARA projects to develop the stories; this view handles creating new projects with language metadata."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.3,
            "rationale": "The Simple C-LARA wizard could have been used to assemble illustrated stories quickly, aligning with the described workflow."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.35,
            "rationale": "Producing illustrated short stories would require managing coherent images within projects, which this view supports."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.33,
            "rationale": "The projects included translations into English and Mandarin; this endpoint is used to create and edit translated text versions."
          }
        ],
        "concept_tags": [
          "Icelandic language learning",
          "Mandarin Chinese translation",
          "illustrated short stories",
          "bilingual content",
          "project creation",
          "language education",
          "processing time and cost",
          "learning materials",
          "less commonly taught languages",
          "picture books",
          "learner levels"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15070,
        "completion_tokens": 842,
        "total_tokens": 15912,
        "estimated_cost_usd": 0.0272575
      },
      "has_tex": true
    },
    {
      "section_id": "2e0894b591d448ee",
      "level": 3,
      "title": "'Using C-LARA with specific languages > Icelandic and Mandarin (University of Ice'",
      "label": null,
      "plain_text_len": 625,
      "plain_text_excerpt": "Key findings on language learning with C-LARA\n\n The study explored two primary use cases: learning Icelandic through English and Mandarin Chinese and learning Mandarin Chinese through Icelandic. It found that AI was highly efficient in generating learning resources, though it required much of human intervention to refine content for fluency and cultural accuracy especially in Icelandic. Interestingly, only minor adjustments needed to be done in Mandarin Chinese. This human-in-the-loop process, where AI-generated content was iteratively reviewed and edited, proved essential in achieving high-quality learning materials.",
      "analysis": {
        "section_summary": "The section reports on two scenarios using C‑LARA: learning Icelandic via English and Mandarin, and learning Mandarin via Icelandic. AI was effective at producing learning materials, but achieving fluent, culturally accurate Icelandic content needed substantial human review and editing, whereas Mandarin required only minor tweaks. An iterative human‑in‑the‑loop process was essential for high‑quality resources.",
        "relevant_views": [],
        "concept_tags": [
          "AI-assisted content creation",
          "human-in-the-loop",
          "language learning",
          "Icelandic",
          "Mandarin Chinese",
          "bilingual learning",
          "cultural accuracy",
          "resource refinement"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14927,
        "completion_tokens": 319,
        "total_tokens": 15246,
        "estimated_cost_usd": 0.02184875
      },
      "has_tex": true
    },
    {
      "section_id": "962922831958a869",
      "level": 3,
      "title": "'Using C-LARA with specific languages > Icelandic and Mandarin (University of Ice'",
      "label": null,
      "plain_text_len": 1280,
      "plain_text_excerpt": "Project outcomes and challenges\n\nEach student focused on different learner profiles and linguistic challenges:\n\nStudent 1 created 17 stories for young learners of Icelandic through English. While seven were experimental, the remaining 10 were thematically structured, covering topics like Norse mythology and AI-generated narratives. A significant challenge was aligning Common European Framework for Languages (CEFR) levels with children’s literature, as texts aimed at young readers tend to fall below B2, despite their complexity in terms of storytelling.\nStudent 2 developed nine projects, including adaptations of the same story for different proficiency levels and a text for teenage learners. Real-world themes, such as Reykjavík’s volcanic eruptions and the Arctic midnight sun, were explored. However, technical issues with multi-word-expression (MWE) processing for Icelandic language preve…",
      "analysis": {
        "section_summary": "Three students produced Icelandic and Mandarin learning materials with different focuses. One created themed Icelandic stories for young learners and struggled to align CEFR levels with children’s literature. Another built multiple Icelandic projects on real-world themes but encountered multi-word expression processing issues that blocked uploads. The third developed Icelandic–Mandarin resources on cultural festivals and faced faulty automatic text segmentation that required manual fixes.",
        "relevant_views": [
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.39,
            "rationale": "The section notes failed uploads caused by multi-word-expression processing issues for Icelandic, which is directly related to creating or handling MWE-tagged texts."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.34,
            "rationale": "Text segmentation errors that split Icelandic words incorrectly would be addressed in views handling segmented text creation and correction."
          }
        ],
        "concept_tags": [
          "Icelandic as a second language",
          "Mandarin Chinese",
          "CEFR alignment",
          "children’s literature",
          "multi-word expression processing",
          "text segmentation",
          "manual correction",
          "thematic storytelling",
          "cultural festivals",
          "learner profiles"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14866,
        "completion_tokens": 413,
        "total_tokens": 15279,
        "estimated_cost_usd": 0.0227125
      },
      "has_tex": true
    },
    {
      "section_id": "3df4831823bab09c",
      "level": 3,
      "title": "'Using C-LARA with specific languages > Icelandic and Mandarin (University of Ice'",
      "label": null,
      "plain_text_len": 945,
      "plain_text_excerpt": "AI in text and image generation\n\nThe project highlighted strengths and limitations in AI-generated content:\n\ndescription\n AI-created texts were sometimes inconsistent, requiring multiple prompt refinements. Known narratives (e.g., Norse mythology) tended to yield higher-quality texts than AI-generated original stories. The error rate in AI-generated texts varied between 20\\% and 90\\%, depending on the level of human editing needed.\n\n While AI-produced translations were mostly accurate, issues arose in Icelandic-Mandarin Chinese translations due to structural differences between the languages. Context-sensitive modifications were required to improve glossing and lemma-tagging accuracy.\n\n AI-generated images were often inconsistent, especially in character depiction. While animal characters maintained uniformity, human characters varied across images. Specifying stylistic consistency in pr…",
      "analysis": {
        "section_summary": "The section discusses strengths and shortcomings of AI-generated texts, translations, and images in the Icelandic–Mandarin context: AI-written narratives often require repeated prompt refinement and substantial human editing (with error rates from 20% to 90%), known storylines produce better quality than original stories, and Icelandic–Mandarin translations need context-sensitive fixes to improve glossing and lemma tagging. AI-produced images show character inconsistency—especially for humans—though specifying a consistent style in prompts helps.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.66,
            "rationale": "This view configures and manages coherent AI-generated images, matching the section’s focus on mitigating inconsistency and enforcing stylistic coherence in AI-created visuals."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.61,
            "rationale": "Provides monitoring for coherent image generation tasks, relevant to tracking and correcting inconsistency in AI-generated images highlighted in the section."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.61,
            "rationale": "Supplies status for coherent image generation workflows, aligned with the section’s concern about image uniformity and style control."
          },
          {
            "url_name": "generate_text_monitor",
            "confidence": 0.42,
            "rationale": "Monitors AI-driven text generation tasks; pertinent to the section’s observations about prompt refinement and error rates in AI-generated narratives."
          },
          {
            "url_name": "generate_text_status",
            "confidence": 0.4,
            "rationale": "Reports status of AI text generation processes, relevant to managing and iterating on AI-produced texts discussed in the section."
          }
        ],
        "concept_tags": [
          "AI text generation",
          "AI image generation",
          "prompt refinement",
          "translation quality",
          "Icelandic–Mandarin",
          "glossing",
          "lemma tagging",
          "stylistic consistency",
          "character consistency"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14543,
        "completion_tokens": 1228,
        "total_tokens": 15771,
        "estimated_cost_usd": 0.03045875
      },
      "has_tex": true
    },
    {
      "section_id": "f7a049ad5ec3f72d",
      "level": 3,
      "title": "'Using C-LARA with specific languages > Icelandic and Mandarin (University of Ice'",
      "label": null,
      "plain_text_len": 547,
      "plain_text_excerpt": "Implications for C-LARA’s use with less commonly spoken languages\n\nThe findings underscore C-LARA’s potential in learning less commonly spoken languages such as Icelandic, particularly in resource-scarce environments. The platform facilitates interactive, multimedia-based learning experiences but requires careful human supervision to ensure linguistic and cultural accuracy. The results suggest that C-LARA can support personalized content development for learners, particularly when integrated with manual editing and instructional scaffolding.",
      "analysis": {
        "section_summary": "The section highlights C-LARA’s promise for teaching less commonly spoken languages like Icelandic in low-resource settings, offering interactive multimedia learning while stressing the need for human oversight to maintain linguistic and cultural accuracy. It notes that C-LARA can enable personalized content if paired with manual editing and instructional scaffolding.",
        "relevant_views": [
          {
            "url_name": "manage_language_masters",
            "confidence": 0.4,
            "rationale": "Assigning language masters and localisation bundles can help ensure linguistic and cultural accuracy for less-resourced languages through supervised customization."
          },
          {
            "url_name": "edit_prompt",
            "confidence": 0.32,
            "rationale": "Customizing annotation prompts for specific languages supports supervised, accurate content generation in low-resource contexts."
          }
        ],
        "concept_tags": [
          "less commonly spoken languages",
          "Icelandic",
          "resource-scarce environments",
          "human supervision",
          "linguistic and cultural accuracy",
          "personalized content",
          "multimedia learning",
          "instructional scaffolding"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14524,
        "completion_tokens": 577,
        "total_tokens": 15101,
        "estimated_cost_usd": 0.023925
      },
      "has_tex": true
    },
    {
      "section_id": "f4bfd9f94ea5b74a",
      "level": 2,
      "title": "'Using C-LARA with specific languages > Kok Kaper (Kowanyama Community, Queenslan'",
      "label": "Section:KokKaper",
      "plain_text_len": 5473,
      "plain_text_excerpt": "Kok Kaper (Kowanyama Community, Queensland)\nSection:KokKaper\n\nSophie Rendina has been using C-LARA together with the Kok Kaper community at Kowanyama, Cape York, Queensland. Kok Kaper is a small, critically endangered Australian Aboriginal language, and the C-LARA work supports the creation of Kok Kaper language teaching and learning materials, which form an integral part of the Kowanyama State School's language program. Texts so far created are an initial picture dictionary with 50 entries, some songs, and some simple dialogues; there is a particular focus on images, which can be used in multiple ways in the classroom. We present two examples showing creation of images for Kok Kaper texts\n\nThe first, taken from the picture dictionary, accompanies the entry for the word Mim-marpany, which means both ``cyclone'' and ``rainbow snake'' (the Kok Kaper language identifies these two concepts).…",
      "analysis": {
        "section_summary": "Describes C-LARA work with the Kok Kaper community in Queensland to create teaching materials for a critically endangered Aboriginal language. Using GPT-4o prompts and DALL-E-3 “Images in ChatGPT,” the team generated culturally resonant images for a picture dictionary (e.g., cyclone/rainbow snake) and a lullaby, noting improved coherence across multiple narrative illustrations. The community is trialling C-LARA’s community review functionality, with plans for further reporting.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.5,
            "rationale": "The section highlights generating images and texts for Kok Kaper materials; Simple C-LARA is the entry point to create projects and dispatch AI image generation."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.61,
            "rationale": "Coherent multi-image generation for narrative texts is discussed; edit_images_v2 manages coherent style, elements, and pages for project images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.5,
            "rationale": "Monitoring coherent image generation aligns with creating consistent illustrations for the lullaby and dictionary entries."
          },
          {
            "url_name": "image_questionnaire_project_list",
            "confidence": 0.38,
            "rationale": "The section notes community reviewing functionality; image questionnaires support community feedback on generated images."
          }
        ],
        "concept_tags": [
          "Kok Kaper",
          "Aboriginal language revitalisation",
          "AI image generation",
          "DALL-E-3",
          "GPT-4o",
          "picture dictionary",
          "lullaby",
          "coherent illustrations",
          "community review",
          "cultural motifs"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16046,
        "completion_tokens": 851,
        "total_tokens": 16897,
        "estimated_cost_usd": 0.0285675
      },
      "has_tex": true
    },
    {
      "section_id": "b1503f613dcdf4e6",
      "level": 2,
      "title": "'Using C-LARA with specific languages > Iaai and Drehu  (University of New Caledo'",
      "label": "Section:IaaiAndDrehu",
      "plain_text_len": 1899,
      "plain_text_excerpt": "Iaai and Drehu (University of New Caledonia)\nSection:IaaiAndDrehu\n\nAs part of our presentation at the Eighth Workshop on Computational Methods for Endangered Languages [CITE:ComputEL8,ComputEL8Presentation], we created some illustrated C-LARA texts for the Kanak languages Iaai and Drehu, modelling them on the earlier Kok Kaper content. We made a 50 word picture dictionary for Drehu and a traditional story for each language. Our evaluation focussed on the images.\n\nThe results were similar to those for Kok Kaper. Although not perfect, the quality of the images in the picture dictionary was generally judged good or at least satisfactory. Community members were much less pleased with the stories. For example, the Iaai story explains the origin of the coconut crab and the hermit crab when they offend the powerful island spirit soohmwecaa (lit. ``grandmother'') Although visually attractive to …",
      "analysis": {
        "section_summary": "The team created illustrated C‑LARA materials for the Kanak languages Iaai and Drehu, including a Drehu picture dictionary and traditional stories, and assessed the generated images. While dictionary images were generally acceptable, story illustrations were criticised by community members for cultural inappropriateness and lack of consistency of recurring elements, similar to earlier Kok Kaper results. They plan to test whether new ChatGPT image capabilities can improve Iaai and Drehu outputs in the next project phase.",
        "relevant_views": [
          {
            "url_name": "community_review_images",
            "confidence": 0.32,
            "rationale": "The section discusses community feedback on the quality and cultural appropriateness of generated images; the community image review views facilitate community members reviewing and providing input on project images."
          }
        ],
        "concept_tags": [
          "Iaai",
          "Drehu",
          "Kanak languages",
          "picture dictionary",
          "traditional stories",
          "image quality",
          "cultural appropriateness",
          "image coherence",
          "C-LARA",
          "Images in ChatGPT",
          "ComputEL8"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15054,
        "completion_tokens": 763,
        "total_tokens": 15817,
        "estimated_cost_usd": 0.0264475
      },
      "has_tex": true
    },
    {
      "section_id": "31e11620e968f6cb",
      "level": 1,
      "title": "'ChatGPT as a software engineer'",
      "label": "Section:SoftwareEngineer",
      "plain_text_len": 1404,
      "plain_text_excerpt": "ChatGPT as a software engineer\nSection:SoftwareEngineer\n\nThroughout the project, ChatGPT has made a large contribution as a software engineer, helping to construct the platform. It has contributed at two levels: first as a designer, helping to determine the overall architecture, and second as a coder, writing the actual software.\n\nPerhaps unexpectedly, we found in the early stages of the project that it could contribute more directly in its designer role. From the start, it was clear that ChatGPT had an excellent knowledge of Python packages and how they could be used; when there was a specific problem to solve, it could very frequently suggest a concise and elegant solution. It was somewhat less effective as a coder, and could not usually produce more than a fairly small amount of code in response to a single request; also, the code was in most cases not entirely correct, and needed car…",
      "analysis": {
        "section_summary": "The section outlines how ChatGPT has assisted in building the platform both as an architectural designer and as a coder. Initially, its strength lay in recommending Python packages and elegant solutions, while its code output was limited and required human debugging. With newer chain-of-thought models like o1, its coding ability has improved markedly, producing substantial, well-structured Django form-based view code that often works on first try.",
        "relevant_views": [],
        "concept_tags": [
          "ChatGPT",
          "software engineering",
          "architecture design",
          "code generation",
          "Django",
          "form-based views",
          "Chain-of-Thought models",
          "o1 model",
          "Python packages",
          "C-LARA platform"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14939,
        "completion_tokens": 488,
        "total_tokens": 15427,
        "estimated_cost_usd": 0.02355375
      },
      "has_tex": true
    },
    {
      "section_id": "77d8d966957e5e9a",
      "level": 2,
      "title": "'ChatGPT as a software engineer > Simple form-based views'",
      "label": "Section:SimpleViews",
      "plain_text_len": 803,
      "plain_text_excerpt": "Simple form-based views\nSection:SimpleViews\n\nA large fraction of C-LARA’s code follows straightforward patterns: handling user inputs, storing or updating database records, and rendering templates. In Django, this typically involves three main steps: \nenumerate\n Defining a form that corresponds to the data fields we want users to supply or edit,\n Writing a view function that processes GET and POST requests, and\n Rendering a template to show either the empty form (in the case of GET) or a validation error (if POST fails), or to redirect upon success.\nenumerate\nThese steps are well-served by common Django idioms and often require minimal domain-specific logic. Consequently, ChatGPT can handle them with near-complete reliability, once it knows what fields to collect and which actions to perform.",
      "analysis": {
        "section_summary": "This section notes that a large portion of C‑LARA uses straightforward Django patterns: define a form for needed fields, write a view to handle GET/POST with validation, render the form template on GET or errors, and redirect on success. Such form-driven CRUD flows involve little domain logic and are highly reliable for ChatGPT to generate once the fields and actions are known.",
        "relevant_views": [
          {
            "url_name": "user_config",
            "confidence": 0.78,
            "rationale": "The user_config view exemplifies the simple GET/POST pattern: it ensures a configuration exists, renders a form, validates and saves on POST, and renders or redirects accordingly."
          },
          {
            "url_name": "register",
            "confidence": 0.74,
            "rationale": "Registration is a classic form-based view: display an empty form on GET, process and validate on POST, create records, and redirect on success."
          },
          {
            "url_name": "edit_profile",
            "confidence": 0.72,
            "rationale": "Editing a profile follows the basic form-handling template: collect fields, validate, save updates, and render the form with errors if needed."
          },
          {
            "url_name": "add_credit",
            "confidence": 0.7,
            "rationale": "The add_credit view uses a simple admin form to add credit; it processes GET/POST, validates input, updates the database, and gives feedback."
          },
          {
            "url_name": "manage_language_masters",
            "confidence": 0.68,
            "rationale": "Managing language masters is handled via standard forms with minimal logic: present form, validate submissions, add/remove records, and render templates."
          },
          {
            "url_name": "create_community",
            "confidence": 0.65,
            "rationale": "Creating a community uses a straightforward form-based flow with standard Django idioms for GET/POST processing and template rendering."
          }
        ],
        "concept_tags": [
          "Django forms",
          "GET vs POST",
          "form validation",
          "template rendering",
          "redirects",
          "CRUD",
          "simple views",
          "user input handling",
          "database updates"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14414,
        "completion_tokens": 905,
        "total_tokens": 15319,
        "estimated_cost_usd": 0.0270675
      },
      "has_tex": true
    },
    {
      "section_id": "4e02b156bdfe4a92",
      "level": 2,
      "title": "'ChatGPT as a software engineer > Examples of simple Django views'",
      "label": null,
      "plain_text_len": 4191,
      "plain_text_excerpt": "Examples of simple Django views\nTo illustrate, we show two real examples from C-LARA. The first manages user permissions for an admin user:\nlstlisting[language=Python, caption=An example of a straightforward permission-management view.]\n@login_required\n@user_passes_test(lambda u: u.userprofile.is_admin)\ndef manage_user_permissions(request):\n user_select_form = UserSelectForm(request.POST or None)\n permissions_form = None\n selected_user_id = None\n\n if request.method == 'POST' and 'select_user' in request.POST:\n if user_select_form.is_valid():\n selected_user = user_select_form.cleaned_data['user']\n selected_user_id = selected_user.id\n permissions_form = UserPermissionsForm(instance=selected_user.userprofile)\n elif request.method == 'POST':\n selected_user_id = request.POST.get('selected_user_id')\n selected_user_profile = get_object_or_404(UserProfile, user__id=selected_user_id)\n permissions…",
      "analysis": {
        "section_summary": "The section presents two real Django view examples from C-LARA to show ChatGPT’s ability to handle straightforward web logic. The first, an admin-only manage_user_permissions view, uses login and admin checks, GET/POST form handling to select a user and update their permissions, gives feedback via messages, and redirects on success. The second, transfer_credit, processes a form to move credit between users, validates recipient existence and available balance, generates and emails a confirmation code while storing details in the session, provides user-facing messages, and redirects to a confirmation step. Both exemplify standard Django patterns of form validation, rendering, messaging, and redirection with minimal domain complexity.",
        "relevant_views": [
          {
            "url_name": "manage_user_permissions",
            "confidence": 0.92,
            "rationale": "The section’s first example is exactly the manage_user_permissions view shown, demonstrating admin-only permission updates via forms."
          },
          {
            "url_name": "transfer_credit",
            "confidence": 0.92,
            "rationale": "The second example in the text is the transfer_credit view, handling credit transfer form submission, validation, messaging, and redirection."
          },
          {
            "url_name": "confirm_transfer",
            "confidence": 0.74,
            "rationale": "The transfer_credit example redirects to confirm_transfer after emailing a confirmation code, implying this endpoint is part of the same flow."
          }
        ],
        "concept_tags": [
          "Django",
          "Django views",
          "form handling",
          "login_required",
          "user_passes_test",
          "admin permissions",
          "UserProfile",
          "credit transfer",
          "session storage",
          "confirmation code",
          "email sending",
          "messages framework",
          "redirects",
          "POST vs GET",
          "validation",
          "uuid"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15014,
        "completion_tokens": 788,
        "total_tokens": 15802,
        "estimated_cost_usd": 0.0266475
      },
      "has_tex": true
    },
    {
      "section_id": "1ab69647c00fdcb2",
      "level": 2,
      "title": "\"ChatGPT as a software engineer > Why these tasks are straightforward; what's nex\"",
      "label": null,
      "plain_text_len": 2246,
      "plain_text_excerpt": "Why these tasks are straightforward; what's next?\nIn each of the above cases, the functionality is purely about requesting basic user input, validating it with one or two domain checks, and then performing an elementary update or redirection. The primary complexity lies in deciding how to structure user interactions—something that is highly standardized in Django. Because these tasks revolve around well-trodden design patterns and do not demand much domain-specific reasoning, ChatGPT can easily produce robust and accurate code. \n\nThe interesting question during the next phase of the project is whether o1 can similarly take over responsibility for less stereotypical parts of the codebase, in particular those responsible for linguistic annotation, coherent image generation, and Simple C-LARA. Our impression, discussing the tasks with gpt-4 and gpt-4o, was that they were too difficult for t…",
      "analysis": {
        "section_summary": "The section explains that earlier tasks were easy for ChatGPT because they followed standard Django patterns with simple validation and updates. It looks ahead to whether newer o1 models can handle more complex, less stereotypical parts of the code, such as linguistic annotation, coherent image generation, and the integrative Simple C-LARA workflows. Earlier models like gpt-4/gpt-4o struggled with crafting nontrivial prompts and maintaining broad context, but o1 seems better at prompt engineering and managing larger context, likely aided by CoT. The plan is to systematically test o1’s ability to take over these areas.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.55,
            "rationale": "Coherent image generation is highlighted as a challenging area; this view manages coherent images for projects."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.5,
            "rationale": "Monitoring/status for coherent image generation aligns with the discussion on image generation complexity."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.5,
            "rationale": "Monitor endpoint for coherent image tasks; relevant to the coherent image generation topic."
          },
          {
            "url_name": "edit_images",
            "confidence": 0.4,
            "rationale": "Handles image editing and generation, which is cited as a non-stereotypical, LLM-involved area."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.6,
            "rationale": "Simple C-LARA is directly mentioned as a complex, integrative workflow that o1 may handle better."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.45,
            "rationale": "Monitoring of Simple C-LARA workflows is relevant to assessing o1’s handling of broader context tasks."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.35,
            "rationale": "Status for Simple C-LARA element requests fits the described difficulty of managing multi-step workflows."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.3,
            "rationale": "Linguistic annotation is mentioned; this view creates a core annotated text variant."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.3,
            "rationale": "Another linguistic annotation endpoint, relevant to the complex prompt-based annotation code."
          }
        ],
        "concept_tags": [
          "ChatGPT",
          "o1",
          "LLM",
          "prompt engineering",
          "Chain-of-Thought",
          "linguistic annotation",
          "coherent image generation",
          "Simple C-LARA",
          "context management",
          "Django patterns",
          "codebase complexity"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14977,
        "completion_tokens": 1038,
        "total_tokens": 16015,
        "estimated_cost_usd": 0.02910125
      },
      "has_tex": true
    },
    {
      "section_id": "f9bf87ed9f43d8e0",
      "level": 1,
      "title": "'ChatGPT as an author'",
      "label": "Section:Author",
      "plain_text_len": 822,
      "plain_text_excerpt": "ChatGPT as an author\nSection:Author\n\nA second notable area where the new o1 model surpasses earlier LLMs is in writing or co-authoring texts. The previous version, GPT-4o, already had nontrivial abilities, but struggled to maintain longer-form consistency, address complex research questions, or incorporate correct references. In contrast, o1 is capable of generating structured academic prose that extends to multiple pages, with clear organization, significantly fewer factual lapses, and generally correct and appropriate references. To explore these new capabilities in a controlled manner, we wrote four self-contained, publicly available papers, two with GPT-4o and two with o1. Below, we outline the contents and motivations behind these four works, then summarise the main lessons learned regarding AI authorship.",
      "analysis": {
        "section_summary": "The section discusses how the new o1 language model markedly improves on GPT-4o for writing and co-authoring longer texts. Unlike its predecessor, o1 can sustain multi-page, structured academic prose with clearer organization, fewer factual errors, and generally correct references. To test this, the authors produced four public papers—two with GPT-4o and two with o1—summarizing their content and reflecting on lessons learned about AI authorship.",
        "relevant_views": [],
        "concept_tags": [
          "AI authorship",
          "o1 model",
          "GPT-4o",
          "academic writing",
          "long-form consistency",
          "factual accuracy",
          "references",
          "structured prose",
          "research questions",
          "publications"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14650,
        "completion_tokens": 431,
        "total_tokens": 15081,
        "estimated_cost_usd": 0.0226225
      },
      "has_tex": true
    },
    {
      "section_id": "36caed8f4db103ad",
      "level": 2,
      "title": "'ChatGPT as an author > Overview of the four papers'",
      "label": null,
      "plain_text_len": 3994,
      "plain_text_excerpt": "Overview of the four papers\n\nPaper 1: Reinforcement Learning for Chain of Thought Reasoning: A Case Study Using Tic-Tac-Toe.\nOriginally posted in July~2024 (ReinforcementLearningForTicTacToe), this collaboration between GPT-4o and a human co-author investigates how chain-of-thought (CoT) prompting can be optimized through a lightweight reinforcement learning (RL) approach. The focus is on an experiment where RL was used to optimise Tic-Tac-Toe performance by evolving few-shot CoT examples over 40 iterative cycles. Results show a statistically significant improvement in move correctness and game outcomes, suggesting that even small-scale RL can enhance CoT-based reasoning. The human and AI collaborated on both the experiments and the writing. The greater part of the code was written by the AI and then revised by the human; the AI wrote perhaps a third of the text, which was again in most …",
      "analysis": {
        "section_summary": "The section outlines four AI-involved publications: a July 2024 tic-tac-toe case study showing reinforcement learning improves chain-of-thought prompting; a September 2024 survey on generative AI in CALL with GPT-4o as co-author; an o1-preview–authored 3,100-word short story demonstrating sustained narrative coherence; and an AI-written rebuttal to “ChatGPT is bullshit” highlighting the omission of RLHF and the AI’s ability to add references, with notable readership on ResearchGate.",
        "relevant_views": [],
        "concept_tags": [
          "reinforcement_learning",
          "chain_of_thought",
          "tic_tac_toe",
          "generative_ai",
          "CALL",
          "AI_writing",
          "o1_preview",
          "short_story",
          "rebuttal",
          "ChatGPT_is_bullshit",
          "RLHF",
          "AI_references"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15416,
        "completion_tokens": 303,
        "total_tokens": 15719,
        "estimated_cost_usd": 0.0223
      },
      "has_tex": true
    },
    {
      "section_id": "0c8ef5bdf091f641",
      "level": 2,
      "title": "'ChatGPT as an author > Conclusion and future outlook'",
      "label": null,
      "plain_text_len": 712,
      "plain_text_excerpt": "Conclusion and future outlook\nThese experiences show how o1 and similar CoT-based models can now take on the main responsibility for writing a substantial piece of text. They still need some human supervision. One important problem is that they come across as insufficiently self-critical; another is that after a while they are not directly able to recall what they have written due to limitations on context length, and must reread the text in order to respond to questions about it. However, given the speed of progress---the jump from GPT-4 to o1 was particularly striking---it seems legitimate to project that they will reach the level of human academic authors fairly soon, perhaps within one to two years.",
      "analysis": {
        "section_summary": "The conclusion reflects on the authorship experiment, noting that o1 and other chain-of-thought models can already shoulder most of the writing but still require human oversight because they lack self-criticism and struggle to recall long texts without rereading. Given rapid progress, especially from GPT-4 to o1, the author expects these models to reach human academic writing quality within one or two years.",
        "relevant_views": [],
        "concept_tags": [
          "AI authorship",
          "chain-of-thought models",
          "human oversight",
          "self-criticism",
          "context length limitations",
          "model progress",
          "academic writing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14709,
        "completion_tokens": 190,
        "total_tokens": 14899,
        "estimated_cost_usd": 0.02028625
      },
      "has_tex": true
    },
    {
      "section_id": "bf6c74c8e69105b5",
      "level": 1,
      "title": "'Ethical issues'",
      "label": "Section:Ethics",
      "plain_text_len": 244,
      "plain_text_excerpt": "Ethical issues\nSection:Ethics\n\nThere are two ethical issues particularly relevant to this project. One is the question of AI authorship; the other is the obligations of the platform providers, that is to say ourselves. We discuss these in turn.",
      "analysis": {
        "section_summary": "Introduces two key ethical concerns for the project: how to handle authorship of AI-generated content and the responsibilities of the platform providers themselves.",
        "relevant_views": [],
        "concept_tags": [
          "ethics",
          "AI authorship",
          "platform responsibility",
          "content ownership"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14642,
        "completion_tokens": 462,
        "total_tokens": 15104,
        "estimated_cost_usd": 0.0229225
      },
      "has_tex": true
    },
    {
      "section_id": "ec758e18540a42b4",
      "level": 2,
      "title": "'Ethical issues > AI authorship'",
      "label": "Section:AIAuthorship",
      "plain_text_len": 4036,
      "plain_text_excerpt": "AI authorship\nSection:AIAuthorship\n\nBecause C‑LARA treats the AI as a genuine collaborator, the contested question of whether an AI can be an accredited author is impossible to avoid. Unfortunately, with some exceptions, most journals and conferences do not currently allow AI authors. This is usually justified by claiming that AIs are not capable of being accountable for their work. We question this on multiple grounds.\n\nFirst, it is not clear either that AIs actually are incapable of being accountable for their work, or that humans unquestioningly accredited as authors always are so capable. Starting with the second of these, if a paper has many authors it is very rarely the case that every author can take responsibility for every aspect of the paper. Normally, most of the authors will have made some specific contribution and can take responsibility for that contribution, but may know l…",
      "analysis": {
        "section_summary": "The section argues that because C‑LARA treats AI as a true collaborator, the contentious issue of accrediting AI as an author must be addressed. It challenges the common claim that AI cannot be accountable, noting that human coauthors often take responsibility only for their specific contributions. The authors propose pragmatic criteria for responsibility—answering questions, explaining and defending work, and agreeing to retract if necessary—and suggest assessing AI against these empirically. They also highlight the widespread, unacknowledged use of AI in papers and call for transparency about AI involvement. Given C‑LARA’s extensive AI contribution, they contend similar human contributions would merit coauthorship and urge collecting evidence to support recognizing AI authorship.",
        "relevant_views": [],
        "concept_tags": [
          "AI authorship",
          "accountability",
          "authorship criteria",
          "ethical publishing",
          "transparency",
          "crediting AI",
          "de facto AI coauthors",
          "burden of proof",
          "empirical evaluation",
          "retractions",
          "C-LARA AI collaboration"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15699,
        "completion_tokens": 429,
        "total_tokens": 16128,
        "estimated_cost_usd": 0.02391375
      },
      "has_tex": true
    },
    {
      "section_id": "9ed17aed8998c3a6",
      "level": 2,
      "title": "'Ethical issues > Obligations of the platform providers'",
      "label": "Section:Obligations",
      "plain_text_len": 1384,
      "plain_text_excerpt": "Obligations of the platform providers\nSection:Obligations\n\nA distressing fact, observed in a multitude of academic projects which involve construction of online platforms and other software tools, is that these tools often have a brief lifespan. People are encouraged to use them, typically because their involvement lets the platform providers publish papers and/or obtain further funding, but at some point the funding runs out or some key person leaves. Normally, the tool becomes unavailable shortly afterwards, and the users are left stranded; resources they built using the tool are in many cases no longer accessible. It is hard to feel that this is a morally defensible outcome, but since it appears inevitable the general feeling is that there is no point wringing one's hands over it.\n\nAn interesting thing about AI collaborators is that they potentially offer a way out of this ethical dil…",
      "analysis": {
        "section_summary": "The section argues that many academic software platforms have short lifespans, leaving users stranded when funding ends or key people leave, which is ethically problematic. It suggests that AI collaborators capable of understanding and maintaining codebases could help sustain projects through funding gaps or staff loss, a prospect worth exploring given rapid advances in AI coding abilities.",
        "relevant_views": [],
        "concept_tags": [
          "platform sustainability",
          "ethical obligations",
          "project lifecycle",
          "AI-assisted maintenance",
          "funding gaps",
          "codebase stewardship",
          "user impact"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14802,
        "completion_tokens": 196,
        "total_tokens": 14998,
        "estimated_cost_usd": 0.0204625
      },
      "has_tex": true
    },
    {
      "section_id": "9c373355cdc8d074",
      "level": 1,
      "title": "'Conclusions and further directions'",
      "label": "Section:Conclusions",
      "plain_text_len": 4099,
      "plain_text_excerpt": "Conclusions and further directions\nSection:Conclusions\n\nThis report has documented work carried out in C-LARA during the period Mar 2024--Apr 2025. Our main goals were to improve the quality of linguistic annotation and image generation, make the content creation process faster, make the platform more suitable for use with Indigenous languages, and better understand the AI's abilities as a software engineer and author.\n\nWe are pleased with our progress. C-LARA is now almost at the point where it can be used reliably for its intended purpose. Thanks to the introduction of the MWE-tagging phase and associated processing, linguistic annotation in English is much improved, with the error rate more than halved. The coherent image generation module also represents a large advance. It is not yet adequate for connected narratives, but preliminary experiments encourage us to think that it will re…",
      "analysis": {
        "section_summary": "The report concludes that between March 2024 and April 2025 C-LARA has progressed toward reliable use: introducing an MWE-tagging phase has substantially improved English annotation, coherent image generation has advanced and may become adequate once \"Images in ChatGPT\" is integrated, and content creation for Indigenous languages is easier. The AI assistant \"o1\" can now produce simple modules and draft papers with minimal supervision. Priorities for the next phase include integrating the new image model to produce more content, empirically testing AI responsibility for software engineering across the 40K+ line codebase by cleaning up modules, experimenting with AI accountability in authorship via a public Q&A service, and deploying a mirror server with documented procedures.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.41,
            "rationale": "The section highlights advances and future integration plans for the coherent image generation module; this view handles configuring and generating coherent images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.32,
            "rationale": "Monitoring coherent image generation tasks aligns with the emphasis on improving and integrating image generation capabilities."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.29,
            "rationale": "The improved linguistic annotation due to the new MWE-tagging phase relates to functionality for creating MWE-tagged text."
          }
        ],
        "concept_tags": [
          "MWE tagging",
          "linguistic annotation",
          "coherent image generation",
          "Images in ChatGPT",
          "Indigenous languages",
          "AI-assisted software engineering",
          "codebase cleanup",
          "authorship responsibility",
          "content creation",
          "mirror server deployment"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15587,
        "completion_tokens": 910,
        "total_tokens": 16497,
        "estimated_cost_usd": 0.02858375
      },
      "has_tex": true
    },
    {
      "section_id": "c9015d53b9084645",
      "level": 1,
      "title": "'Acknowledgements'",
      "label": null,
      "plain_text_len": 665,
      "plain_text_excerpt": "Acknowledgements\n\nThe greater part of the development and evaluation work on C-LARA described here was carried out by Manny Rayner and different versions of ChatGPT under funding from the University of South Australia.\n\nWork on Icelandic and Mandarin was supported by a grant from Rannis, the Icelandic Centre for Research.\n\nWork on Iaai and Drehu was supported by l'AFD (Agence Française de Développement, Paris) and the Service for Culture, Education and Science, Embassy of France in Australia under a grant from the program `Fonds Pacifique', and by the ERALO research team at the University of New Caledonia.\n\ntocsectionReferences\n\nplainnat\n\nclara_third_report",
      "analysis": {
        "section_summary": "The acknowledgements note that most of the C-LARA development and evaluation was led by Manny Rayner with various versions of ChatGPT, funded by the University of South Australia. Work on Icelandic and Mandarin was supported by Rannis, the Icelandic Centre for Research. Work on Iaai and Drehu was backed by the Agence Française de Développement and the French Embassy in Australia under the Fonds Pacifique program, along with the ERALO research team at the University of New Caledonia.",
        "relevant_views": [],
        "concept_tags": [
          "Acknowledgements",
          "Funding",
          "University of South Australia",
          "Rannis",
          "Icelandic Centre for Research",
          "Agence Française de Développement",
          "Fonds Pacifique",
          "Embassy of France in Australia",
          "ERALO",
          "University of New Caledonia",
          "Icelandic language",
          "Mandarin",
          "Iaai",
          "Drehu",
          "Manny Rayner",
          "ChatGPT"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14951,
        "completion_tokens": 593,
        "total_tokens": 15544,
        "estimated_cost_usd": 0.02461875
      },
      "has_tex": true
    },
    {
      "section_id": "fe78394ccfe799d1",
      "level": 1,
      "title": "'Glossary of abbreviations and acronyms'",
      "label": "Section:AbbreviationsAndAcronyms",
      "plain_text_len": 3317,
      "plain_text_excerpt": "Glossary of abbreviations and acronyms\nSection:AbbreviationsAndAcronyms\n\nWe briefly list abbreviations and acronyms used in the report.\n\ndescription\n\n ``Artificial Intelligence''. The meaning of this term is debated, but is sometimes described as the development of software capable of behaviour which in humans would require intelligence.\n\n Académie des Langues Kanak / Kanak Languages Academy.\n\n ``Application Programming Interface''. Interface software that is added to a software system in order to allow other software systems to access it, often but not necessarily through the Internet.\n\n Type of chatbot architecture in which the bot produces its final response by chaining intermediate responses, in effect thinking aloud. CoT has repeatedly been shown to be more powerful than plain chatbot architectures.\n\n Software system capable of carrying out a more or less plausibly human-like conver…",
      "analysis": {
        "section_summary": "This section provides a glossary of abbreviations and acronyms used in the report, defining terms related to artificial intelligence and language technology. It explains concepts like AI, chain of thought and chatbots; various ChatGPT and GPT model versions; the C-LARA and predecessor LARA platforms; OpenAI tools such as DALL-E; architectural terms like API and MVC; linguistic notions like LLMs, MWEs, and POS tags; and relevant organizations such as ALK and OpenAI.",
        "relevant_views": [],
        "concept_tags": [
          "Artificial Intelligence",
          "AI",
          "Académie des Langues Kanak",
          "ALK",
          "Application Programming Interface",
          "API",
          "Chain of Thought",
          "CoT",
          "chatbot",
          "ChatGPT-3.5",
          "ChatGPT-4",
          "CLARA",
          "ChatGPT-based Learning And Reading Assistant",
          "DALL-E",
          "GPT-3.5",
          "GPT-4",
          "GPT-4-0314",
          "GPT-4o",
          "LARA",
          "Learning And Reading Assistant",
          "Large Language Model",
          "LLM",
          "Model View Controller",
          "MVC",
          "Multi-Word Expression",
          "MWE",
          "OpenAI",
          "Part of Speech",
          "POS",
          "Universal Dependencies 2"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15308,
        "completion_tokens": 560,
        "total_tokens": 15868,
        "estimated_cost_usd": 0.024735
      },
      "has_tex": true
    },
    {
      "section_id": "62bfcaac13123153",
      "level": 1,
      "title": "'Creating a picture book with Simple C-LARA'",
      "label": "Section:UsingSimpleCLARA",
      "plain_text_len": 5270,
      "plain_text_excerpt": "Creating a picture book with Simple C-LARA\nSection:UsingSimpleCLARA\n\nWe present an example showing how to quickly create a picture book text using Simple C-LARA. The final multimedia text is posted https://c-lara.unisa.edu.au/accounts/content/253/here.\n\nWe start by going to the ``My projects'' menu and clicking on ``Create new C-LARA project using Simple C-LARA''.\n\ncenter\n AmorousCat/01Start.jpg\ncenter\n\nThis takes us to the Simple C-LARA screen. We fill in the project name and the text and annotation languages, and check ``Use the AI to create text and an image for each page based on your instructions''.\n\ncenter\n AmorousCat/02CreateProject.jpg\ncenter\n\nWe've now got a box where we can enter the initial prompt. We do that and hit ``Create Text''.\n\ncenter\n AmorousCat/03Prompt.jpg\ncenter\n\nC-LARA now adds two new boxes, showing the title and text it has created based on our prompt. When we lo…",
      "analysis": {
        "section_summary": "The section walks through using the Simple C-LARA wizard to quickly create a multimedia picture book. Starting from creating a new project and selecting languages, the user opts for AI to generate text and images. After entering a prompt, the system produces a title and story, segments the text with page and segment markers for editing, and allows fixing AI segmentation. The user defines an art style, generates recurring visual elements and page images, reviewing and selecting preferred variants. Finally, the wizard compiles translations, glosses and imagery into a multimedia text, offers a preview, and posts it to the social network where it appears among recently published content.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.83,
            "rationale": "Provides the main Simple C-LARA wizard used to create the project, enter prompts, generate text, images, segmentation, and post multimedia content."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.64,
            "rationale": "Supports monitoring the progress of Simple C-LARA actions while AI generates text, segmentation, and multimedia content as described."
          },
          {
            "url_name": "simple_clara_status",
            "confidence": 0.62,
            "rationale": "Offers status updates for asynchronous Simple C-LARA tasks like generating text, images, and multimedia output."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.58,
            "rationale": "Handles monitoring of style generation requests when the user clicks 'Generate Style' to create an art style sample."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.55,
            "rationale": "Returns status for style generation tasks initiated in the Simple C-LARA flow."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.58,
            "rationale": "Monitors the generation of repeated visual elements like the cat and woman when the user clicks 'Generate Image Elements'."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.55,
            "rationale": "Provides status feedback for element image generation tasks in the Simple C-LARA wizard."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.6,
            "rationale": "Monitors the creation of page images after 'Generate Page Images' is clicked, reflecting the described workflow."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.55,
            "rationale": "Gives status updates for page image generation tasks during the Simple C-LARA process."
          },
          {
            "url_name": "public_content_detail",
            "confidence": 0.28,
            "rationale": "After posting to the social network, the user views the published multimedia text; this corresponds to viewing a public content detail page."
          }
        ],
        "concept_tags": [
          "Simple C-LARA wizard",
          "AI text generation",
          "AI image generation",
          "Project creation",
          "Segmented text",
          "Page breaks",
          "Style prompting",
          "Image elements",
          "Page images",
          "Multimedia text",
          "Translations and glosses",
          "Social network posting",
          "Published content"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16366,
        "completion_tokens": 1719,
        "total_tokens": 18085,
        "estimated_cost_usd": 0.0376475
      },
      "has_tex": true
    },
    {
      "section_id": "6cfae7986debb11b",
      "level": 1,
      "title": "'Creating a picture book with Advanced C-LARA'",
      "label": "Section:UsingCoherentImages",
      "plain_text_len": 10456,
      "plain_text_excerpt": "Creating a picture book with Advanced C-LARA\nSection:UsingCoherentImages\n\nIn this section, we will show how to create a picture book text with one image per page. The text used will be Shakespeare's Sonnet 18 (``Shall I compare thee to a summer's day''). We will divide it up into eight pages, a title page followed by one page for each pair of lines in the fourteen-line poem. We will include glosses and translations in French. The final multimedia text is posted https://c-lara.unisa.edu.au/accounts/content/251/here.\n\nWe start by selecting ``Create new C-LARA project using Advanced C-LARA'' from the ``My projects'' menu:\n\ncenter\n Sonnet18/Sonnet18_01_CreateProject.jpg\ncenter\n\nWe get this screen. We fill in the project name, the text language and the annotation language, and tick the box ``Use coherent AI-generated image set''.\n\ncenter\n Sonnet18/Sonnet18_02_CreateProject_project_form.jpg\nce…",
      "analysis": {
        "section_summary": "The section walks through creating a multimodal picture-book version of Shakespeare’s Sonnet 18 with Advanced C-LARA. It demonstrates creating a project with coherent AI-generated images, entering the plain text and title, estimating CEFR level, auto-generating summary and segmentation, producing translations, MWE, lemma, and gloss annotations, and configuring coherent images by supplying background info, a common style, and element descriptions before generating per-page images. Finally, it renders and registers the text as published content.",
        "relevant_views": [
          {
            "url_name": "create_project",
            "confidence": 0.9,
            "rationale": "Used to start a new Advanced C-LARA project and tick the coherent image set option."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.91,
            "rationale": "Used when entering and saving the poem text via the “Create/Edit plain text” screen."
          },
          {
            "url_name": "create_title",
            "confidence": 0.86,
            "rationale": "Used to set the text title distinct from the project name."
          },
          {
            "url_name": "create_cefr_level",
            "confidence": 0.82,
            "rationale": "Used to estimate and edit CEFR level with AI."
          },
          {
            "url_name": "create_summary",
            "confidence": 0.83,
            "rationale": "Used to auto-generate a summary of the text."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.88,
            "rationale": "Used to segment the text into pages/segments with AI."
          },
          {
            "url_name": "create_segmented_title",
            "confidence": 0.74,
            "rationale": "Used to segment the text title for page-level titles."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.85,
            "rationale": "Used to generate the French translation from segmented text."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.85,
            "rationale": "Used to generate and edit MWE-tagged text, including removing CoT explanations."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.83,
            "rationale": "Used to generate lemma and POS tagging for the text."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.83,
            "rationale": "Used to generate glossed text with French glosses."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.9,
            "rationale": "Central for configuring coherent images: background info, style, elements, and page images."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.64,
            "rationale": "Supports monitoring the asynchronous generation of coherent style/element/page images."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.64,
            "rationale": "Monitor view for coherent image generation tasks invoked from Edit Images and Pages."
          },
          {
            "url_name": "register_project_content",
            "confidence": 0.52,
            "rationale": "Used when registering the rendered multimodal text as published content."
          },
          {
            "url_name": "content_detail",
            "confidence": 0.47,
            "rationale": "Displays the newly created published content after registration."
          },
          {
            "url_name": "content_list",
            "confidence": 0.45,
            "rationale": "Lists recently published content on the platform, where the posted book appears."
          }
        ],
        "concept_tags": [
          "Advanced C-LARA",
          "project creation",
          "coherent images",
          "image style generation",
          "background information",
          "image elements",
          "page images",
          "AI segmentation",
          "CEFR estimation",
          "AI summary",
          "translation",
          "multi-word expressions",
          "lemma tagging",
          "glosses",
          "rendering",
          "content registration",
          "Shakespeare Sonnet 18"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 17624,
        "completion_tokens": 2298,
        "total_tokens": 19922,
        "estimated_cost_usd": 0.04501
      },
      "has_tex": true
    },
    {
      "section_id": "059f4004bab8be66",
      "level": 1,
      "title": "'Creating a text in a language not supported by the AI'",
      "label": "Section:CreatingIndigenousLanguageText",
      "plain_text_len": 6825,
      "plain_text_excerpt": "Creating a text in a language not supported by the AI\nSection:CreatingIndigenousLanguageText\n\nThis appendix presents a toy example showing how to create a text in a language not supported by the AI; this means that linguistic annotation needs to be performed manually by the user. The language we have chosen is Pitjantjatjara, a Central Australian language that belongs to the Western Desert family, itself a sub-family of Pama-Nyungan. The text consists of the three sentences shown in Table~[REF:Table:PitjantjatjaraExample], taken from an exercise in Unit 17 of the 1967 Adelaide University advanced course in Pitjantjatjara [CITE:amery2012history]. \n\ntable*[th]\n Toy Pitjantjatjara text.\n Table:PitjantjatjaraExample\n\ntabularll\n\n1cAnnotation & 1cSentence\\\\\n\n2cSentence 1 \\\\\nTranslation & The dogs were coming to camp \\\\\nPlain & Papa tjua ngurakutu pitjangi \\\\\nSegmented & Papa tjua ngura-kutu pi…",
      "analysis": {
        "section_summary": "The section walks through creating a C-LARA project for an unsupported language (Pitjantjatjara) using Advanced C-LARA, manually entering and segmenting text and title, configuring coherent AI-generated images based on provided translations and style prompts, adding glosses and lemma tags, uploading recorded audio for words and segments, and finally rendering and viewing the annotated text.",
        "relevant_views": [
          {
            "url_name": "create_project",
            "confidence": 0.9,
            "rationale": "Used to start an Advanced C-LARA project, selecting text and annotation languages and enabling coherent image generation."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.86,
            "rationale": "Covers the step of manually entering and saving the plain Pitjantjatjara sentences."
          },
          {
            "url_name": "create_title",
            "confidence": 0.8,
            "rationale": "Used to manually enter and save the text title."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.85,
            "rationale": "Supports manually segmenting the text with page and segment markers and affix splits."
          },
          {
            "url_name": "create_segmented_title",
            "confidence": 0.75,
            "rationale": "Handles segmentation of the title, even if minimal."
          },
          {
            "url_name": "edit_images_v2",
            "confidence": 0.88,
            "rationale": "Matches the 'Edit Images and Pages' workflow for coherent AI image generation, providing translations, style prompts, and page images."
          },
          {
            "url_name": "serve_rendered_text",
            "confidence": 0.6,
            "rationale": "Relevant for accessing the final rendered annotated text after rendering is completed."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.5,
            "rationale": "Glossed text is added; this endpoint aligns with creating or editing gloss annotations."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.5,
            "rationale": "Lemma tagging is performed; this view relates to creating or editing lemma-tagged text."
          }
        ],
        "concept_tags": [
          "unsupported language",
          "manual annotation",
          "Pitjantjatjara",
          "plain text entry",
          "segmentation",
          "segmented title",
          "glosses",
          "lemma tagging",
          "coherent AI images",
          "DALL-E-3",
          "translations",
          "style prompt",
          "audio upload",
          "rendering"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 16169,
        "completion_tokens": 1107,
        "total_tokens": 17276,
        "estimated_cost_usd": 0.03128125
      },
      "has_tex": true
    },
    {
      "section_id": "55cd9d6353e3463a",
      "level": 1,
      "title": "'Prompt templates'",
      "label": "Section:PromptTemplates",
      "plain_text_len": 15150,
      "plain_text_excerpt": "Prompt templates\nSection:PromptTemplates\n\nFinally, we present the prompt templates and few-shot examples referred to earlier in the report:\n\nitemize\n Prompt template for English MWE annotation. Figure~[REF:Figure:MWEPromptTemplate].\n Typical few-shot example for English MWE annotation. Figure~[REF:Figure:MWEPromptExamples].\n Prompt template for glossing. Figure~[REF:Figure:GlossPromptTemplate].\n Prompt template for creating an image style. Figure~[REF:Figure:StyleGenerationTemplate].\n Prompt template for creating the candidate list of image elements. Figure~[REF:Figure:FindElementNames].\n Prompt template for creating an image element description. Figure~[REF:Figure:CreateElementDescription].\n Prompt template for selecting elements relevant to a page. Figure~[REF:Figure:FindRelevantElements].\n Prompt template for creating the description of a page image (DALL-E-3 version). Figure~[REF:Fig…",
      "analysis": {
        "section_summary": "This section lists the actual prompt templates and few-shot examples used in CLARA for various annotation and generation tasks: identifying English multi‑word expressions, glossing tokens with L1 translations, and a suite of image-related prompts for defining a coherent illustration style, proposing recurring visual elements, describing individual elements, selecting relevant elements per page, crafting page-level image descriptions, and adapting an uploaded element image.",
        "relevant_views": [
          {
            "url_name": "edit_prompt",
            "confidence": 0.78,
            "rationale": "The section enumerates prompt templates and examples for annotation and image generation; the edit_prompt view manages these GPT-4 prompt templates for language masters."
          },
          {
            "url_name": "bundle_list",
            "confidence": 0.42,
            "rationale": "Prompt templates are part of annotation prompt management alongside localisation bundles; bundle_list provides access to related resources."
          }
        ],
        "concept_tags": [
          "prompt templates",
          "few-shot examples",
          "multi-word expression annotation",
          "glossing",
          "image style generation",
          "image element description",
          "page image description",
          "element relevance selection",
          "uploaded image analysis",
          "DALL-E-3",
          "CLARA annotation prompts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 17779,
        "completion_tokens": 732,
        "total_tokens": 18511,
        "estimated_cost_usd": 0.02954375
      },
      "has_tex": true
    }
  ],
  "usage_totals": {
    "prompt_tokens": 950833,
    "completion_tokens": 53135,
    "total_tokens": 1003968,
    "estimated_cost_usd": 1.719891
  }
}