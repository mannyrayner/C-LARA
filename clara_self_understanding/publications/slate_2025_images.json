{
  "publication_id": "slate_2025_images",
  "created_at": "2025-12-29T11:02:11.885524+00:00",
  "source_zip": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\data\\overleaf_zips\\SLATE2025Paper.zip",
  "source_zip_sha256": "0a55a7ebb57a1623317f515f64d5f326d9e25a9d51c8566f225b8c78cc4d8383",
  "root_tex": "slate2025clara.tex",
  "flattened_tex": "C:\\cygwin64\\home\\github\\c-lara\\clara_self_understanding\\publications\\slate_2025_images\\flattened.tex",
  "model": "gpt-5.1-codex-max",
  "sections_count": 12,
  "candidate_top_k": 100,
  "sections": [
    {
      "section_id": "2bea80cd9ee95b6d",
      "level": 1,
      "title": "'Introduction'",
      "label": "sec:intro",
      "plain_text_len": 3258,
      "plain_text_excerpt": "Introduction\nsec:intro\n\n reader books have long been a staple of language teaching, particularly \nsecond-language teaching: pictures anchor meaning, lower cognitive\nload for beginners and provide cultural context for advanced learners\n[CITE:plass2005multimedia]. Until now, producing a coherent, fully\nillustrated text has required a team of teachers, illustrators and\nlayout specialists—too costly for many programmes, and logistically problematic for low-resource or minority languages. In this paper, we present an initial case study showing how the situation is rapidly evolving towards a point where it is reasonable to hope that a single non-technical person will soon be able to create a good-quality illustrated text in a time measured in tens of minutes.\n\n sudden inflection point. The release of\nOpenAI’s GPT-Image-1 model [CITE:OpenAIImageGeneration2025] marks a step-change over earlier\nt…",
      "analysis": {
        "section_summary": "The introduction argues that advances in OpenAI’s GPT-Image-1 model now make it feasible for a single non‑technical person to generate coherent, fully illustrated reader texts within minutes. GPT-Image-1 improves on DALL-E-3 with reliable reuse of visual elements and effective negative prompting, but still requires a packaging layer. The open-source C-LARA platform addresses this by wrapping the model in a three-stage Style → Elements → Pages pipeline that enforces global style, supports reusable characters and locations, and allows per-page tweaks or global regenerations. A lightweight revision layer with community moderation supports crowd up/down-voting, and the paper previews an evaluation across multiple languages, including low-resource cases.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.9,
            "rationale": "The introduction describes C-LARA’s coherent Style → Elements → Pages pipeline for GPT-Image-1 image generation; edit_images_v2 implements configuring and managing coherent images with those stages."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.82,
            "rationale": "Monitoring asynchronous generation of styles, elements, and pages aligns with the described three-stage pipeline packaging GPT-Image-1 in C-LARA."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.82,
            "rationale": "Status reporting for coherent image generation tasks matches the need for a non-technical workflow across style, elements, and pages."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.7,
            "rationale": "The Simple C-LARA wizard offers a non-technical interface to generate text and coherent images, fitting the goal of enabling non-technical users to create illustrated texts."
          },
          {
            "url_name": "simple_clara_monitor",
            "confidence": 0.65,
            "rationale": "Provides monitoring for the simple C-LARA workflow that dispatches image generation tasks, relevant to the non-technical pipeline described."
          }
        ],
        "concept_tags": [
          "C-LARA",
          "GPT-Image-1",
          "text-to-image",
          "coherent images",
          "Style→Elements→Pages",
          "negative prompting",
          "non-technical users",
          "language teaching",
          "low-resource languages",
          "revision layer"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15643,
        "completion_tokens": 1562,
        "total_tokens": 17205,
        "estimated_cost_usd": 0.03517375
      },
      "has_tex": true
    },
    {
      "section_id": "fac92698840d32b5",
      "level": 1,
      "title": "'The C‑LARA platform'",
      "label": "sec:C-LARA",
      "plain_text_len": 1288,
      "plain_text_excerpt": "The C‑LARA platform\nsec:C-LARA\n\nExtensive documentation on the C-LARA platform is available elsewhere; in particular, the third C-LARA progress report [CITE:ChatGPTReport3] was released only a few months ago and is essentially up to date except for the issues concerning GPT-Image-1 covered in this paper. We consequently give only a very brief summary here. C-LARA is an open source project, inaugurated in March 2023 and following on from the earlier LARA project [CITE:LARA2019SLaTE]. The core goal of the project is to create a suite of online tools that simplify the task of creating, using and evaluating multimodal pedagogical texts in a wide variety of languages. Wherever possible, the user is given the option of allowing tasks to be performed by integrated AI models, most commonly OpenAI's GPT-4o. Creation of a multimodal C-LARA text proceeds through a number of stages, of which the mos…",
      "analysis": {
        "section_summary": "C-LARA is an open-source platform, launched in 2023 and built to streamline the creation, use, and evaluation of multimodal pedagogical texts across languages. It offers AI-assisted tooling (often via GPT-4o) for stages such as importing or generating plain text, segmenting it into pages, sentences, and words, translating into a glossing language, creating or importing illustrative images, identifying multi-word expressions, glossing, and lemma tagging. A simplified wizard interface, Simple-CLARA, allows rapid production with sensible defaults.",
        "relevant_views": [
          {
            "url_name": "simple_clara",
            "confidence": 0.48,
            "rationale": "The section highlights a wizard-style interface for rapid text production; the simple_clara view implements the Simple-CLARA workflow."
          },
          {
            "url_name": "create_plain_text",
            "confidence": 0.36,
            "rationale": "Generating or importing plain text is a key stage described; this view handles creating the base text within a project."
          },
          {
            "url_name": "create_segmented_text",
            "confidence": 0.35,
            "rationale": "The process includes segmenting text into pages, sentences, and words, which aligns with the segmented text creation view."
          },
          {
            "url_name": "create_translated_text",
            "confidence": 0.32,
            "rationale": "Translation into the glossing language is mentioned; this view manages translated text creation."
          },
          {
            "url_name": "create_mwe_tagged_text",
            "confidence": 0.31,
            "rationale": "Identification of multi-word expressions is part of the workflow; this view supports tagging MWEs."
          },
          {
            "url_name": "create_glossed_text",
            "confidence": 0.31,
            "rationale": "Glossing is an explicit stage; this endpoint handles glossed text creation."
          },
          {
            "url_name": "create_lemma_tagged_text",
            "confidence": 0.3,
            "rationale": "Lemma tagging is listed among key tasks; this view facilitates lemma-tagged text creation."
          }
        ],
        "concept_tags": [
          "C-LARA platform",
          "open source",
          "multimodal pedagogical texts",
          "AI-assisted annotation",
          "GPT-4o",
          "text segmentation",
          "translation",
          "glossing",
          "lemma tagging",
          "multi-word expressions",
          "illustrative images",
          "Simple-CLARA wizard"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14902,
        "completion_tokens": 1140,
        "total_tokens": 16042,
        "estimated_cost_usd": 0.0300275
      },
      "has_tex": true
    },
    {
      "section_id": "5d205ebcbcea8b06",
      "level": 1,
      "title": "'Integrating GPT-Image-1 into C-LARA'",
      "label": "sec:integration",
      "plain_text_len": 1408,
      "plain_text_excerpt": "Integrating GPT-Image-1 into C-LARA\nsec:integration\n\nA naive integration of GPT-Image-1 into a platform like C-LARA is trivial: we just pass the text of each page to the generation endpoint (for simplicity's sake we assume in this paper that we want one image per page), telling the model to create the image. A little experimentation shows that the results of the naive integration are generally unsatisfactory for texts that contain more than one image, since the images produced are inconsistent with each other. Two kinds of consistency are particularly important: consistency of style (usually, we want all the images to have the same style) and consistency of content (usually, if the same element, e.g. a character or location, occurs in two images we want it to be depicted similarly each time). \n\nThe revised integration we describe here, which has gone through several design iterations, ad…",
      "analysis": {
        "section_summary": "The section outlines integrating GPT-Image-1 into C-LARA, noting that naïvely generating one image per page leads to inconsistent styles and depictions across multiple images. A revised approach, developed through iterations and prior experience, introduces generation and revision components to enforce consistent style and recurring element representation, yielding better results across varied texts.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.58,
            "rationale": "Discusses integrating image generation into C-LARA with attention to style and content consistency; the v2 image editing view manages coherent image generation and revision across pages."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.45,
            "rationale": "The section highlights consistency and iterative revision of generated images, which aligns with monitoring coherent image generation tasks."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.43,
            "rationale": "Tracking status of coherent, consistent image generation relates to ensuring style and content consistency described in the integration."
          },
          {
            "url_name": "simple_clara",
            "confidence": 0.24,
            "rationale": "Simple C-LARA includes workflows for creating images for pages; while not explicitly mentioned, it embodies the platform’s image generation integration."
          }
        ],
        "concept_tags": [
          "GPT-Image-1",
          "image generation",
          "integration",
          "C-LARA",
          "style consistency",
          "content consistency",
          "coherent images",
          "image revision",
          "multi-page texts"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14803,
        "completion_tokens": 497,
        "total_tokens": 15300,
        "estimated_cost_usd": 0.02347375
      },
      "has_tex": true
    },
    {
      "section_id": "3ff3b4d74c51dfa7",
      "level": 2,
      "title": "'Integrating GPT-Image-1 into C-LARA > Image generation'",
      "label": "sec:ImageGeneration",
      "plain_text_len": 3231,
      "plain_text_excerpt": "Image generation\nsec:ImageGeneration\n\nThe image generation process is a sequence of three phases. First, the user creates the style; then the ``elements'' (repeated visual components); finally the images that will be included in the document. In each case, invocation of a text model like GPT-4o is alternated with invocation of the multimodal model GPT-Image-1. The details are as follows.\n\nStyle:\nThe user provides an initial specification of the style they have in mind, and optionally adds some background information that may be generally useful in all three phases. In practice, we find we usually get best results by making both the description of the style and the background information simple and generic. For example, the initial style specification might be ``Create an amusing manga-inspired style appropriate to this text and its expected readers'' and the background information might …",
      "analysis": {
        "section_summary": "This section outlines C-LARA's three-phase workflow for generating coherent images using GPT-Image-1 alongside a text model like GPT-4o: first refining a user’s stylistic brief into a detailed style description and sample image; then extracting and visualising repeated elements (characters, objects, locations) by prompting and rendering each; and finally composing page-level images by selecting relevant elements per page, crafting a detailed prompt, and invoking GPT-Image-1’s generate or edit endpoint depending on whether element images are included.",
        "relevant_views": [
          {
            "url_name": "edit_images_v2",
            "confidence": 0.86,
            "rationale": "This view manages the coherent image workflow for a CLARA project, including configuring and kicking off style, element, and page image generation tasks that mirror the three phases described."
          },
          {
            "url_name": "coherent_images_v2_monitor",
            "confidence": 0.78,
            "rationale": "Provides monitoring for the coherent images generation tasks (style, elements, pages), aligning with the staged GPT-4o/GPT-Image-1 process outlined."
          },
          {
            "url_name": "coherent_images_v2_status",
            "confidence": 0.78,
            "rationale": "Supplies status information for asynchronous style, element, and page image generation, directly relevant to tracking the phases described."
          },
          {
            "url_name": "execute_simple_clara_style_requests_monitor",
            "confidence": 0.57,
            "rationale": "In the Simple C-LARA wizard, this monitors style-generation requests, corresponding to the first phase of style creation with GPT-4o and GPT-Image-1."
          },
          {
            "url_name": "execute_simple_clara_style_requests_status",
            "confidence": 0.57,
            "rationale": "Provides status for style-generation tasks in the Simple C-LARA flow, reflecting the initial style phase of the described process."
          },
          {
            "url_name": "execute_simple_clara_element_requests_monitor",
            "confidence": 0.56,
            "rationale": "Monitors element-generation requests in Simple C-LARA, matching the second phase of creating repeated components."
          },
          {
            "url_name": "execute_simple_clara_element_requests_status",
            "confidence": 0.56,
            "rationale": "Returns status for element-generation tasks in the Simple C-LARA workflow, aligning with the elements phase described."
          },
          {
            "url_name": "execute_simple_clara_image_requests_monitor",
            "confidence": 0.55,
            "rationale": "Tracks page image generation tasks in the Simple C-LARA wizard, corresponding to the final page image phase."
          },
          {
            "url_name": "execute_simple_clara_image_requests_status",
            "confidence": 0.55,
            "rationale": "Provides status updates for page-level image generation in Simple C-LARA, reflecting the third phase of creating page images."
          }
        ],
        "concept_tags": [
          "image generation workflow",
          "GPT-Image-1",
          "GPT-4o",
          "style specification",
          "repeated elements",
          "page images",
          "prompt engineering",
          "multimodal model",
          "C-LARA"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15339,
        "completion_tokens": 1547,
        "total_tokens": 16886,
        "estimated_cost_usd": 0.03464375
      },
      "has_tex": true
    },
    {
      "section_id": "b66a668c2bffb595",
      "level": 2,
      "title": "'Integrating GPT-Image-1 into C-LARA > Image revision'",
      "label": "sec:ImageRevision",
      "plain_text_len": 1072,
      "plain_text_excerpt": "Image revision\nsec:ImageRevision\n\nAs already indicated, the user is allowed at each stage to revise the generated material. Descriptions and images are created in multiple versions. There are upvoting/downvoting controls that let the user chose which version of the image to include in the final document. The user can also create new versions of element and page images by making new descriptions. This can be done either by editing and resubmitting an existing description, or by providing advice which is passed to the GPT-4o-like model with a request that it creates an appropriate new description. In practice, we find both modes are used frequently.\n\nWe have also created an image revision mode suitable for collaborative reviewing. This has particularly been developed with Indigenous communities in mind (three of the authors work with Indigenous communities). In the collaborative mode, upvo…",
      "analysis": {
        "section_summary": "Users can iteratively refine generated image descriptions and images, with multiple versions available. They upvote/downvote to choose a final version and can generate new variants by editing descriptions or supplying advice to the GPT-4o-like model. A collaborative review mode collects votes and variant requests from many users; an accredited coordinator periodically reviews and approves which requests proceed, enabling community-driven image curation.",
        "relevant_views": [
          {
            "url_name": "community_review_images",
            "confidence": 0.76,
            "rationale": "Supports community review of images with voting and requesting variants, matching the described collaborative upvote/downvote workflow."
          },
          {
            "url_name": "community_organiser_review_images",
            "confidence": 0.76,
            "rationale": "Provides organiser/coordinator interface to review community votes and decide approvals, aligning with coordinator approval of requests."
          },
          {
            "url_name": "community_review_images_for_page",
            "confidence": 0.7,
            "rationale": "Allows drilling into a page for voting and requesting new images, corresponding to per-page revision and variant creation."
          },
          {
            "url_name": "community_review_images_external",
            "confidence": 0.58,
            "rationale": "Exposes external view of page descriptions and requests, related to shared collaborative reviewing of image versions."
          },
          {
            "url_name": "execute_community_requests_for_page_monitor",
            "confidence": 0.55,
            "rationale": "Monitors execution of approved community requests for new/variant images, relevant to coordinator-approved revisions."
          },
          {
            "url_name": "execute_community_requests_for_page_status",
            "confidence": 0.55,
            "rationale": "Returns status of executing approved community requests, fitting the periodic processing of coordinator-approved changes."
          }
        ],
        "concept_tags": [
          "image revision",
          "multiple versions",
          "upvote",
          "downvote",
          "variant request",
          "description editing",
          "GPT-4o-like model",
          "collaborative review",
          "community voting",
          "coordinator approval",
          "Indigenous communities"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14694,
        "completion_tokens": 860,
        "total_tokens": 15554,
        "estimated_cost_usd": 0.0269675
      },
      "has_tex": true
    },
    {
      "section_id": "09fc36dc022b7616",
      "level": 1,
      "title": "'Evaluation'",
      "label": "sec:eval",
      "plain_text_len": 369,
      "plain_text_excerpt": "Evaluation\nsec:eval\n\nWe describe an initial evaluation of the C-LARA/GPT-Image-1 integration. We first present the evaluation material, then the procedure and the results. The evaluation shown here is of necessity a preliminary one; GPT-Image-1 was released only a few weeks before the SLaTE submission deadline and some time was needed to develop the integration code.",
      "analysis": {
        "section_summary": "Introduces a preliminary evaluation of the C-LARA integration with GPT-Image-1, outlining the materials, procedure, and results, noting the recency of the model and limited time for integration before submission.",
        "relevant_views": [],
        "concept_tags": [
          "evaluation",
          "GPT-Image-1 integration",
          "preliminary study",
          "procedure",
          "results"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14756,
        "completion_tokens": 206,
        "total_tokens": 14962,
        "estimated_cost_usd": 0.020505
      },
      "has_tex": true
    },
    {
      "section_id": "74faabc200366634",
      "level": 2,
      "title": "'Evaluation > Evaluation material'",
      "label": "Table:ImagePrompts",
      "plain_text_len": 3123,
      "plain_text_excerpt": "Evaluation material\n\nOur evaluation material consisted of 11 C-LARA texts, which divided into two groups; six AI-generated English-language pedagogical texts (plus a revised version of one text), and four classical literary texts. The language in the texts from the second group was far more complex than that in the first group. \n\nThe AI-generated texts were taken from the evaluation presented in 5.2 of the third report [CITE:ChatGPTReport3], which used DALL-E-3 and Imagen 3; this time, we used GPT-Image-1. As explained in the third report, we created texts of widely different kinds using prompts based on the brief descriptions shown in Table~[REF:Table:ImagePrompts].\n\ntable[bh!]\n Core prompts used to generate English-language pedagogical texts used in evaluation. The links are to compiled versions of the texts posted on the C-LARA platform.\n Table:ImagePrompts\n\ntabularll\n\n1cLabel & 1cPro…",
      "analysis": {
        "section_summary": "The evaluation used eleven C-LARA texts split between AI-generated English pedagogical pieces and four well-known classical works in their original languages. AI texts were produced with GPT-Image-1 using varied prompts (e.g., picture dictionary, scientific explanation, animal friendship story, robot tale, traditional song, unusual occupation), while literary texts included classics by La Fontaine, Dante, and excerpts from Old Norse and Japanese works. Each text links to a compiled version on the C-LARA platform. An image evaluation questionnaire asked five questions about how well images matched page text, style consistency, element coherence, cultural appropriateness, and visual appeal.",
        "relevant_views": [
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.55,
            "rationale": "The section describes an image evaluation questionnaire and lists its questions; the image_questionnaire_start view initializes and guides users through image questionnaires on CLARA projects."
          },
          {
            "url_name": "content_detail",
            "confidence": 0.32,
            "rationale": "Links are provided to compiled versions of texts posted on the C-LARA platform; content_detail displays registered content with compiled text links."
          }
        ],
        "concept_tags": [
          "evaluation material",
          "AI-generated texts",
          "literary texts",
          "GPT-Image-1",
          "image prompts",
          "image evaluation questionnaire",
          "text-image correspondence",
          "style consistency",
          "element coherence",
          "cultural appropriateness",
          "visual appeal"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15377,
        "completion_tokens": 1232,
        "total_tokens": 16609,
        "estimated_cost_usd": 0.03154125
      },
      "has_tex": true
    },
    {
      "section_id": "ba3b782c8ee0d1eb",
      "level": 2,
      "title": "'Evaluation > Evaluation procedure and results'",
      "label": "tab:results",
      "plain_text_len": 1222,
      "plain_text_excerpt": "Evaluation procedure and results\n\nWe evaluated the images in the texts using an online questionnaire, one question per image, five point Likert scale, based on the questions in Table~[REF:Table:ImageQuestions]. Table~[REF:tab:results] presents summary figures for the questionnaire-based evaluation of the AI-generated and literary texts. We discuss the results for the two groups.\n\ntable[h]\nImage questionnaire results; \\#p = \\#pages, \\#r = \\#raters, Q1--5 as in Table~[REF:Table:ImageQuestions]. Mean Likert score out of 5 for each question.tab:results\n\ntabularlccccccc\n\nText & \\#p & \\#r & Q1 & Q2 & Q3 & Q4 & Q5 \\\\\n\n8cAI-generated pedagogical texts \\\\\n\nDI & 26 & 3 & 4.71 & 4.90 & -- & 4.99 & 4.21\\\\\nSC & 21 & 2 & 3.67 & 4.69 & -- & 4.76 & 3.60\\\\\nSCR & 21 & 2 & 3.81 & 4.62 & -- & 4.10 & 3.71\\\\\nAN & 20 & 2 & 4.62 & 4.97 & 4.87 & 5.00 & 4.53\\\\\nRO & 21 & 2 & 4.50 & 4.98 & 4.79 & 5.00 & 4.69\\\\\nTR &…",
      "analysis": {
        "section_summary": "This section describes an online evaluation of images using a five-point Likert questionnaire, with one question per image based on predefined items. It presents a table summarising mean scores for five questions across AI-generated pedagogical texts and literary texts, along with numbers of pages and raters for each text.",
        "relevant_views": [
          {
            "url_name": "image_questionnaire_start",
            "confidence": 0.52,
            "rationale": "The evaluation used an online questionnaire to rate images per page, aligning with the image questionnaire start view that initiates such evaluations."
          },
          {
            "url_name": "image_questionnaire_summary",
            "confidence": 0.63,
            "rationale": "The section reports aggregated mean scores from the image questionnaire, corresponding to the summary view for image quality questionnaires."
          },
          {
            "url_name": "image_questionnaire_all_projects_summary",
            "confidence": 0.48,
            "rationale": "Overall summaries of questionnaire results across texts mirror the all-projects summary endpoint for image quality questionnaires."
          }
        ],
        "concept_tags": [
          "image questionnaire",
          "Likert scale",
          "evaluation results",
          "AI-generated texts",
          "literary texts",
          "mean scores",
          "raters",
          "image quality assessment"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15034,
        "completion_tokens": 675,
        "total_tokens": 15709,
        "estimated_cost_usd": 0.0255425
      },
      "has_tex": true
    },
    {
      "section_id": "7ff0f35e48310a9b",
      "level": 3,
      "title": "'Evaluation > Evaluation procedure and results > AI-generated pedagogical texts'",
      "label": "sec:AIGeneratedPedagogicalResults",
      "plain_text_len": 2194,
      "plain_text_excerpt": "AI-generated pedagogical texts\nsec:AIGeneratedPedagogicalResults\n\nFour of the texts in this group (DI, AN, RO and TY) were handled well by the models, with scores for text/image correspondence around 4.5 or better and even higher coherence scores. The models did somewhat less well with TR, a retelling of the traditional story ``The Brave Little Tailor'' / ``Seven With One Blow'', which featured two revealing systematic errors. In the first scenes, the tailor is incorrectly depicted wearing his trademark belt featuring the ``Seven With One Blow'' motto, despite the fact that he has not yet made it; examination of the details shows that this is because the element image for the tailor includes the belt, so it consequently is used for all images where the tailor appears. In later scenes, where the tailor meets the giant, both characters are drawn consistently, but their relative heights are…",
      "analysis": {
        "section_summary": "The evaluation of AI-generated pedagogical texts found that models produced coherent, well-matched images for several narratives (DI, AN, RO, TY), but struggled with a retelling of \"The Brave Little Tailor\" due to element reuse (a belt appearing before it was earned) and inconsistent relative character heights. The poorest outcomes were for simple scientific concepts: GPT-Image-1 failed to enforce consistent motion in a car-as-electron circuit analogy, and while it eventually connected a battery, switch, and bulb correctly, DALL-E-3 and Imagen 3 could not render proper wiring. These issues highlight challenges in maintaining element consistency and understanding relationships in illustrations.",
        "relevant_views": [],
        "concept_tags": [
          "AI image generation",
          "text-image correspondence",
          "visual coherence",
          "element consistency",
          "illustration errors",
          "scientific concept illustration",
          "GPT-Image-1",
          "DALL-E-3",
          "Imagen 3",
          "electrical circuits",
          "Brave Little Tailor",
          "computer vision challenges"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14908,
        "completion_tokens": 413,
        "total_tokens": 15321,
        "estimated_cost_usd": 0.022765
      },
      "has_tex": true
    },
    {
      "section_id": "c0cd113768a1187c",
      "level": 3,
      "title": "'Evaluation > Evaluation procedure and results > Literary texts'",
      "label": "sec:LiteraryResults",
      "plain_text_len": 4623,
      "plain_text_excerpt": "Literary texts\nsec:LiteraryResults\n\nGPT-Image-1 did well on LF, an 18th century French adaptation of the Aesop's Fable of the Fox and the Crow. This provides another good example of the large difference between DALL-E-3 and GPT-Image-1. We had carried out many experiments with the LF text using DALL-E-3, always producing unconvincing results that suggested little understanding for most pages; we have posted a typical DALL-E-3 version online [CITE:LeCorbeauEtLeRenardDALL-E-3]. GPT-Image-1, in sharp contrast, immediately draws everything in a way that suggests strong understanding [CITE:LeCorbeauEtLeRenardGPT-Image-1].\n\nFor Japanese, GPT-Image-1’s capabilities were tested with an excerpt from Genji Monogatari, which is written in the Early Middle Japanese of the Heian period of Japan, and is just over 1000 years old.\nThe grammar of EMJ is expectedly difficult to parse, as evident in the au…",
      "analysis": {
        "section_summary": "Discusses GPT-Image-1’s performance on various literary texts: it outperforms DALL-E-3 on a French fable, shows culturally apt imagery for a Heian-era Japanese Genji excerpt despite comprehension gaps, generates Norse-themed illustrations for Völuspá with some spelling and unwanted text issues, and produces consistent, gloomy visuals for Dante’s Inferno, where manual edits could refine historical accuracy and metaphor depiction.",
        "relevant_views": [],
        "concept_tags": [
          "GPT-Image-1",
          "DALL-E-3",
          "literary texts",
          "image generation",
          "French fables",
          "Aesop’s Fables",
          "Genji Monogatari",
          "Early Middle Japanese",
          "cultural motifs",
          "Norse mythology",
          "Völuspá",
          "Old Norse",
          "Dante’s Inferno",
          "style coherence",
          "manual image editing"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15507,
        "completion_tokens": 368,
        "total_tokens": 15875,
        "estimated_cost_usd": 0.02306375
      },
      "has_tex": true
    },
    {
      "section_id": "75c14acc476c139c",
      "level": 1,
      "title": "'Low‑resource languages'",
      "label": "sec:lr",
      "plain_text_len": 1571,
      "plain_text_excerpt": "Low‑resource languages\nsec:lr\nIncorporating images in language teaching and learning materials is essential for developing effective resources for young learners. However, in low-resource language contexts, finding suitable images or producing them is a considerable challenge. The first author (Sophie Rendina) has for several years been collaborating with the Traditional Owners of Kok Kaper/Kokoberra - a critically endangered language of Cape York (Kowanyama, Australia). This collaboration has recently begun to explore the idea of using AI-generated images in their language program, which is delivered at the local school. \n\nPreliminary results from three texts, a lullaby, a song, and a short conversation that includes greetings, highlight the potential of AI-generated imagery in this particular context, while also revealing issues specific to it; while GPT-Image-1 outperforms DALL-E-3 in…",
      "analysis": {
        "section_summary": "The section discusses the challenges of integrating AI-generated illustrations into teaching materials for low-resource Indigenous languages, focusing on a project with Kok Kaper/Kokoberra. Initial trials on texts like a lullaby and greetings show GPT-Image-1 outperforming DALL-E-3 in accuracy and handling negative prompts, yet both struggle to depict Aboriginal characters consistently. Extra manual revision is often needed, but with careful prompt adjustments GPT-Image-1 can yield culturally respectful, age-appropriate classroom illustrations.",
        "relevant_views": [
          {
            "url_name": "edit_images",
            "confidence": 0.62,
            "rationale": "Provides the interface to edit and generate images for project pages, including OpenAI-based generation; relevant to iterating on culturally appropriate illustrations."
          },
          {
            "url_name": "create_dall_e_3_image_monitor",
            "confidence": 0.6,
            "rationale": "Monitors asynchronous DALL-E-3 image generation tasks, matching the section’s comparison of DALL-E-3 performance in low-resource language contexts."
          },
          {
            "url_name": "create_dall_e_3_image_status",
            "confidence": 0.58,
            "rationale": "Returns status for DALL-E-3 image creation jobs, pertinent to tracking and adjusting image generation workflows discussed in the section."
          }
        ],
        "concept_tags": [
          "low-resource languages",
          "AI-generated images",
          "Kok Kaper",
          "Kokoberra",
          "Aboriginal representation",
          "DALL-E-3",
          "GPT-Image-1",
          "negative prompts",
          "visual consistency",
          "culturally respectful illustrations",
          "pedagogical materials",
          "manual revision"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 14715,
        "completion_tokens": 770,
        "total_tokens": 15485,
        "estimated_cost_usd": 0.02609375
      },
      "has_tex": true
    },
    {
      "section_id": "ca5b8caa5f02f7d2",
      "level": 1,
      "title": "'Summary and further directions'",
      "label": "sec:disc",
      "plain_text_len": 914,
      "plain_text_excerpt": "Summary and further directions\nsec:disc\nWe have described how we integrated the new GPT-Image-1 model into the C-LARA platform, and presented an initial evaluation where we tested the functionality on eleven texts of widely different types, some apparently very challenging, to achieve strong results. \n\nThis is an initial paper where we have a limited amount of space available; all we want to establish is that the results are promising and well worth further investigation. We are already engaged in two larger studies which follow on from the one reported here:\n\ndescription\n We have just submitted a paper which greatly expands on the material presented in [REF:sec:lr].\n \n We are carrying out a substantial study which investigates use of these methods in real non-native language learning contexts, and expect to submit a paper about it in late September.\n\ndescription\n\nIEEEtran\nclara_third_re…",
      "analysis": {
        "section_summary": "The section recaps integrating the GPT-Image-1 model into the C-LARA platform and an initial evaluation across eleven varied, challenging texts that yielded strong results, framing this as a promising early study. It notes two ongoing, more extensive follow-up studies: one expanding on prior material and another examining the methods in real non-native language learning contexts, with planned submissions later in the year.",
        "relevant_views": [],
        "concept_tags": [
          "GPT-Image-1",
          "C-LARA",
          "image generation",
          "evaluation",
          "future work",
          "language learning"
        ]
      },
      "usage": {
        "model": "gpt-5.1-codex-max",
        "prompt_tokens": 15003,
        "completion_tokens": 396,
        "total_tokens": 15399,
        "estimated_cost_usd": 0.02271375
      },
      "has_tex": true
    }
  ],
  "usage_totals": {
    "prompt_tokens": 180681,
    "completion_tokens": 9666,
    "total_tokens": 190347,
    "estimated_cost_usd": 0.322511
  }
}